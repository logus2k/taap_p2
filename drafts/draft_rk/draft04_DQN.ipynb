{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f600e7a",
   "metadata": {},
   "source": [
    "# LunarLander-v3 Project using DQN and PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954b6f4",
   "metadata": {},
   "source": [
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6558cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Libraries  ======\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# DQN \n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d22d2",
   "metadata": {},
   "source": [
    "# 2. Create LunarLander-v3 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc165b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Gymnasium: 1.2.3\n"
     ]
    }
   ],
   "source": [
    "ENV_ID = \"LunarLander-v3\"\n",
    "OUT_DIR = \"runs_lunarlander\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Gymnasium:\", gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a598379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: LunarLander-v3\n",
      "Obs space: Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ], (8,), float32)\n",
      "Action space: Discrete(4)\n",
      "Obs shape: (8,)\n",
      "Example obs: [ 0.00229702  1.4181306   0.2326471   0.3204666  -0.00265488 -0.05269805\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_ID)\n",
    "obs, _ = env.reset(seed=42)\n",
    "print(\"Env:\", ENV_ID)\n",
    "print(\"Obs space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Obs shape:\", obs.shape)\n",
    "print(\"Example obs:\", obs)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0318ce70",
   "metadata": {},
   "source": [
    "# 3. CLASSES (ReplayBuffer, Network, DQNAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bd5dce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple(\"Transition\", [\"s\", \"a\", \"r\", \"s2\", \"d\"])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device):\n",
    "        self.buf = deque(maxlen=capacity)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buf)\n",
    "\n",
    "    def add(self, s, a, r, s2, d):\n",
    "        self.buf.append(Transition(s, a, r, s2, d))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buf, batch_size)\n",
    "        s  = torch.tensor(np.vstack([t.s  for t in batch]), dtype=torch.float32, device=self.device)\n",
    "        a  = torch.tensor(np.vstack([t.a  for t in batch]), dtype=torch.int64,   device=self.device)\n",
    "        r  = torch.tensor(np.vstack([t.r  for t in batch]), dtype=torch.float32, device=self.device)\n",
    "        s2 = torch.tensor(np.vstack([t.s2 for t in batch]), dtype=torch.float32, device=self.device)\n",
    "        d  = torch.tensor(np.vstack([t.d  for t in batch]).astype(np.uint8), dtype=torch.float32, device=self.device)\n",
    "        return s, a, r, s2, d\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hidden=256, seed=0):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(seed)\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, action_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(\n",
    "        self, state_size, action_size,\n",
    "        lr, gamma, buffer_size, batch_size, tau,\n",
    "        update_every, learning_starts, grad_clip, seed\n",
    "    ):\n",
    "        random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "        self.device = DEVICE\n",
    "        self.action_size = action_size\n",
    "\n",
    "        self.q = Network(state_size, action_size, seed=seed).to(self.device)\n",
    "        self.q_tgt = Network(state_size, action_size, seed=seed+1).to(self.device)\n",
    "        self.q_tgt.load_state_dict(self.q.state_dict())\n",
    "\n",
    "        self.opt = optim.Adam(self.q.parameters(), lr=lr)\n",
    "        self.buf = ReplayBuffer(buffer_size, self.device)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "        self.update_every = update_every\n",
    "        self.learning_starts = learning_starts\n",
    "        self.grad_clip = grad_clip\n",
    "\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def act(self, state, eps):\n",
    "        if random.random() < eps:\n",
    "            return random.randrange(self.action_size)\n",
    "        s = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            qvals = self.q(s)\n",
    "        return int(torch.argmax(qvals, dim=1).item())\n",
    "\n",
    "    def store(self, s, a, r, s2, done):\n",
    "        self.buf.add(s, a, r, s2, done)\n",
    "\n",
    "    def soft_update(self):\n",
    "        for p_tgt, p in zip(self.q_tgt.parameters(), self.q.parameters()):\n",
    "            p_tgt.data.mul_(1.0 - self.tau)\n",
    "            p_tgt.data.add_(self.tau * p.data)\n",
    "\n",
    "    def train_step(self):\n",
    "        if len(self.buf) < max(self.batch_size, self.learning_starts):\n",
    "            return None\n",
    "\n",
    "        s, a, r, s2, d = self.buf.sample(self.batch_size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            a2 = torch.argmax(self.q(s2), dim=1, keepdim=True)\n",
    "            q2 = self.q_tgt(s2).gather(1, a2)\n",
    "            y  = r + self.gamma * q2 * (1.0 - d)\n",
    "\n",
    "        q_sa = self.q(s).gather(1, a)\n",
    "        loss = F.smooth_l1_loss(q_sa, y)\n",
    "\n",
    "        self.opt.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.q.parameters(), self.grad_clip)\n",
    "        self.opt.step()\n",
    "\n",
    "        self.soft_update()\n",
    "        return float(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e07fda",
   "metadata": {},
   "source": [
    "# 4. UTILITIES (make_env, eval, gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db799a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(seed: int, render_mode=None):\n",
    "    env = gym.make(ENV_ID, render_mode=render_mode)\n",
    "    env.reset(seed=seed)\n",
    "    env.action_space.seed(seed)\n",
    "    return env\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_dqn(agent, n_episodes=20, seed=0, max_steps=1000):\n",
    "    env = make_env(seed + 10_000)\n",
    "    rets, lens = [], []\n",
    "    for ep in range(n_episodes):\n",
    "        s, _ = env.reset(seed=seed + 10_000 + ep)\n",
    "        ep_ret, ep_len = 0.0, 0\n",
    "        for _ in range(max_steps):\n",
    "            a = agent.act(s, eps=0.0)\n",
    "            s, r, terminated, truncated, _ = env.step(a)\n",
    "            ep_ret += float(r); ep_len += 1\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        rets.append(ep_ret); lens.append(ep_len)\n",
    "    env.close()\n",
    "    return float(np.mean(rets)), float(np.std(rets, ddof=1)), rets, lens\n",
    "\n",
    "def moving_avg(x, w=100):\n",
    "    x = np.asarray(x, float)\n",
    "    if len(x) < w:\n",
    "        return np.full_like(x, np.nan)\n",
    "    c = np.convolve(x, np.ones(w)/w, mode=\"valid\")\n",
    "    return np.concatenate([np.full(w-1, np.nan), c])\n",
    "\n",
    "def find_center_seed(agent, base_seed, tries, center_tol, max_steps):\n",
    "    env = make_env(base_seed)\n",
    "    for i in range(tries):\n",
    "        s, _ = env.reset(seed=base_seed + i)\n",
    "        for _ in range(max_steps):\n",
    "            a = agent.act(s, eps=0.0)\n",
    "            s, r, terminated, truncated, _ = env.step(a)\n",
    "            if terminated or truncated:\n",
    "                x = float(s[0])\n",
    "                vx, vy = float(s[2]), float(s[3])\n",
    "                left, right = float(s[6]), float(s[7])\n",
    "                landed = (left > 0.5 or right > 0.5) and abs(vx) < 0.35 and abs(vy) < 0.35\n",
    "                if landed and abs(x) < center_tol:\n",
    "                    env.close()\n",
    "                    return base_seed + i\n",
    "                break\n",
    "    env.close()\n",
    "    return None\n",
    "\n",
    "def save_gif(agent, seed_gif, gif_path, max_steps=1000, fps=30):\n",
    "    import imageio\n",
    "    env = make_env(seed_gif, render_mode=\"rgb_array\")\n",
    "    s, _ = env.reset(seed=seed_gif)\n",
    "    frames, total_r = [], 0.0\n",
    "    for _ in range(max_steps):\n",
    "        frames.append(env.render())\n",
    "        a = agent.act(s, eps=0.0)\n",
    "        s, r, terminated, truncated, _ = env.step(a)\n",
    "        total_r += float(r)\n",
    "        if terminated or truncated:\n",
    "            frames.append(env.render())\n",
    "            break\n",
    "    env.close()\n",
    "    imageio.mimsave(gif_path, frames, fps=fps)\n",
    "    return float(total_r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34666ec5",
   "metadata": {},
   "source": [
    "# 5. SINGLE TRAIN ENTRYPOINT: run_dqn(cfg, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eabcaf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_dqn(cfg: dict, seed: int, run_name: str):\n",
    "    cfg = dict(cfg)\n",
    "    # defaults\n",
    "    cfg.setdefault(\"gamma\", 0.99)\n",
    "    cfg.setdefault(\"grad_clip\", 10.0)\n",
    "    cfg.setdefault(\"eps_start\", 1.0)\n",
    "    cfg.setdefault(\"total_episodes\", 2500)          # ↑ mais margem p/ performance\n",
    "    cfg.setdefault(\"max_steps\", 1000)\n",
    "    cfg.setdefault(\"solved_avg100\", 200.0)\n",
    "    cfg.setdefault(\"lr_finetune\", min(cfg[\"lr_finetune\"], cfg[\"lr\"]))\n",
    "    cfg.setdefault(\"eval_episodes\", 20)\n",
    "    cfg.setdefault(\"extra_after_solved\", 500)       # ↑ sobe “ceiling”\n",
    "    cfg.setdefault(\"make_gif\", False)\n",
    "    cfg.setdefault(\"gif_tries\", 1200)\n",
    "    cfg.setdefault(\"center_tol\", 0.12)\n",
    "\n",
    "    # reproducibility\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "\n",
    "    env = make_env(seed)\n",
    "    state_size = env.observation_space.shape[0]\n",
    "    action_size = env.action_space.n\n",
    "\n",
    "    agent = DQNAgent(\n",
    "        state_size=state_size,\n",
    "        action_size=action_size,\n",
    "        lr=cfg[\"lr\"],\n",
    "        gamma=cfg[\"gamma\"],\n",
    "        buffer_size=int(cfg[\"buffer_size\"]),\n",
    "        batch_size=int(cfg[\"batch_size\"]),\n",
    "        tau=cfg[\"tau\"],\n",
    "        update_every=int(cfg[\"update_every\"]),\n",
    "        learning_starts=int(cfg[\"learning_starts\"]),\n",
    "        grad_clip=cfg[\"grad_clip\"],\n",
    "        seed=seed,\n",
    "    )\n",
    "\n",
    "    scores_100 = deque(maxlen=100)\n",
    "    eps = cfg[\"eps_start\"]\n",
    "    solved_at, stop_at = None, None\n",
    "    finetune_done = False\n",
    "\n",
    "    ep_returns, ep_lengths, ep_eps, step_losses = [], [], [], []\n",
    "    grad_updates = 0\n",
    "\n",
    "    for ep in range(1, cfg[\"total_episodes\"] + 1):\n",
    "        s, _ = env.reset(seed=seed + ep)\n",
    "        ep_ret, ep_len = 0.0, 0\n",
    "\n",
    "        for _ in range(cfg[\"max_steps\"]):\n",
    "            a = agent.act(s, eps=eps)\n",
    "            s2, r, terminated, truncated, _ = env.step(a)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            agent.store(s, a, float(r), s2, done)\n",
    "\n",
    "            agent.total_steps += 1\n",
    "            if agent.total_steps % agent.update_every == 0:\n",
    "                loss = agent.train_step()\n",
    "                if loss is not None:\n",
    "                    step_losses.append(loss)\n",
    "                    grad_updates += 1\n",
    "\n",
    "            s = s2\n",
    "            ep_ret += float(r); ep_len += 1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        ep_returns.append(ep_ret)\n",
    "        ep_lengths.append(ep_len)\n",
    "        ep_eps.append(eps)\n",
    "\n",
    "        scores_100.append(ep_ret)\n",
    "        avg100 = float(np.mean(scores_100))\n",
    "\n",
    "        eps = max(cfg[\"eps_min\"], eps * cfg[\"eps_decay\"])\n",
    "\n",
    "        if ep % 50 == 0:\n",
    "            last_loss = float(np.mean(step_losses[-200:])) if len(step_losses) else np.nan\n",
    "            print(f\"{run_name} | Ep {ep:4d} | avg100: {avg100:7.2f} | eps: {eps:6.3f} | loss~: {last_loss:6.3f}\")\n",
    "\n",
    "        if len(scores_100) == 100 and avg100 >= cfg[\"solved_avg100\"] and stop_at is None:\n",
    "            solved_at = ep\n",
    "            stop_at = min(cfg[\"total_episodes\"], ep + cfg[\"extra_after_solved\"])\n",
    "            if not finetune_done:\n",
    "                for g in agent.opt.param_groups:\n",
    "                    g[\"lr\"] = cfg[\"lr_finetune\"]\n",
    "                finetune_done = True\n",
    "            print(f\"\\n {run_name} solved at ep {ep} | avg100 {avg100:.2f} | continue until ep {stop_at} | finetune_lr {cfg['lr_finetune']}\\n\")\n",
    "\n",
    "        if stop_at is not None and ep >= stop_at:\n",
    "            break\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    mean_eval, std_eval, _, _ = evaluate_dqn(agent, n_episodes=cfg[\"eval_episodes\"], seed=seed, max_steps=cfg[\"max_steps\"])\n",
    "\n",
    "    model_path = os.path.join(OUT_DIR, f\"{run_name}_seed{seed}.pth\")\n",
    "    torch.save(agent.q.state_dict(), model_path)\n",
    "\n",
    "    curves_path = os.path.join(OUT_DIR, f\"{run_name}_seed{seed}_curves.npz\")\n",
    "    np.savez(\n",
    "        curves_path,\n",
    "        returns=np.array(ep_returns, float),\n",
    "        lengths=np.array(ep_lengths, int),\n",
    "        eps=np.array(ep_eps, float),\n",
    "        losses=np.array(step_losses, float),\n",
    "        solved_at_episode=np.array([-1 if solved_at is None else solved_at], int),\n",
    "        total_steps=np.array([agent.total_steps], int),\n",
    "        grad_updates=np.array([grad_updates], int),\n",
    "    )\n",
    "\n",
    "    res = {\n",
    "        \"seed\": seed,\n",
    "        \"cfg\": cfg,\n",
    "        \"run_name\": run_name,\n",
    "        \"solved_at_episode\": solved_at,\n",
    "        \"train_episodes\": len(ep_returns),\n",
    "        \"train_total_steps\": agent.total_steps,\n",
    "        \"grad_updates\": grad_updates,\n",
    "        \"eval_mean\": mean_eval,\n",
    "        \"eval_std\": std_eval,\n",
    "        \"model_path\": model_path,\n",
    "        \"curves_path\": curves_path,\n",
    "    }\n",
    "\n",
    "    if cfg[\"make_gif\"]:\n",
    "        good = find_center_seed(agent, base_seed=seed + 1000, tries=cfg[\"gif_tries\"], center_tol=cfg[\"center_tol\"], max_steps=cfg[\"max_steps\"])\n",
    "        if good is not None:\n",
    "            gif_path = os.path.join(OUT_DIR, f\"{run_name}_seed{seed}_best.gif\")\n",
    "            gif_ret = save_gif(agent, seed_gif=good, gif_path=gif_path, max_steps=cfg[\"max_steps\"], fps=30)\n",
    "            res[\"center_gif_path\"] = gif_path\n",
    "            res[\"center_gif_seed\"] = int(good)\n",
    "            res[\"center_gif_return\"] = float(gif_ret)\n",
    "            print(\"Saved GIF:\", gif_path, \"| return:\", round(gif_ret, 2))\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006bb2c",
   "metadata": {},
   "source": [
    "# 6. MONTE CARLO LOOP + select best (triagem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce0d366",
   "metadata": {},
   "source": [
    "## 6.1 Coverage Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ff5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Coverage scheduler for (lr, gamma) ----------\n",
    "class CoveragePairs:\n",
    "    \"\"\"\n",
    "    Generates (lr, gamma) pairs with guaranteed coverage.\n",
    "    It cycles through ALL combinations in random order, then reshuffles and repeats.\n",
    "    \"\"\"\n",
    "    def __init__(self, lr_list, gamma_list, rng: random.Random):\n",
    "        self.lr_list = list(lr_list)\n",
    "        self.gamma_list = list(gamma_list)\n",
    "        self.rng = rng\n",
    "        self._pool = []\n",
    "        self._refill()\n",
    "\n",
    "    def _refill(self):\n",
    "        self._pool = list(product(self.lr_list, self.gamma_list))\n",
    "        self.rng.shuffle(self._pool)\n",
    "\n",
    "    def next(self):\n",
    "        if not self._pool:\n",
    "            self._refill()\n",
    "        return self._pool.pop()\n",
    "\n",
    "\n",
    "# ---------- tuned discrete spaces ----------\n",
    "LR_LIST = [2e-4, 1e-4, 3e-4, 5e-5, 3e-5]\n",
    "GAMMA_LIST = [0.98, 0.99, 0.995]\n",
    "\n",
    "TAU_LIST = [2e-3, 1e-3, 5e-4]\n",
    "BATCH_LIST = [64, 128, 256]\n",
    "BUFFER_LIST = [200_000, 500_000]\n",
    "LEARNING_STARTS_LIST = [1000, 2000]\n",
    "UPDATE_EVERY_LIST = [2, 4]\n",
    "EPS_DECAY_LIST = [0.995, 0.997, 0.999]\n",
    "EPS_MIN_LIST = [0.01, 0.005]\n",
    "EXTRA_AFTER_SOLVED_LIST = [300, 500]\n",
    "\n",
    "LR_FINETUNE_CANDIDATES = [1e-4, 1e-5, 3e-5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cccede",
   "metadata": {},
   "source": [
    "## 6.2 Sample Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a814d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_dqn_cfg_coverage(rng: random.Random, pair_sched: CoveragePairs):\n",
    "    \"\"\"\n",
    "    Monte Carlo sampler with guaranteed coverage for lr and gamma.\n",
    "    Other hyperparameters are sampled randomly from discrete sets.\n",
    "    \"\"\"\n",
    "    lr, gamma = pair_sched.next()\n",
    "\n",
    "    # Ensure lr_finetune <= lr (safe fine-tune)\n",
    "    lr_ft_choices = [x for x in LR_FINETUNE_CANDIDATES if x <= lr]\n",
    "    lr_finetune = rng.choice(lr_ft_choices) if lr_ft_choices else lr\n",
    "\n",
    "    cfg = {\n",
    "        # Guaranteed-coverage dimensions\n",
    "        \"lr\": lr,\n",
    "        \"gamma\": gamma,\n",
    "\n",
    "        # Fine-tuning\n",
    "        \"lr_finetune\": lr_finetune,\n",
    "\n",
    "        # Target network update\n",
    "        \"tau\": rng.choice(TAU_LIST),\n",
    "\n",
    "        # Replay / training\n",
    "        \"batch_size\": rng.choice(BATCH_LIST),\n",
    "        \"buffer_size\": rng.choice(BUFFER_LIST),\n",
    "        \"learning_starts\": rng.choice(LEARNING_STARTS_LIST),\n",
    "        \"update_every\": rng.choice(UPDATE_EVERY_LIST),\n",
    "\n",
    "        # Exploration\n",
    "        \"eps_decay\": rng.choice(EPS_DECAY_LIST),\n",
    "        \"eps_min\": rng.choice(EPS_MIN_LIST),\n",
    "\n",
    "        # Training control\n",
    "        \"extra_after_solved\": rng.choice(EXTRA_AFTER_SOLVED_LIST),\n",
    "        \"eval_episodes\": 20,\n",
    "\n",
    "        # Qualitative\n",
    "        \"make_gif\": False,\n",
    "    }\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a7014",
   "metadata": {},
   "source": [
    "# 7. FINAL EVAL (3 seeds) + plots + gif do melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6301204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_tune_dqn_coverage(n_trials=20, tune_seed=42, train_seed=42):\n",
    "    \"\"\"\n",
    "    Monte Carlo tuning with guaranteed lr×gamma coverage.\n",
    "    - tune_seed controls the sampling order (reproducible)\n",
    "    - train_seed controls training randomness (fixed for fair tuning)\n",
    "    \"\"\"\n",
    "    rng = random.Random(tune_seed)\n",
    "    pair_sched = CoveragePairs(LR_LIST, GAMMA_LIST, rng)\n",
    "\n",
    "    trials, best = [], None\n",
    "\n",
    "    # Useful: how many trials to cover all lr×gamma at least once\n",
    "    n_cover = len(LR_LIST) * len(GAMMA_LIST)\n",
    "    print(f\"[INFO] lr×gamma combinations: {n_cover}. If n_trials >= {n_cover}, coverage is guaranteed at least once.\")\n",
    "\n",
    "    for t in range(1, n_trials + 1):\n",
    "        cfg = sample_dqn_cfg_coverage(rng, pair_sched)\n",
    "        run_name = f\"dqn_trial{t:02d}\"\n",
    "\n",
    "        print(f\"\\n=== Trial {t}/{n_trials} | cfg: {cfg} ===\")\n",
    "        res = run_dqn(cfg, seed=train_seed, run_name=run_name)  \n",
    "\n",
    "        trials.append(res)\n",
    "        if (best is None) or (res[\"eval_mean\"] > best[\"eval_mean\"]):\n",
    "            best = res\n",
    "\n",
    "        print(f\"Trial {t} -> eval_mean: {res['eval_mean']:.2f} ± {res['eval_std']:.2f} | BEST: {best['eval_mean']:.2f}\")\n",
    "\n",
    "    return trials, best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54356e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] lr×gamma combinations: 15. If n_trials >= 15, coverage is guaranteed at least once.\n",
      "\n",
      "=== Trial 1/20 | cfg: {'lr': 5e-05, 'gamma': 0.99, 'lr_finetune': 1e-05, 'tau': 0.002, 'batch_size': 64, 'buffer_size': 200000, 'learning_starts': 1000, 'update_every': 2, 'eps_decay': 0.999, 'eps_min': 0.01, 'extra_after_solved': 500, 'eval_episodes': 20, 'make_gif': False} ===\n",
      "dqn_trial01 | Ep   50 | avg100: -194.11 | eps:  0.951 | loss~:  1.689\n",
      "dqn_trial01 | Ep  100 | avg100: -177.89 | eps:  0.905 | loss~:  1.249\n",
      "dqn_trial01 | Ep  150 | avg100: -151.65 | eps:  0.861 | loss~:  1.415\n",
      "dqn_trial01 | Ep  200 | avg100: -122.76 | eps:  0.819 | loss~:  1.534\n",
      "dqn_trial01 | Ep  250 | avg100:  -97.93 | eps:  0.779 | loss~:  1.602\n",
      "dqn_trial01 | Ep  300 | avg100:  -83.08 | eps:  0.741 | loss~:  1.750\n",
      "dqn_trial01 | Ep  350 | avg100:  -72.71 | eps:  0.705 | loss~:  1.692\n",
      "dqn_trial01 | Ep  400 | avg100:  -72.09 | eps:  0.670 | loss~:  1.462\n",
      "dqn_trial01 | Ep  450 | avg100:  -65.87 | eps:  0.637 | loss~:  1.802\n",
      "dqn_trial01 | Ep  500 | avg100:  -55.52 | eps:  0.606 | loss~:  1.744\n",
      "dqn_trial01 | Ep  550 | avg100:  -47.96 | eps:  0.577 | loss~:  1.727\n",
      "dqn_trial01 | Ep  600 | avg100:  -45.49 | eps:  0.549 | loss~:  1.607\n",
      "dqn_trial01 | Ep  650 | avg100:  -43.83 | eps:  0.522 | loss~:  1.969\n",
      "dqn_trial01 | Ep  700 | avg100:  -36.98 | eps:  0.496 | loss~:  1.782\n",
      "dqn_trial01 | Ep  750 | avg100:  -27.16 | eps:  0.472 | loss~:  2.412\n",
      "dqn_trial01 | Ep  800 | avg100:  -21.64 | eps:  0.449 | loss~:  2.462\n",
      "dqn_trial01 | Ep  850 | avg100:  -17.88 | eps:  0.427 | loss~:  2.680\n",
      "dqn_trial01 | Ep  900 | avg100:  -15.52 | eps:  0.406 | loss~:  3.131\n",
      "dqn_trial01 | Ep  950 | avg100:  -15.73 | eps:  0.387 | loss~:  3.187\n",
      "dqn_trial01 | Ep 1000 | avg100:  -13.15 | eps:  0.368 | loss~:  3.510\n",
      "dqn_trial01 | Ep 1050 | avg100:  -13.69 | eps:  0.350 | loss~:  3.792\n",
      "dqn_trial01 | Ep 1100 | avg100:   -7.08 | eps:  0.333 | loss~:  3.296\n",
      "dqn_trial01 | Ep 1150 | avg100:   -5.29 | eps:  0.316 | loss~:  3.187\n",
      "dqn_trial01 | Ep 1200 | avg100:  -22.29 | eps:  0.301 | loss~:  2.612\n",
      "dqn_trial01 | Ep 1250 | avg100:  -27.38 | eps:  0.286 | loss~:  2.164\n",
      "dqn_trial01 | Ep 1300 | avg100:  -16.53 | eps:  0.272 | loss~:  2.010\n",
      "dqn_trial01 | Ep 1350 | avg100:    8.53 | eps:  0.259 | loss~:  1.843\n",
      "dqn_trial01 | Ep 1400 | avg100:   71.25 | eps:  0.246 | loss~:  1.960\n",
      "dqn_trial01 | Ep 1450 | avg100:  126.13 | eps:  0.234 | loss~:  1.909\n",
      "dqn_trial01 | Ep 1500 | avg100:  139.60 | eps:  0.223 | loss~:  1.732\n",
      "dqn_trial01 | Ep 1550 | avg100:  152.40 | eps:  0.212 | loss~:  1.502\n",
      "dqn_trial01 | Ep 1600 | avg100:  187.60 | eps:  0.202 | loss~:  1.193\n",
      "\n",
      " dqn_trial01 solved at ep 1623 | avg100 202.34 | continue until ep 2123 | finetune_lr 1e-05\n",
      "\n",
      "dqn_trial01 | Ep 1650 | avg100:  212.12 | eps:  0.192 | loss~:  1.017\n",
      "dqn_trial01 | Ep 1700 | avg100:  224.44 | eps:  0.183 | loss~:  0.773\n",
      "dqn_trial01 | Ep 1750 | avg100:  229.49 | eps:  0.174 | loss~:  0.846\n",
      "dqn_trial01 | Ep 1800 | avg100:  209.49 | eps:  0.165 | loss~:  0.746\n",
      "dqn_trial01 | Ep 1850 | avg100:  200.46 | eps:  0.157 | loss~:  0.749\n",
      "dqn_trial01 | Ep 1900 | avg100:  219.20 | eps:  0.149 | loss~:  0.733\n",
      "dqn_trial01 | Ep 1950 | avg100:  226.72 | eps:  0.142 | loss~:  0.748\n",
      "dqn_trial01 | Ep 2000 | avg100:  220.00 | eps:  0.135 | loss~:  0.841\n",
      "dqn_trial01 | Ep 2050 | avg100:  226.02 | eps:  0.129 | loss~:  0.778\n",
      "dqn_trial01 | Ep 2100 | avg100:  233.01 | eps:  0.122 | loss~:  0.719\n",
      "Trial 1 -> eval_mean: 248.84 ± 44.98 | BEST: 248.84\n",
      "\n",
      "=== Trial 2/20 | cfg: {'lr': 0.0002, 'gamma': 0.99, 'lr_finetune': 0.0001, 'tau': 0.001, 'batch_size': 256, 'buffer_size': 500000, 'learning_starts': 1000, 'update_every': 2, 'eps_decay': 0.999, 'eps_min': 0.005, 'extra_after_solved': 500, 'eval_episodes': 20, 'make_gif': False} ===\n",
      "dqn_trial02 | Ep   50 | avg100: -165.90 | eps:  0.951 | loss~:  1.491\n",
      "dqn_trial02 | Ep  100 | avg100: -146.54 | eps:  0.905 | loss~:  1.263\n",
      "dqn_trial02 | Ep  150 | avg100: -131.08 | eps:  0.861 | loss~:  1.223\n",
      "dqn_trial02 | Ep  200 | avg100: -123.95 | eps:  0.819 | loss~:  1.151\n",
      "dqn_trial02 | Ep  250 | avg100: -108.92 | eps:  0.779 | loss~:  1.210\n",
      "dqn_trial02 | Ep  300 | avg100:  -93.89 | eps:  0.741 | loss~:  1.287\n",
      "dqn_trial02 | Ep  350 | avg100:  -80.92 | eps:  0.705 | loss~:  1.282\n",
      "dqn_trial02 | Ep  400 | avg100:  -76.32 | eps:  0.670 | loss~:  1.333\n",
      "dqn_trial02 | Ep  450 | avg100:  -63.99 | eps:  0.637 | loss~:  1.379\n",
      "dqn_trial02 | Ep  500 | avg100:  -54.28 | eps:  0.606 | loss~:  1.534\n",
      "dqn_trial02 | Ep  550 | avg100:  -47.21 | eps:  0.577 | loss~:  1.548\n",
      "dqn_trial02 | Ep  600 | avg100:  -34.18 | eps:  0.549 | loss~:  1.698\n",
      "dqn_trial02 | Ep  650 | avg100:  -29.06 | eps:  0.522 | loss~:  1.765\n",
      "dqn_trial02 | Ep  700 | avg100:  -27.07 | eps:  0.496 | loss~:  1.838\n",
      "dqn_trial02 | Ep  750 | avg100:  -19.17 | eps:  0.472 | loss~:  2.016\n",
      "dqn_trial02 | Ep  800 | avg100:  -15.48 | eps:  0.449 | loss~:  2.291\n",
      "dqn_trial02 | Ep  850 | avg100:  -13.00 | eps:  0.427 | loss~:  2.446\n",
      "dqn_trial02 | Ep  900 | avg100:   -6.61 | eps:  0.406 | loss~:  2.737\n",
      "dqn_trial02 | Ep  950 | avg100:   -4.09 | eps:  0.387 | loss~:  2.755\n",
      "dqn_trial02 | Ep 1000 | avg100:    2.56 | eps:  0.368 | loss~:  2.810\n",
      "dqn_trial02 | Ep 1050 | avg100:   16.67 | eps:  0.350 | loss~:  3.095\n",
      "dqn_trial02 | Ep 1100 | avg100:   20.01 | eps:  0.333 | loss~:  2.906\n",
      "dqn_trial02 | Ep 1150 | avg100:   18.83 | eps:  0.316 | loss~:  2.909\n",
      "dqn_trial02 | Ep 1200 | avg100:   17.64 | eps:  0.301 | loss~:  2.920\n",
      "dqn_trial02 | Ep 1250 | avg100:   20.49 | eps:  0.286 | loss~:  2.708\n",
      "dqn_trial02 | Ep 1300 | avg100:   31.22 | eps:  0.272 | loss~:  2.439\n",
      "dqn_trial02 | Ep 1350 | avg100:   36.64 | eps:  0.259 | loss~:  2.362\n",
      "dqn_trial02 | Ep 1400 | avg100:   60.38 | eps:  0.246 | loss~:  2.396\n",
      "dqn_trial02 | Ep 1450 | avg100:   69.33 | eps:  0.234 | loss~:  2.554\n",
      "dqn_trial02 | Ep 1500 | avg100:   91.17 | eps:  0.223 | loss~:  2.189\n",
      "dqn_trial02 | Ep 1550 | avg100:  138.45 | eps:  0.212 | loss~:  1.899\n",
      "dqn_trial02 | Ep 1600 | avg100:  151.31 | eps:  0.202 | loss~:  1.789\n",
      "dqn_trial02 | Ep 1650 | avg100:  145.94 | eps:  0.192 | loss~:  1.625\n",
      "dqn_trial02 | Ep 1700 | avg100:  159.78 | eps:  0.183 | loss~:  1.963\n",
      "dqn_trial02 | Ep 1750 | avg100:  172.50 | eps:  0.174 | loss~:  1.396\n",
      "dqn_trial02 | Ep 1800 | avg100:  169.10 | eps:  0.165 | loss~:  1.213\n",
      "dqn_trial02 | Ep 1850 | avg100:  193.48 | eps:  0.157 | loss~:  0.977\n",
      "\n",
      " dqn_trial02 solved at ep 1865 | avg100 201.23 | continue until ep 2365 | finetune_lr 0.0001\n",
      "\n",
      "dqn_trial02 | Ep 1900 | avg100:  222.36 | eps:  0.149 | loss~:  0.859\n",
      "dqn_trial02 | Ep 1950 | avg100:  224.05 | eps:  0.142 | loss~:  0.834\n",
      "dqn_trial02 | Ep 2000 | avg100:  225.37 | eps:  0.135 | loss~:  0.746\n",
      "dqn_trial02 | Ep 2050 | avg100:  218.17 | eps:  0.129 | loss~:  0.760\n",
      "dqn_trial02 | Ep 2100 | avg100:  203.02 | eps:  0.122 | loss~:  0.658\n",
      "dqn_trial02 | Ep 2150 | avg100:  196.37 | eps:  0.116 | loss~:  0.633\n",
      "dqn_trial02 | Ep 2200 | avg100:  208.90 | eps:  0.111 | loss~:  0.619\n",
      "dqn_trial02 | Ep 2250 | avg100:  224.88 | eps:  0.105 | loss~:  0.599\n",
      "dqn_trial02 | Ep 2300 | avg100:  219.83 | eps:  0.100 | loss~:  0.552\n",
      "dqn_trial02 | Ep 2350 | avg100:  229.13 | eps:  0.095 | loss~:  0.562\n",
      "Trial 2 -> eval_mean: 260.55 ± 33.87 | BEST: 260.55\n",
      "\n",
      "=== Trial 3/20 | cfg: {'lr': 0.0002, 'gamma': 0.98, 'lr_finetune': 1e-05, 'tau': 0.002, 'batch_size': 64, 'buffer_size': 500000, 'learning_starts': 1000, 'update_every': 2, 'eps_decay': 0.997, 'eps_min': 0.01, 'extra_after_solved': 500, 'eval_episodes': 20, 'make_gif': False} ===\n",
      "dqn_trial03 | Ep   50 | avg100: -183.52 | eps:  0.861 | loss~:  1.508\n",
      "dqn_trial03 | Ep  100 | avg100: -145.82 | eps:  0.740 | loss~:  1.313\n",
      "dqn_trial03 | Ep  150 | avg100:  -97.82 | eps:  0.637 | loss~:  1.323\n"
     ]
    }
   ],
   "source": [
    "# 1) Tune\n",
    "dqn_trials, best_dqn = mc_tune_dqn_coverage(n_trials=20, tune_seed=42, train_seed=42)\n",
    "print(\"\\nBEST CFG:\", best_dqn[\"cfg\"])\n",
    "\n",
    "# 2) Final 3 seeds \n",
    "FINAL_SEEDS = [42, 123, 999]\n",
    "final_cfg = dict(best_dqn[\"cfg\"])\n",
    "final_cfg[\"eval_episodes\"] = 20\n",
    "final_cfg[\"make_gif\"] = False  # gifs só no melhor final\n",
    "\n",
    "dqn_final = []\n",
    "for s in FINAL_SEEDS:\n",
    "    print(\"\\n=== FINAL seed\", s, \"===\")\n",
    "    dqn_final.append(run_dqn(final_cfg, seed=s, run_name=\"dqn_final\"))\n",
    "\n",
    "means = [r[\"eval_mean\"] for r in dqn_final]\n",
    "print(\"\\nFINAL mean across seeds:\", float(np.mean(means)), \"| std:\", float(np.std(means, ddof=1)))\n",
    "\n",
    "# 3) Pick best FINAL seed and generate GIF (1 run extra, só para gif)\n",
    "best_seed = max(dqn_final, key=lambda x: x[\"eval_mean\"])\n",
    "gif_cfg = dict(final_cfg)\n",
    "gif_cfg[\"make_gif\"] = True\n",
    "gif_run = run_dqn(gif_cfg, seed=best_seed[\"seed\"], run_name=\"dqn_best_gif\")\n",
    "\n",
    "from IPython.display import Image, display\n",
    "display(Image(filename=gif_run[\"center_gif_path\"]))\n",
    "\n",
    "# 4) Plots: returns + avg100, epsilon, loss (por seed)\n",
    "def load_curves(path):\n",
    "    d = np.load(path, allow_pickle=True)\n",
    "    return {k: d[k] for k in d.files}\n",
    "\n",
    "plt.figure()\n",
    "for r in dqn_final:\n",
    "    c = load_curves(r[\"curves_path\"])\n",
    "    y = c[\"returns\"]\n",
    "    plt.plot(y, alpha=0.25, label=f\"seed {r['seed']} raw\")\n",
    "    plt.plot(moving_avg(y, 100), linewidth=2, label=f\"seed {r['seed']} avg100\")\n",
    "plt.title(\"DQN — Training Return per Episode\")\n",
    "plt.xlabel(\"Episode\"); plt.ylabel(\"Return\"); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for r in dqn_final:\n",
    "    c = load_curves(r[\"curves_path\"])\n",
    "    plt.plot(c[\"eps\"], label=f\"seed {r['seed']}\")\n",
    "plt.title(\"DQN — Epsilon Decay\")\n",
    "plt.xlabel(\"Episode\"); plt.ylabel(\"Epsilon\"); plt.legend(); plt.show()\n",
    "\n",
    "plt.figure()\n",
    "for r in dqn_final:\n",
    "    c = load_curves(r[\"curves_path\"])\n",
    "    if len(c[\"losses\"]) > 0:\n",
    "        plt.plot(moving_avg(c[\"losses\"], 200), label=f\"seed {r['seed']} loss avg200\")\n",
    "plt.title(\"DQN — Q-loss (smoothed)\")\n",
    "plt.xlabel(\"Update step\"); plt.ylabel(\"Loss\"); plt.legend(); plt.show() \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ad6d6",
   "metadata": {},
   "source": [
    "# 9. DIFFERENCIATOR: stress-test wind (evaluation only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4f14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wind_kwargs = dict(enable_wind=True, wind_power=10.0, turbulence_power=1.0)\n",
    "\n",
    "# Evaluate FINAL best seed agent under wind by reloading weights into a fresh agent\n",
    "# (keeps the notebook minimal and reproducible)\n",
    "def eval_saved_model_under_wind(model_path, seed=0, n_episodes=20, max_steps=1000):\n",
    "    env = make_env(seed, **wind_kwargs)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.n\n",
    "    env.close()\n",
    "\n",
    "    agent = DQNAgent(obs_dim, act_dim, seed=seed)\n",
    "    agent.q.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    agent.q.eval()\n",
    "    agent.q_tgt.load_state_dict(agent.q.state_dict())\n",
    "\n",
    "    mean_w, std_w, _, _ = evaluate_dqn(agent, n_episodes=n_episodes, seed=seed, max_steps=max_steps, **wind_kwargs)\n",
    "    return mean_w, std_w\n",
    "\n",
    "best_seed = max(dqn_final, key=lambda x: x[\"eval_mean\"])\n",
    "mw, sw = eval_saved_model_under_wind(best_seed[\"model_path\"], seed=best_seed[\"seed\"], n_episodes=20)\n",
    "print(f\"Wind stress-test (20 eps) mean±std: {mw:.2f} ± {sw:.2f} | wind_power=10 | turbulence=1.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ede2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taap_p2 (3.10.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
