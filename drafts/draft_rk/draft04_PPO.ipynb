{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f600e7a",
   "metadata": {},
   "source": [
    "# LunarLander-v3 Project using DQN and PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e954b6f4",
   "metadata": {},
   "source": [
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6558cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Libraries  ======\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# PPO \n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecMonitor\n",
    "\n",
    "import json\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "\n",
    "\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d22d2",
   "metadata": {},
   "source": [
    "# 2. Create LunarLander-v3 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc165b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Gymnasium: 1.2.3\n"
     ]
    }
   ],
   "source": [
    "ENV_ID = \"LunarLander-v3\"\n",
    "OUT_DIR = \"runs_lunarlander_ppo\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Gymnasium:\", gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a598379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env: LunarLander-v3\n",
      "Obs space: Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ], (8,), float32)\n",
      "Action space: Discrete(4)\n",
      "Obs shape: (8,)\n",
      "Example obs: [ 0.00229702  1.4181306   0.2326471   0.3204666  -0.00265488 -0.05269805\n",
      "  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(ENV_ID)\n",
    "obs, _ = env.reset(seed=42)\n",
    "print(\"Env:\", ENV_ID)\n",
    "print(\"Obs space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)\n",
    "print(\"Obs shape:\", obs.shape)\n",
    "print(\"Example obs:\", obs)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a449a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: set this equal to DQN's total env steps for a fair comparison\n",
    "TOTAL_STEPS = 1_050_000   \n",
    "EVAL_EPISODES = 20\n",
    "FINAL_SEEDS = [42, 123, 999]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e07fda",
   "metadata": {},
   "source": [
    "# 3. ENV FACTORY (train env, eval env, wind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db799a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(seed: int, enable_wind: bool = False, wind_power: float = 15.0, turbulence_power: float = 1.5):\n",
    "    def _init():\n",
    "        env = gym.make(\n",
    "            ENV_ID,\n",
    "            continuous=False,\n",
    "            enable_wind=enable_wind,\n",
    "            wind_power=wind_power,\n",
    "            turbulence_power=turbulence_power,\n",
    "        )\n",
    "        env = Monitor(env)  # logs episode return/length\n",
    "        env.reset(seed=seed)\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "    return _init\n",
    "\n",
    "def make_vec_env(seed: int, enable_wind: bool = False):\n",
    "    venv = DummyVecEnv([make_env(seed, enable_wind=enable_wind)])\n",
    "    venv = VecMonitor(venv)\n",
    "    return venv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34666ec5",
   "metadata": {},
   "source": [
    "# 4. PPO CFG SAMPLER (Monte Carlo configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eabcaf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  includes exploration/entropy via ent_coef\n",
    "#  includes net architecture via policy_kwargs\n",
    "\n",
    "def sample_ppo_cfg(rng: random.Random):\n",
    "    # discrete LR choices like DQN\n",
    "    lr = rng.choice([2e-4, 1e-4, 8e-5, 5e-5, 3e-5])\n",
    "\n",
    "    net_arch = rng.choice([\n",
    "        [256, 256],\n",
    "        [128, 128],\n",
    "        [256, 128],\n",
    "        [64, 64],\n",
    "    ])\n",
    "\n",
    "    return {\n",
    "        \"learning_rate\": lr,\n",
    "        \"gamma\": rng.choice([0.97, 0.98, 0.99, 0.995]),\n",
    "        \"gae_lambda\": rng.choice([0.90, 0.95, 0.97]),\n",
    "        \"n_steps\": rng.choice([512, 1024, 2048]),\n",
    "        \"batch_size\": rng.choice([64, 128, 256]),\n",
    "        \"n_epochs\": rng.choice([5, 10, 15]),\n",
    "        \"clip_range\": rng.choice([0.1, 0.2, 0.3]),\n",
    "        \"ent_coef\": rng.choice([0.0, 0.001, 0.005, 0.01]),  # exploration/entropy \n",
    "        \"vf_coef\": rng.choice([0.3, 0.5, 0.7]),\n",
    "        \"max_grad_norm\": rng.choice([0.5, 1.0, 2.0]),\n",
    "        \"policy_kwargs\": dict(net_arch=net_arch),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006bb2c",
   "metadata": {},
   "source": [
    "# 5. HELPERS (load monitor csv, moving avg, grad updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ff5892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg(x, w=100):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if len(x) < w:\n",
    "        return np.array([])\n",
    "    return np.convolve(x, np.ones(w)/w, mode=\"valid\")\n",
    "\n",
    "def load_monitor_csv(monitor_csv_path: str):\n",
    "    # SB3 Monitor CSV has commented header lines starting with '#'\n",
    "    data = np.genfromtxt(monitor_csv_path, delimiter=\",\", names=True, dtype=None, encoding=\"utf-8\", comments=\"#\")\n",
    "    # columns: r (reward), l (length), t (time)\n",
    "    return data[\"r\"], data[\"l\"], data[\"t\"]\n",
    "\n",
    "def ppo_grad_updates(total_steps: int, n_steps: int, batch_size: int, n_epochs: int):\n",
    "    # For PPO: each rollout collects n_steps transitions (per env; here n_env=1).\n",
    "    # Updates per rollout = n_epochs * (n_steps / batch_size) (integer batches if divisible)\n",
    "    rollouts = total_steps // n_steps\n",
    "    minibatches = int(np.ceil(n_steps / batch_size))\n",
    "    return int(rollouts * n_epochs * minibatches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cccede",
   "metadata": {},
   "source": [
    "# 6. RUN PPO (single function used by tuning + final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a814d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    - trains with TOTAL_STEPS (for fairness)\n",
    "#    - evaluates deterministically for EVAL_EPISODES\n",
    "#    - saves cfg, model, monitor logs\n",
    "\n",
    "def run_ppo(cfg: dict, seed: int, run_name: str, total_steps: int = TOTAL_STEPS,\n",
    "            eval_episodes: int = EVAL_EPISODES, wind_eval: bool = False):\n",
    "    cfg = dict(cfg)\n",
    "\n",
    "    # Reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    set_random_seed(seed)\n",
    "\n",
    "    # Train env (NO WIND for fair training by default)\n",
    "    venv = make_vec_env(seed, enable_wind=False)\n",
    "\n",
    "    # Tensorboard + monitor logs\n",
    "    tb_log = os.path.join(OUT_DIR, \"tb\")\n",
    "    os.makedirs(tb_log, exist_ok=True)\n",
    "\n",
    "    model = PPO(\n",
    "        policy=\"MlpPolicy\",\n",
    "        env=venv,\n",
    "        seed=seed,\n",
    "        verbose=0,\n",
    "        tensorboard_log=tb_log,\n",
    "        **cfg\n",
    "    )\n",
    "\n",
    "    model.learn(total_timesteps=int(total_steps), tb_log_name=run_name)\n",
    "\n",
    "    # Save model\n",
    "    model_path = os.path.join(OUT_DIR, f\"{run_name}_seed{seed}.zip\")\n",
    "    model.save(model_path)\n",
    "\n",
    "    # Save cfg\n",
    "    cfg_path = os.path.join(OUT_DIR, f\"{run_name}_seed{seed}_cfg.json\")\n",
    "    with open(cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cfg, f, indent=2)\n",
    "\n",
    "    # Eval env (optionally wind stress-test)\n",
    "    eval_env = make_vec_env(seed + 10_000, enable_wind=wind_eval)\n",
    "    mean_eval, std_eval = evaluate_policy(\n",
    "        model, eval_env, n_eval_episodes=int(eval_episodes), deterministic=True, return_episode_rewards=False\n",
    "    )\n",
    "\n",
    "    # Grad update estimate\n",
    "    grad_updates = ppo_grad_updates(\n",
    "        total_steps=int(total_steps),\n",
    "        n_steps=int(cfg[\"n_steps\"]),\n",
    "        batch_size=int(cfg[\"batch_size\"]),\n",
    "        n_epochs=int(cfg[\"n_epochs\"]),\n",
    "    )\n",
    "\n",
    "    res = {\n",
    "        \"algo\": \"PPO\",\n",
    "        \"run_name\": run_name,\n",
    "        \"seed\": seed,\n",
    "        \"cfg\": cfg,\n",
    "        \"total_steps\": int(total_steps),\n",
    "        \"grad_updates\": int(grad_updates),\n",
    "        \"eval_episodes\": int(eval_episodes),\n",
    "        \"eval_mean\": float(mean_eval),\n",
    "        \"eval_std\": float(std_eval),\n",
    "        \"model_path\": model_path,\n",
    "        \"cfg_path\": cfg_path,\n",
    "        # monitor file path (VecMonitor stores monitor.csv under its log dir;\n",
    "        # easiest is to store a separate Monitor per env; here we used VecMonitor w/ default)\n",
    "    }\n",
    "\n",
    "    venv.close()\n",
    "    eval_env.close()\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2a7014",
   "metadata": {},
   "source": [
    "# 7. MONTE CARLO TUNING LOOP (select best cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6301204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_tune_ppo(n_trials=30, tune_seed=42, train_seed=42, total_steps: int = TOTAL_STEPS):\n",
    "    rng = random.Random(tune_seed)\n",
    "    trials, best = [], None\n",
    "\n",
    "    for t in range(1, n_trials + 1):\n",
    "        cfg = sample_ppo_cfg(rng)\n",
    "        run_name = f\"ppo_trial{t:02d}\"\n",
    "\n",
    "        print(f\"\\n=== PPO Trial {t}/{n_trials} | cfg: {cfg} ===\")\n",
    "        res = run_ppo(cfg, seed=train_seed, run_name=run_name, total_steps=total_steps, eval_episodes=EVAL_EPISODES)\n",
    "        trials.append(res)\n",
    "\n",
    "        if best is None or res[\"eval_mean\"] > best[\"eval_mean\"]:\n",
    "            best = res\n",
    "\n",
    "        print(f\"Trial {t} -> eval_mean: {res['eval_mean']:.2f} ± {res['eval_std']:.2f} | BEST: {best['eval_mean']:.2f}\")\n",
    "\n",
    "    return trials, best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab35da0",
   "metadata": {},
   "source": [
    "# 8. RUN TUNING (pick best cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54356e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PPO Trial 1/30 | cfg: {'learning_rate': 0.0002, 'gamma': 0.99, 'gae_lambda': 0.9, 'n_steps': 512, 'batch_size': 64, 'n_epochs': 15, 'clip_range': 0.1, 'ent_coef': 0.0, 'vf_coef': 0.7, 'max_grad_norm': 1.0, 'policy_kwargs': {'net_arch': [256, 256]}} ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ricar\\dev\\iscte\\taap_p2\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ricar\\dev\\iscte\\taap_p2\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 -> eval_mean: -69.30 ± 112.35 | BEST: -69.30\n",
      "\n",
      "=== PPO Trial 2/30 | cfg: {'learning_rate': 0.0002, 'gamma': 0.97, 'gae_lambda': 0.9, 'n_steps': 512, 'batch_size': 256, 'n_epochs': 15, 'clip_range': 0.1, 'ent_coef': 0.001, 'vf_coef': 0.7, 'max_grad_norm': 2.0, 'policy_kwargs': {'net_arch': [256, 256]}} ===\n",
      "Trial 2 -> eval_mean: -110.40 ± 36.40 | BEST: -69.30\n",
      "\n",
      "=== PPO Trial 3/30 | cfg: {'learning_rate': 3e-05, 'gamma': 0.98, 'gae_lambda': 0.95, 'n_steps': 2048, 'batch_size': 128, 'n_epochs': 5, 'clip_range': 0.1, 'ent_coef': 0.01, 'vf_coef': 0.5, 'max_grad_norm': 1.0, 'policy_kwargs': {'net_arch': [64, 64]}} ===\n",
      "Trial 3 -> eval_mean: -1496.99 ± 316.12 | BEST: -69.30\n",
      "\n",
      "=== PPO Trial 4/30 | cfg: {'learning_rate': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.9, 'n_steps': 512, 'batch_size': 128, 'n_epochs': 5, 'clip_range': 0.2, 'ent_coef': 0.005, 'vf_coef': 0.7, 'max_grad_norm': 1.0, 'policy_kwargs': {'net_arch': [128, 128]}} ===\n",
      "Trial 4 -> eval_mean: 126.64 ± 90.94 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 5/30 | cfg: {'learning_rate': 0.0002, 'gamma': 0.97, 'gae_lambda': 0.95, 'n_steps': 512, 'batch_size': 256, 'n_epochs': 10, 'clip_range': 0.3, 'ent_coef': 0.005, 'vf_coef': 0.7, 'max_grad_norm': 0.5, 'policy_kwargs': {'net_arch': [64, 64]}} ===\n",
      "Trial 5 -> eval_mean: -115.24 ± 69.85 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 6/30 | cfg: {'learning_rate': 0.0002, 'gamma': 0.98, 'gae_lambda': 0.95, 'n_steps': 512, 'batch_size': 64, 'n_epochs': 5, 'clip_range': 0.2, 'ent_coef': 0.005, 'vf_coef': 0.5, 'max_grad_norm': 2.0, 'policy_kwargs': {'net_arch': [256, 256]}} ===\n",
      "Trial 6 -> eval_mean: -89.62 ± 93.61 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 7/30 | cfg: {'learning_rate': 8e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'n_steps': 512, 'batch_size': 256, 'n_epochs': 10, 'clip_range': 0.3, 'ent_coef': 0.0, 'vf_coef': 0.7, 'max_grad_norm': 2.0, 'policy_kwargs': {'net_arch': [128, 128]}} ===\n",
      "Trial 7 -> eval_mean: -99.50 ± 34.31 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 8/30 | cfg: {'learning_rate': 0.0001, 'gamma': 0.98, 'gae_lambda': 0.95, 'n_steps': 1024, 'batch_size': 128, 'n_epochs': 15, 'clip_range': 0.3, 'ent_coef': 0.001, 'vf_coef': 0.7, 'max_grad_norm': 1.0, 'policy_kwargs': {'net_arch': [128, 128]}} ===\n",
      "Trial 8 -> eval_mean: -51.54 ± 90.13 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 9/30 | cfg: {'learning_rate': 0.0002, 'gamma': 0.97, 'gae_lambda': 0.95, 'n_steps': 1024, 'batch_size': 128, 'n_epochs': 5, 'clip_range': 0.1, 'ent_coef': 0.005, 'vf_coef': 0.3, 'max_grad_norm': 2.0, 'policy_kwargs': {'net_arch': [128, 128]}} ===\n",
      "Trial 9 -> eval_mean: -122.34 ± 50.80 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 10/30 | cfg: {'learning_rate': 5e-05, 'gamma': 0.995, 'gae_lambda': 0.9, 'n_steps': 1024, 'batch_size': 64, 'n_epochs': 5, 'clip_range': 0.3, 'ent_coef': 0.005, 'vf_coef': 0.7, 'max_grad_norm': 2.0, 'policy_kwargs': {'net_arch': [64, 64]}} ===\n",
      "Trial 10 -> eval_mean: -131.74 ± 27.18 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 11/30 | cfg: {'learning_rate': 5e-05, 'gamma': 0.99, 'gae_lambda': 0.9, 'n_steps': 512, 'batch_size': 256, 'n_epochs': 10, 'clip_range': 0.1, 'ent_coef': 0.0, 'vf_coef': 0.3, 'max_grad_norm': 0.5, 'policy_kwargs': {'net_arch': [64, 64]}} ===\n",
      "Trial 11 -> eval_mean: -198.23 ± 24.01 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 12/30 | cfg: {'learning_rate': 0.0001, 'gamma': 0.97, 'gae_lambda': 0.95, 'n_steps': 1024, 'batch_size': 256, 'n_epochs': 10, 'clip_range': 0.3, 'ent_coef': 0.005, 'vf_coef': 0.7, 'max_grad_norm': 0.5, 'policy_kwargs': {'net_arch': [64, 64]}} ===\n",
      "Trial 12 -> eval_mean: -98.14 ± 30.90 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 13/30 | cfg: {'learning_rate': 0.0002, 'gamma': 0.99, 'gae_lambda': 0.9, 'n_steps': 1024, 'batch_size': 128, 'n_epochs': 5, 'clip_range': 0.2, 'ent_coef': 0.0, 'vf_coef': 0.7, 'max_grad_norm': 2.0, 'policy_kwargs': {'net_arch': [256, 128]}} ===\n",
      "Trial 13 -> eval_mean: -127.30 ± 30.30 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 14/30 | cfg: {'learning_rate': 8e-05, 'gamma': 0.97, 'gae_lambda': 0.97, 'n_steps': 1024, 'batch_size': 256, 'n_epochs': 15, 'clip_range': 0.3, 'ent_coef': 0.001, 'vf_coef': 0.3, 'max_grad_norm': 1.0, 'policy_kwargs': {'net_arch': [128, 128]}} ===\n",
      "Trial 14 -> eval_mean: -71.23 ± 30.46 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 15/30 | cfg: {'learning_rate': 0.0001, 'gamma': 0.99, 'gae_lambda': 0.95, 'n_steps': 512, 'batch_size': 64, 'n_epochs': 10, 'clip_range': 0.2, 'ent_coef': 0.001, 'vf_coef': 0.3, 'max_grad_norm': 0.5, 'policy_kwargs': {'net_arch': [256, 256]}} ===\n",
      "Trial 15 -> eval_mean: 1.74 ± 118.10 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 16/30 | cfg: {'learning_rate': 3e-05, 'gamma': 0.97, 'gae_lambda': 0.97, 'n_steps': 1024, 'batch_size': 64, 'n_epochs': 15, 'clip_range': 0.1, 'ent_coef': 0.001, 'vf_coef': 0.7, 'max_grad_norm': 1.0, 'policy_kwargs': {'net_arch': [256, 256]}} ===\n",
      "Trial 16 -> eval_mean: -70.57 ± 22.50 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 17/30 | cfg: {'learning_rate': 3e-05, 'gamma': 0.99, 'gae_lambda': 0.97, 'n_steps': 2048, 'batch_size': 128, 'n_epochs': 5, 'clip_range': 0.3, 'ent_coef': 0.001, 'vf_coef': 0.7, 'max_grad_norm': 1.0, 'policy_kwargs': {'net_arch': [128, 128]}} ===\n",
      "Trial 17 -> eval_mean: -1747.97 ± 803.66 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 18/30 | cfg: {'learning_rate': 5e-05, 'gamma': 0.995, 'gae_lambda': 0.97, 'n_steps': 1024, 'batch_size': 64, 'n_epochs': 5, 'clip_range': 0.1, 'ent_coef': 0.0, 'vf_coef': 0.5, 'max_grad_norm': 0.5, 'policy_kwargs': {'net_arch': [256, 128]}} ===\n",
      "Trial 18 -> eval_mean: -77.96 ± 26.82 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 19/30 | cfg: {'learning_rate': 3e-05, 'gamma': 0.98, 'gae_lambda': 0.9, 'n_steps': 512, 'batch_size': 256, 'n_epochs': 15, 'clip_range': 0.1, 'ent_coef': 0.001, 'vf_coef': 0.3, 'max_grad_norm': 0.5, 'policy_kwargs': {'net_arch': [128, 128]}} ===\n",
      "Trial 19 -> eval_mean: -162.47 ± 29.24 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 20/30 | cfg: {'learning_rate': 8e-05, 'gamma': 0.98, 'gae_lambda': 0.95, 'n_steps': 2048, 'batch_size': 128, 'n_epochs': 5, 'clip_range': 0.3, 'ent_coef': 0.001, 'vf_coef': 0.7, 'max_grad_norm': 2.0, 'policy_kwargs': {'net_arch': [256, 256]}} ===\n",
      "Trial 20 -> eval_mean: -122.12 ± 33.99 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 21/30 | cfg: {'learning_rate': 3e-05, 'gamma': 0.98, 'gae_lambda': 0.95, 'n_steps': 1024, 'batch_size': 64, 'n_epochs': 5, 'clip_range': 0.1, 'ent_coef': 0.01, 'vf_coef': 0.5, 'max_grad_norm': 1.0, 'policy_kwargs': {'net_arch': [64, 64]}} ===\n",
      "Trial 21 -> eval_mean: -719.83 ± 192.54 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 22/30 | cfg: {'learning_rate': 5e-05, 'gamma': 0.97, 'gae_lambda': 0.97, 'n_steps': 2048, 'batch_size': 256, 'n_epochs': 5, 'clip_range': 0.1, 'ent_coef': 0.01, 'vf_coef': 0.7, 'max_grad_norm': 1.0, 'policy_kwargs': {'net_arch': [64, 64]}} ===\n",
      "Trial 22 -> eval_mean: -4453.94 ± 1708.64 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 23/30 | cfg: {'learning_rate': 0.0002, 'gamma': 0.98, 'gae_lambda': 0.9, 'n_steps': 2048, 'batch_size': 128, 'n_epochs': 5, 'clip_range': 0.2, 'ent_coef': 0.001, 'vf_coef': 0.5, 'max_grad_norm': 1.0, 'policy_kwargs': {'net_arch': [128, 128]}} ===\n",
      "Trial 23 -> eval_mean: -71.87 ± 18.10 | BEST: 126.64\n",
      "\n",
      "=== PPO Trial 24/30 | cfg: {'learning_rate': 0.0001, 'gamma': 0.995, 'gae_lambda': 0.97, 'n_steps': 512, 'batch_size': 64, 'n_epochs': 15, 'clip_range': 0.3, 'ent_coef': 0.0, 'vf_coef': 0.3, 'max_grad_norm': 0.5, 'policy_kwargs': {'net_arch': [256, 256]}} ===\n",
      "Trial 24 -> eval_mean: 162.75 ± 91.99 | BEST: 162.75\n",
      "\n",
      "=== PPO Trial 25/30 | cfg: {'learning_rate': 0.0001, 'gamma': 0.995, 'gae_lambda': 0.95, 'n_steps': 512, 'batch_size': 128, 'n_epochs': 5, 'clip_range': 0.1, 'ent_coef': 0.01, 'vf_coef': 0.3, 'max_grad_norm': 1.0, 'policy_kwargs': {'net_arch': [64, 64]}} ===\n",
      "Trial 25 -> eval_mean: -102.88 ± 19.64 | BEST: 162.75\n",
      "\n",
      "=== PPO Trial 26/30 | cfg: {'learning_rate': 8e-05, 'gamma': 0.99, 'gae_lambda': 0.95, 'n_steps': 2048, 'batch_size': 256, 'n_epochs': 15, 'clip_range': 0.3, 'ent_coef': 0.01, 'vf_coef': 0.3, 'max_grad_norm': 0.5, 'policy_kwargs': {'net_arch': [64, 64]}} ===\n",
      "Trial 26 -> eval_mean: -0.33 ± 124.12 | BEST: 162.75\n",
      "\n",
      "=== PPO Trial 27/30 | cfg: {'learning_rate': 8e-05, 'gamma': 0.97, 'gae_lambda': 0.97, 'n_steps': 2048, 'batch_size': 256, 'n_epochs': 5, 'clip_range': 0.3, 'ent_coef': 0.005, 'vf_coef': 0.3, 'max_grad_norm': 0.5, 'policy_kwargs': {'net_arch': [128, 128]}} ===\n",
      "Trial 27 -> eval_mean: -115.09 ± 22.22 | BEST: 162.75\n",
      "\n",
      "=== PPO Trial 28/30 | cfg: {'learning_rate': 3e-05, 'gamma': 0.98, 'gae_lambda': 0.9, 'n_steps': 2048, 'batch_size': 64, 'n_epochs': 5, 'clip_range': 0.1, 'ent_coef': 0.0, 'vf_coef': 0.7, 'max_grad_norm': 0.5, 'policy_kwargs': {'net_arch': [64, 64]}} ===\n",
      "Trial 28 -> eval_mean: -1223.03 ± 166.33 | BEST: 162.75\n",
      "\n",
      "=== PPO Trial 29/30 | cfg: {'learning_rate': 5e-05, 'gamma': 0.98, 'gae_lambda': 0.97, 'n_steps': 2048, 'batch_size': 64, 'n_epochs': 15, 'clip_range': 0.1, 'ent_coef': 0.01, 'vf_coef': 0.7, 'max_grad_norm': 2.0, 'policy_kwargs': {'net_arch': [256, 256]}} ===\n",
      "Trial 29 -> eval_mean: 171.66 ± 90.28 | BEST: 171.66\n",
      "\n",
      "=== PPO Trial 30/30 | cfg: {'learning_rate': 3e-05, 'gamma': 0.99, 'gae_lambda': 0.9, 'n_steps': 2048, 'batch_size': 256, 'n_epochs': 10, 'clip_range': 0.1, 'ent_coef': 0.005, 'vf_coef': 0.5, 'max_grad_norm': 0.5, 'policy_kwargs': {'net_arch': [256, 128]}} ===\n",
      "Trial 30 -> eval_mean: -675.48 ± 57.22 | BEST: 171.66\n",
      "\n",
      "BEST PPO CFG:\n",
      " {'learning_rate': 5e-05, 'gamma': 0.98, 'gae_lambda': 0.97, 'n_steps': 2048, 'batch_size': 64, 'n_epochs': 15, 'clip_range': 0.1, 'ent_coef': 0.01, 'vf_coef': 0.7, 'max_grad_norm': 2.0, 'policy_kwargs': {'net_arch': [256, 256]}}\n",
      "BEST PPO eval: 171.65811157226562 ± 90.2822265625\n"
     ]
    }
   ],
   "source": [
    "ppo_trials, best_ppo = mc_tune_ppo(n_trials=30, tune_seed=42, train_seed=42, total_steps=TOTAL_STEPS)\n",
    "print(\"\\nBEST PPO CFG:\\n\", best_ppo[\"cfg\"])\n",
    "print(\"BEST PPO eval:\", best_ppo[\"eval_mean\"], \"±\", best_ppo[\"eval_std\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0c0f2",
   "metadata": {},
   "source": [
    "# 9. FINAL TRAINING (3 seeds) using best cfg + fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ffdab06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PPO FINAL seed 42 ===\n",
      "\n",
      "=== PPO FINAL seed 123 ===\n",
      "\n",
      "=== PPO FINAL seed 999 ===\n",
      "\n",
      "FINAL PPO (20 eps) mean across seeds: 188.22471618652344 | std across seeds: 18.200725422976788\n",
      "Gradient updates (per seed): [105120, 105120, 105120]\n"
     ]
    }
   ],
   "source": [
    "final_cfg = dict(best_ppo[\"cfg\"])\n",
    "\n",
    "ppo_final = []\n",
    "for s in FINAL_SEEDS:\n",
    "    print(\"\\n=== PPO FINAL seed\", s, \"===\")\n",
    "    ppo_final.append(run_ppo(final_cfg, seed=s, run_name=\"ppo_final\", total_steps=TOTAL_STEPS, eval_episodes=EVAL_EPISODES))\n",
    "\n",
    "means = [r[\"eval_mean\"] for r in ppo_final]\n",
    "print(\"\\nFINAL PPO (20 eps) mean across seeds:\", float(np.mean(means)), \"| std across seeds:\", float(np.std(means, ddof=1)))\n",
    "print(\"Gradient updates (per seed):\", [r[\"grad_updates\"] for r in ppo_final])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ad6d6",
   "metadata": {},
   "source": [
    "# 10. DIFFERENCIATOR: stress-test wind (evaluation only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46b4f14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PPO WIND-EVAL seed 42 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ricar\\dev\\iscte\\taap_p2\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\vec_monitor.py:44: UserWarning: The environment is already wrapped with a `Monitor` wrapperbut you are wrapping it with a `VecMonitor` wrapper, the `Monitor` statistics will beoverwritten by the `VecMonitor` ones.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PPO WIND-EVAL seed 123 ===\n",
      "\n",
      "=== PPO WIND-EVAL seed 999 ===\n",
      "\n",
      "WIND-EVAL PPO mean across seeds: 112.46104939778645 | std: 21.85254123618533\n"
     ]
    }
   ],
   "source": [
    "ppo_wind = []\n",
    "for s in FINAL_SEEDS:\n",
    "    print(\"\\n=== PPO WIND-EVAL seed\", s, \"===\")\n",
    "    ppo_wind.append(run_ppo(final_cfg, seed=s, run_name=\"ppo_final_wind\", total_steps=TOTAL_STEPS,\n",
    "                            eval_episodes=EVAL_EPISODES, wind_eval=True))\n",
    "\n",
    "means_w = [r[\"eval_mean\"] for r in ppo_wind]\n",
    "print(\"\\nWIND-EVAL PPO mean across seeds:\", float(np.mean(means_w)), \"| std:\", float(np.std(means_w, ddof=1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ede2c",
   "metadata": {},
   "source": [
    "# 11. SUMMARY TABLE (clean reporting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c08b010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PPO FINAL (no wind, deterministic eval)\n",
      "seed   42 | eval_mean±std: 171.66 ± 90.28 | grad_updates: 105120\n",
      "seed  123 | eval_mean±std: 207.71 ± 60.09 | grad_updates: 105120\n",
      "seed  999 | eval_mean±std: 185.31 ± 85.90 | grad_updates: 105120\n",
      "Across seeds: mean=188.22 | std=18.20\n",
      "\n",
      "PPO STRESS-TEST (wind in eval only)\n",
      "seed   42 | eval_mean±std: 87.49 ± 130.91 | grad_updates: 105120\n",
      "seed  123 | eval_mean±std: 128.08 ± 117.93 | grad_updates: 105120\n",
      "seed  999 | eval_mean±std: 121.81 ± 139.12 | grad_updates: 105120\n",
      "Across seeds: mean=112.46 | std=21.85\n"
     ]
    }
   ],
   "source": [
    "def summarize(results, title):\n",
    "    means = [r[\"eval_mean\"] for r in results]\n",
    "    stds  = [r[\"eval_std\"]  for r in results]\n",
    "    print(\"\\n\" + title)\n",
    "    for r in results:\n",
    "        print(f\"seed {r['seed']:>4d} | eval_mean±std: {r['eval_mean']:.2f} ± {r['eval_std']:.2f} | grad_updates: {r['grad_updates']}\")\n",
    "    print(f\"Across seeds: mean={np.mean(means):.2f} | std={np.std(means, ddof=1):.2f}\")\n",
    "\n",
    "summarize(ppo_final, \"PPO FINAL (no wind, deterministic eval)\")\n",
    "summarize(ppo_wind,  \"PPO STRESS-TEST (wind in eval only)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taap_p2 (3.10.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
