{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN & PPO Multi-Seed Report\n",
    "\n",
    "This notebook loads pre-trained models (one per algorithm per seed) and runs evaluation and visualization.\n",
    "No training is required — run lab006_v1.ipynb first to generate the model files.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import imageio\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "SEED_LIST = [42, 123, 3407]\n",
    "\n",
    "ALGORITHM_MAP = {\n",
    "    \"dqn\": DQN,\n",
    "    \"ppo\": PPO,\n",
    "}\n",
    "\n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "GYMNASIUM_MODEL = \"LunarLander-v3\"\n",
    "\n",
    "WIND_ENABLED = False\n",
    "\n",
    "# EVALUATION_EPISODES = 20\n",
    "\n",
    "# Code smoke test\n",
    "EVALUATION_EPISODES = 2\n",
    "\n",
    "\n",
    "TRAJECTORY_EPISODES = 3  # Episodes to visualize per algorithm for trajectory plots\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Model file prefix (must match lab006_v1.ipynb save names)\n",
    "MODEL_PREFIX = \"lab006\"\n",
    "\n",
    "# LunarLander-v3 action labels\n",
    "ACTION_LABELS = [\"Do Nothing\", \"Fire Left\", \"Fire Main\", \"Fire Right\"]\n",
    "\n",
    "print(f\"Algorithms: {list(ALGORITHM_MAP.keys())}\")\n",
    "print(f\"Seeds: {SEED_LIST}\")\n",
    "print(f\"Wind enabled: {WIND_ENABLED}\")\n",
    "print(f\"Evaluation episodes per seed: {EVALUATION_EPISODES}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all models and evaluate\n",
    "\n",
    "evaluation_results = {}  # {algo: {seed: np.array}}\n",
    "\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "    models_dir = os.path.join(NOTEBOOK_DIR, \"../../../models\", algo_name)\n",
    "    evaluation_results[algo_name] = {}\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        print(f\"Loading and evaluating {algo_name.upper()} seed {seed}...\")\n",
    "\n",
    "        load_path = os.path.join(models_dir, f\"{MODEL_PREFIX}_{algo_name}_{seed}\")\n",
    "\n",
    "        def make_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        model = algo_class.load(load_path, env=DummyVecEnv([make_env]), device=DEVICE)\n",
    "\n",
    "        eval_env = Monitor(gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED))\n",
    "        eval_env.reset(seed=seed)\n",
    "\n",
    "        rewards, _ = evaluate_policy(\n",
    "            model,\n",
    "            eval_env,\n",
    "            n_eval_episodes=EVALUATION_EPISODES,\n",
    "            deterministic=True,\n",
    "            return_episode_rewards=True\n",
    "        )\n",
    "\n",
    "        evaluation_results[algo_name][seed] = np.array(rewards)\n",
    "        eval_env.close()\n",
    "\n",
    "    print(f\"{algo_name.upper()}: evaluation complete.\\n\")\n",
    "\n",
    "print(f\"All evaluations complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Per-Algorithm Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm: Evaluation Summary Tables\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    rows = []\n",
    "    for seed in SEED_LIST:\n",
    "        r = evaluation_results[algo_name][seed]\n",
    "        rows.append({\n",
    "            \"Seed\": seed,\n",
    "            \"Mean Reward\": f\"{np.mean(r):.2f}\",\n",
    "            \"Std Dev\": f\"{np.std(r):.2f}\",\n",
    "            \"Min Reward\": f\"{np.min(r):.2f}\",\n",
    "            \"Max Reward\": f\"{np.max(r):.2f}\",\n",
    "            \"Success Rate\": f\"{(r >= 200).sum() / len(r) * 100:.1f}%\"\n",
    "        })\n",
    "\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "    rows.append({\n",
    "        \"Seed\": \"Overall\",\n",
    "        \"Mean Reward\": f\"{np.mean(all_r):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(all_r):.2f}\",\n",
    "        \"Min Reward\": f\"{np.min(all_r):.2f}\",\n",
    "        \"Max Reward\": f\"{np.max(all_r):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_r >= 200).sum() / len(all_r) * 100:.1f}%\"\n",
    "    })\n",
    "\n",
    "    print(f\"*** {algo_name.upper()} MULTI-SEED EVALUATION SUMMARY ***\")\n",
    "    print(f\"Episodes per seed: {EVALUATION_EPISODES} | Total: {len(all_r)}\")\n",
    "    print(pd.DataFrame(rows).to_string(index=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm, Per-Seed: Evaluation Convergence Plots\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "    if len(SEED_LIST) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, seed in zip(axes, SEED_LIST):\n",
    "        rewards = evaluation_results[algo_name][seed]\n",
    "        episodes = np.arange(1, len(rewards) + 1)\n",
    "        running_mean = np.cumsum(rewards) / episodes\n",
    "        running_std = np.array([np.std(rewards[:i]) for i in episodes])\n",
    "\n",
    "        ax.scatter(episodes, rewards, color='gray', alpha=0.4, s=20, label='Episode Reward')\n",
    "        ax.plot(episodes, running_mean, color='blue', linewidth=2, label='Running Mean')\n",
    "        ax.fill_between(episodes, running_mean - running_std, running_mean + running_std,\n",
    "                        color='blue', alpha=0.15)\n",
    "        ax.axhline(y=200, color='red', linestyle='--')\n",
    "        ax.set_title(f\"Seed {seed}\")\n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"Reward\")\n",
    "    fig.suptitle(f\"{algo_name.upper()} Evaluation: {EVALUATION_EPISODES} Episodes per Seed\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm: Evaluation Bar Chart (mean reward per seed with error bars)\n",
    "\n",
    "seed_colors = list(plt.colormaps[\"tab10\"](range(10)))  # type: ignore[arg-type]\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "    means = [np.mean(evaluation_results[algo_name][s]) for s in SEED_LIST]\n",
    "    stds = [np.std(evaluation_results[algo_name][s]) for s in SEED_LIST]\n",
    "    labels = [str(s) for s in SEED_LIST]\n",
    "\n",
    "    plt.figure(figsize=(max(8, 3 * len(SEED_LIST)), 6))\n",
    "    plt.bar(labels, means, yerr=stds, capsize=5, color=seed_colors[:len(SEED_LIST)], alpha=0.8)\n",
    "    plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "    plt.axhline(y=float(np.mean(all_r)), color='blue', linestyle='-', linewidth=2,\n",
    "                label=f'Overall Mean ({np.mean(all_r):.1f})')\n",
    "\n",
    "    plt.title(f\"{algo_name.upper()} Mean Reward per Seed ({EVALUATION_EPISODES} episodes each)\", fontsize=14)\n",
    "    plt.xlabel(\"Seed\")\n",
    "    plt.ylabel(\"Mean Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm: Reward Distribution Histograms (overlaid per seed)\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i, seed in enumerate(SEED_LIST):\n",
    "        plt.hist(evaluation_results[algo_name][seed], bins=10, alpha=0.5,\n",
    "                 color=seed_colors[i], edgecolor='black', label=f\"Seed {seed}\")\n",
    "\n",
    "    plt.axvline(x=float(np.mean(all_r)), color='blue', linestyle='-', linewidth=2,\n",
    "                label=f'Overall Mean ({np.mean(all_r):.1f})')\n",
    "    plt.axvline(x=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "    plt.title(f'{algo_name.upper()} Reward Distribution across Seeds')\n",
    "    plt.xlabel('Reward')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cross-Algorithm Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm: Combined Summary Table\n",
    "\n",
    "rows = []\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "    rows.append({\n",
    "        \"Algorithm\": algo_name.upper(),\n",
    "        \"Mean Reward\": f\"{np.mean(all_r):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(all_r):.2f}\",\n",
    "        \"Min Reward\": f\"{np.min(all_r):.2f}\",\n",
    "        \"Max Reward\": f\"{np.max(all_r):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_r >= 200).sum() / len(all_r) * 100:.1f}%\"\n",
    "    })\n",
    "\n",
    "print(f\"*** CROSS-ALGORITHM EVALUATION SUMMARY ***\")\n",
    "print(f\"Seeds: {SEED_LIST} | Episodes per seed: {EVALUATION_EPISODES}\")\n",
    "print(f\"Total episodes per algorithm: {EVALUATION_EPISODES * len(SEED_LIST)}\")\n",
    "print()\n",
    "print(pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm: Grouped Bar Chart (DQN vs PPO per seed)\n",
    "\n",
    "algo_names = list(ALGORITHM_MAP.keys())\n",
    "n_algos = len(algo_names)\n",
    "n_seeds = len(SEED_LIST)\n",
    "bar_width = 0.8 / n_algos\n",
    "x = np.arange(n_seeds)\n",
    "\n",
    "plt.figure(figsize=(max(10, 3 * n_seeds), 6))\n",
    "for i, algo_name in enumerate(algo_names):\n",
    "    means = [np.mean(evaluation_results[algo_name][s]) for s in SEED_LIST]\n",
    "    stds = [np.std(evaluation_results[algo_name][s]) for s in SEED_LIST]\n",
    "    offset = (i - (n_algos - 1) / 2) * bar_width\n",
    "    plt.bar(x + offset, means, bar_width, yerr=stds, capsize=4,\n",
    "            label=algo_name.upper(), alpha=0.8)\n",
    "\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.xticks(x, [str(s) for s in SEED_LIST])\n",
    "plt.title(f\"DQN vs PPO: Mean Reward per Seed ({EVALUATION_EPISODES} episodes each)\", fontsize=14)\n",
    "plt.xlabel(\"Seed\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm: Overall Mean Reward Bar Chart\n",
    "\n",
    "algo_colors = {\"dqn\": \"tab:blue\", \"ppo\": \"tab:orange\"}\n",
    "\n",
    "overall_means = []\n",
    "overall_stds = []\n",
    "for algo_name in algo_names:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "    overall_means.append(np.mean(all_r))\n",
    "    overall_stds.append(np.std(all_r))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar([a.upper() for a in algo_names], overall_means, yerr=overall_stds,\n",
    "               capsize=6, color=[algo_colors[a] for a in algo_names], alpha=0.8)\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "\n",
    "for bar, mean in zip(bars, overall_means):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5,\n",
    "             f'{mean:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.title(f\"Overall Mean Reward: DQN vs PPO ({EVALUATION_EPISODES * len(SEED_LIST)} episodes each)\", fontsize=14)\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm: Reward Distribution Comparison (overlaid histograms)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for algo_name in algo_names:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "    plt.hist(all_r, bins=15, alpha=0.5, color=algo_colors[algo_name],\n",
    "             edgecolor='black', label=f\"{algo_name.upper()} (mean={np.mean(all_r):.1f})\")\n",
    "\n",
    "plt.axvline(x=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.title('Reward Distribution: DQN vs PPO (all seeds combined)', fontsize=14)\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm: Box Plot Comparison per Seed\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    data = [evaluation_results[algo_name][seed] for algo_name in algo_names]\n",
    "    bp = ax.boxplot(data, labels=[a.upper() for a in algo_names], patch_artist=True)\n",
    "    for patch, algo_name in zip(bp['boxes'], algo_names):\n",
    "        patch.set_facecolor(algo_colors[algo_name])\n",
    "        patch.set_alpha(0.6)\n",
    "    ax.axhline(y=200, color='red', linestyle='--')\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Reward\")\n",
    "fig.suptitle(f\"DQN vs PPO: Reward Distribution per Seed ({EVALUATION_EPISODES} episodes each)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Significance: Mann-Whitney U Tests\n",
    "\n",
    "algo_names = list(ALGORITHM_MAP.keys())\n",
    "\n",
    "# Gather all rewards per algorithm\n",
    "algo_all_rewards = {}\n",
    "for algo_name in algo_names:\n",
    "    algo_all_rewards[algo_name] = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "\n",
    "# --- Reward comparison (Mann-Whitney U) ---\n",
    "mwu_result = stats.mannwhitneyu(\n",
    "    algo_all_rewards[algo_names[0]],\n",
    "    algo_all_rewards[algo_names[1]],\n",
    "    alternative='two-sided'\n",
    ")\n",
    "stat_reward = float(mwu_result.statistic)\n",
    "p_reward = float(mwu_result.pvalue)\n",
    "\n",
    "# --- Success rate comparison (Chi-squared) ---\n",
    "successes = []\n",
    "totals = []\n",
    "for algo_name in algo_names:\n",
    "    r = algo_all_rewards[algo_name]\n",
    "    successes.append(int((r >= 200).sum()))\n",
    "    totals.append(len(r))\n",
    "\n",
    "failures = [t - s for t, s in zip(totals, successes)]\n",
    "contingency = np.array([successes, failures])\n",
    "\n",
    "# Chi-squared requires all expected frequencies > 0; skip if any row/col is all-zero\n",
    "if np.all(contingency.sum(axis=1) > 0) and np.all(contingency.sum(axis=0) > 0):\n",
    "    chi2_result = stats.chi2_contingency(contingency)\n",
    "    chi2 = float(chi2_result[0])      # type: ignore[arg-type]  # statistic\n",
    "    p_success = float(chi2_result[1])  # type: ignore[arg-type]  # pvalue\n",
    "    chi2_valid = True\n",
    "else:\n",
    "    chi2, p_success = 0.0, 1.0\n",
    "    chi2_valid = False\n",
    "\n",
    "# --- Results table ---\n",
    "chi2_note = \"\" if chi2_valid else \" (skipped: zero row/col)\"\n",
    "rows = [\n",
    "    {\n",
    "        \"Metric\": \"Mean Reward\",\n",
    "        f\"{algo_names[0].upper()} Value\": f\"{np.mean(algo_all_rewards[algo_names[0]]):.2f}\",\n",
    "        f\"{algo_names[1].upper()} Value\": f\"{np.mean(algo_all_rewards[algo_names[1]]):.2f}\",\n",
    "        \"Test\": \"Mann-Whitney U\",\n",
    "        \"Statistic\": f\"{stat_reward:.1f}\",\n",
    "        \"p-value\": f\"{p_reward:.4f}\",\n",
    "        \"Significant (p<0.05)\": \"Yes\" if p_reward < 0.05 else \"No\"\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Success Rate (>=200)\",\n",
    "        f\"{algo_names[0].upper()} Value\": f\"{successes[0]/totals[0]*100:.1f}%\",\n",
    "        f\"{algo_names[1].upper()} Value\": f\"{successes[1]/totals[1]*100:.1f}%\",\n",
    "        \"Test\": f\"Chi-squared{chi2_note}\",\n",
    "        \"Statistic\": f\"{chi2:.2f}\",\n",
    "        \"p-value\": f\"{p_success:.4f}\",\n",
    "        \"Significant (p<0.05)\": \"Yes\" if (chi2_valid and p_success < 0.05) else \"No\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"*** STATISTICAL SIGNIFICANCE TESTS ***\")\n",
    "print(f\"Sample size per algorithm: {totals[0]} episodes ({EVALUATION_EPISODES} episodes x {len(SEED_LIST)} seeds)\")\n",
    "print()\n",
    "print(pd.DataFrame(rows).to_string(index=False))\n",
    "print()\n",
    "if not chi2_valid:\n",
    "    print(\"Note: Chi-squared test skipped because one or both algorithms had 0% or 100% success rate.\")\n",
    "    print(\"      This is expected during smoke tests with few episodes. Full run will have enough data.\")\n",
    "    print()\n",
    "if p_reward < 0.05:\n",
    "    print(f\"The reward difference between {algo_names[0].upper()} and {algo_names[1].upper()} is statistically significant (p={p_reward:.4f}).\")\n",
    "else:\n",
    "    print(f\"No statistically significant reward difference between {algo_names[0].upper()} and {algo_names[1].upper()} (p={p_reward:.4f}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Agent Baseline Evaluation\n",
    "\n",
    "random_results = {}\n",
    "\n",
    "for seed in SEED_LIST:\n",
    "    print(f\"Running random agent with seed {seed}...\")\n",
    "    env = gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED)\n",
    "    env.action_space.seed(seed)\n",
    "    episode_rewards = []\n",
    "\n",
    "    for ep in range(EVALUATION_EPISODES):\n",
    "        obs, info = env.reset(seed=seed + ep)\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += float(reward)\n",
    "            done = terminated or truncated\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "    random_results[seed] = np.array(episode_rewards)\n",
    "    env.close()\n",
    "\n",
    "print(\"Random baseline evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Comparison: Table + Chart\n",
    "\n",
    "algo_names = list(ALGORITHM_MAP.keys())\n",
    "algo_colors = {\"dqn\": \"tab:blue\", \"ppo\": \"tab:orange\"}\n",
    "algo_all_rewards = {a: np.concatenate([evaluation_results[a][s] for s in SEED_LIST]) for a in algo_names}\n",
    "\n",
    "all_random = np.concatenate([random_results[s] for s in SEED_LIST])\n",
    "\n",
    "rows = [\n",
    "    {\n",
    "        \"Agent\": \"Random\",\n",
    "        \"Mean Reward\": f\"{np.mean(all_random):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(all_random):.2f}\",\n",
    "        \"Min\": f\"{np.min(all_random):.2f}\",\n",
    "        \"Max\": f\"{np.max(all_random):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_random >= 200).sum() / len(all_random) * 100:.1f}%\"\n",
    "    }\n",
    "]\n",
    "for algo_name in algo_names:\n",
    "    all_r = algo_all_rewards[algo_name]\n",
    "    rows.append({\n",
    "        \"Agent\": algo_name.upper(),\n",
    "        \"Mean Reward\": f\"{np.mean(all_r):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(all_r):.2f}\",\n",
    "        \"Min\": f\"{np.min(all_r):.2f}\",\n",
    "        \"Max\": f\"{np.max(all_r):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_r >= 200).sum() / len(all_r) * 100:.1f}%\"\n",
    "    })\n",
    "rows.append({\n",
    "    \"Agent\": \"Human (ref)\",\n",
    "    \"Mean Reward\": \"~200-300\",\n",
    "    \"Std Dev\": \"-\",\n",
    "    \"Min\": \"-\",\n",
    "    \"Max\": \"-\",\n",
    "    \"Success Rate\": \"~100%\"\n",
    "})\n",
    "\n",
    "print(\"*** BASELINE COMPARISON ***\")\n",
    "print(pd.DataFrame(rows).to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Bar chart\n",
    "agent_labels = [\"Random\"] + [a.upper() for a in algo_names]\n",
    "agent_means = [np.mean(all_random)] + [np.mean(algo_all_rewards[a]) for a in algo_names]\n",
    "agent_stds = [np.std(all_random)] + [np.std(algo_all_rewards[a]) for a in algo_names]\n",
    "bar_colors = [\"gray\"] + [algo_colors[a] for a in algo_names]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(agent_labels, agent_means, yerr=agent_stds, capsize=6,\n",
    "               color=bar_colors, alpha=0.8)\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "\n",
    "for bar, mean in zip(bars, agent_means):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5,\n",
    "             f'{mean:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.title(f\"Baseline Comparison: Random vs DQN vs PPO ({EVALUATION_EPISODES * len(SEED_LIST)} episodes each)\", fontsize=14)\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Agent Behavior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect per-step data: actions and trajectories\n",
    "\n",
    "action_counts = {}       # {algo: np.array of shape (4,)} total action counts\n",
    "trajectory_data = {}     # {algo: list of (x_positions, y_positions)} one per TRAJECTORY_EPISODES\n",
    "\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "    models_dir = os.path.join(NOTEBOOK_DIR, \"../../../models\", algo_name)\n",
    "    action_counts[algo_name] = np.zeros(len(ACTION_LABELS), dtype=int)\n",
    "    trajectory_data[algo_name] = []\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        load_path = os.path.join(models_dir, f\"{MODEL_PREFIX}_{algo_name}_{seed}\")\n",
    "\n",
    "        def make_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        model = algo_class.load(load_path, env=DummyVecEnv([make_env]), device=DEVICE)\n",
    "        env = gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED)\n",
    "\n",
    "        for ep in range(EVALUATION_EPISODES):\n",
    "            obs, info = env.reset(seed=seed + ep)\n",
    "            done = False\n",
    "            x_pos, y_pos = [obs[0]], [obs[1]]\n",
    "\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                action_int = int(action)\n",
    "                action_counts[algo_name][action_int] += 1\n",
    "\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                x_pos.append(obs[0])\n",
    "                y_pos.append(obs[1])\n",
    "\n",
    "            # Keep trajectory for the first TRAJECTORY_EPISODES episodes of the first seed\n",
    "            if seed == SEED_LIST[0] and ep < TRAJECTORY_EPISODES:\n",
    "                trajectory_data[algo_name].append((np.array(x_pos), np.array(y_pos)))\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    total_actions = action_counts[algo_name].sum()\n",
    "    print(f\"{algo_name.upper()}: {total_actions:,} total actions collected across {EVALUATION_EPISODES * len(SEED_LIST)} episodes\")\n",
    "\n",
    "print(\"\\nBehavior data collection complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Distribution: DQN vs PPO\n",
    "\n",
    "n_actions = len(ACTION_LABELS)\n",
    "x = np.arange(n_actions)\n",
    "bar_width = 0.35\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Absolute counts\n",
    "for i, algo_name in enumerate(algo_names):\n",
    "    offset = (i - 0.5) * bar_width\n",
    "    ax1.bar(x + offset, action_counts[algo_name], bar_width,\n",
    "            label=algo_name.upper(), color=algo_colors[algo_name], alpha=0.8)\n",
    "\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(ACTION_LABELS, rotation=15)\n",
    "ax1.set_title(\"Action Counts (Absolute)\", fontsize=13)\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Percentage distribution\n",
    "for i, algo_name in enumerate(algo_names):\n",
    "    pcts = action_counts[algo_name] / action_counts[algo_name].sum() * 100\n",
    "    offset = (i - 0.5) * bar_width\n",
    "    bars = ax2.bar(x + offset, pcts, bar_width,\n",
    "                   label=algo_name.upper(), color=algo_colors[algo_name], alpha=0.8)\n",
    "    for bar, pct in zip(bars, pcts):\n",
    "        ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,\n",
    "                 f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(ACTION_LABELS, rotation=15)\n",
    "ax2.set_title(\"Action Distribution (Percentage)\", fontsize=13)\n",
    "ax2.set_ylabel(\"Percentage (%)\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "fig.suptitle(f\"DQN vs PPO: Action Distribution ({EVALUATION_EPISODES * len(SEED_LIST)} episodes each)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory Plots: x-y paths of the lander\n",
    "\n",
    "fig, axes = plt.subplots(1, len(algo_names), figsize=(8 * len(algo_names), 6), sharey=True)\n",
    "if len(algo_names) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "traj_colors = list(plt.colormaps[\"Set2\"](range(8)))  # type: ignore[arg-type]\n",
    "\n",
    "for ax, algo_name in zip(axes, algo_names):\n",
    "    for i, (x_pos, y_pos) in enumerate(trajectory_data[algo_name]):\n",
    "        ax.plot(x_pos, y_pos, color=traj_colors[i], linewidth=1.5, alpha=0.8,\n",
    "                label=f\"Episode {i+1}\")\n",
    "        ax.scatter(x_pos[0], y_pos[0], color=traj_colors[i], marker='o', s=60, zorder=5)\n",
    "        ax.scatter(x_pos[-1], y_pos[-1], color=traj_colors[i], marker='x', s=80, zorder=5)\n",
    "\n",
    "    # Landing pad reference\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    ax.scatter(0, 0, color='red', marker='^', s=120, zorder=10, label='Landing Pad')\n",
    "\n",
    "    ax.set_title(f\"{algo_name.upper()} Trajectories (seed {SEED_LIST[0]})\", fontsize=13)\n",
    "    ax.set_xlabel(\"X Position\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Y Position\")\n",
    "fig.suptitle(f\"Lander Trajectories: DQN vs PPO ({TRAJECTORY_EPISODES} episodes each)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory Comparison: DQN vs PPO overlaid on one chart\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for algo_name in algo_names:\n",
    "    # Plot the first trajectory from each algorithm\n",
    "    x_pos, y_pos = trajectory_data[algo_name][0]\n",
    "    plt.plot(x_pos, y_pos, color=algo_colors[algo_name], linewidth=2, alpha=0.8,\n",
    "             label=f\"{algo_name.upper()}\")\n",
    "    plt.scatter(x_pos[0], y_pos[0], color=algo_colors[algo_name], marker='o', s=80, zorder=5)\n",
    "    plt.scatter(x_pos[-1], y_pos[-1], color=algo_colors[algo_name], marker='x', s=100, zorder=5)\n",
    "\n",
    "plt.scatter(0, 0, color='red', marker='^', s=150, zorder=10, label='Landing Pad')\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "plt.title(f\"DQN vs PPO: Landing Trajectory Comparison (seed {SEED_LIST[0]}, episode 1)\", fontsize=14)\n",
    "plt.xlabel(\"X Position\")\n",
    "plt.ylabel(\"Y Position\")\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GIF Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIF Visualizations (one per algorithm per seed)\n",
    "\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "    models_dir = os.path.join(NOTEBOOK_DIR, \"../../../models\", algo_name)\n",
    "    output_dir = os.path.join(NOTEBOOK_DIR, \"outputs_\" + algo_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        print(f\"Generating GIF for {algo_name.upper()} seed {seed}...\")\n",
    "\n",
    "        load_path = os.path.join(models_dir, f\"{MODEL_PREFIX}_{algo_name}_{seed}\")\n",
    "\n",
    "        def make_vis_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        vis_model = algo_class.load(load_path, env=DummyVecEnv([make_vis_env]), device=DEVICE)\n",
    "\n",
    "        vis_env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "        frames = []\n",
    "        obs, info = vis_env.reset(seed=seed)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _ = vis_model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = vis_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            frames.append(vis_env.render())\n",
    "\n",
    "        vis_env.close()\n",
    "\n",
    "        gif_path = os.path.join(output_dir, f\"{algo_name}_seed{seed}.gif\")\n",
    "        imageio.mimsave(gif_path, frames, fps=30)\n",
    "        print(f\"  Saved: {gif_path}\")\n",
    "        display(Image(filename=gif_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Experimental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Details\n",
    "\n",
    "| Property | Value |\n",
    "|---|---|\n",
    "| Environment | `LunarLander-v3` (Gymnasium) |\n",
    "| Observation Space | `Box(8,)` — continuous 8-dimensional vector |\n",
    "| Action Space | `Discrete(4)` — do nothing, fire left, fire main, fire right |\n",
    "| Solved Threshold | Mean reward >= 200 over 100 consecutive episodes |\n",
    "| Wind | Disabled (`enable_wind=False`) |\n",
    "\n",
    "**Observation vector:** `[x, y, vx, vy, angle, angular_velocity, left_leg_contact, right_leg_contact]`\n",
    "\n",
    "**Reward structure:**\n",
    "- Moving toward the landing pad: positive\n",
    "- Moving away: negative\n",
    "- Crash: -100\n",
    "- Successful landing: +100\n",
    "- Each leg ground contact: +10\n",
    "- Firing main engine: -0.3 per frame\n",
    "- Firing side engine: -0.03 per frame\n",
    "\n",
    "**Termination rules:**\n",
    "- **Terminated (success):** The lander comes to rest on the ground with both legs in contact, near-zero velocity\n",
    "- **Terminated (crash):** The lander body contacts the ground, or the lander moves outside the viewport boundaries\n",
    "- **Truncated (timeout):** The episode exceeds 1000 timesteps without termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment inspection\n",
    "\n",
    "env_tmp = gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED)\n",
    "print(f\"Environment: {GYMNASIUM_MODEL}\")\n",
    "print(f\"Observation space: {env_tmp.observation_space}\")\n",
    "print(f\"Action space: {env_tmp.action_space}\")\n",
    "print(f\"Wind enabled: {WIND_ENABLED}\")\n",
    "\n",
    "obs, info = env_tmp.reset(seed=42)\n",
    "print(f\"\\nSample observation: {obs}\")\n",
    "print(f\"Observation labels: [x, y, vx, vy, angle, angular_vel, left_leg, right_leg]\")\n",
    "env_tmp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and library versions\n",
    "\n",
    "import stable_baselines3\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Stable-Baselines3: {stable_baselines3.__version__}\")\n",
    "print(f\"Gymnasium: {gym.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"CUDA: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_taap_p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
