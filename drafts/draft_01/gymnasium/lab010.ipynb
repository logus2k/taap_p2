{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# DQN & PPO Multi-Seed Training with Checkpointing & Best-Model Selection\n",
    "\n",
    "This notebook trains both DQN and PPO across multiple seeds, with:\n",
    "- **Periodic checkpoints** saved every N episodes\n",
    "- **Best-model tracking** using a combined metric (mean reward - std reward)\n",
    "- **Timestamped run folders** for organized model storage\n",
    "\n",
    "Evaluates each model and produces per-algorithm, per-seed, and aggregated comparison charts.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random, time\n",
    "from datetime import datetime\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import imageio\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Configuration\n",
    "\n",
    "SEED_LIST = [42, 123, 999]\n",
    "\n",
    "ALGORITHM_MAP = {\n",
    "    \"dqn\": DQN,\n",
    "    \"ppo\": PPO,\n",
    "}\n",
    "\n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "GYMNASIUM_MODEL = \"LunarLander-v3\"\n",
    "MLP_POLICY = \"MlpPolicy\"\n",
    "\n",
    "WIND_ENABLED = False\n",
    "\n",
    "TOTAL_TIMESTEPS = 1_500_000\n",
    "EVALUATION_EPISODES = 20\n",
    "\n",
    "# Session prefix — used in final model filenames (e.g. lab010_dqn_42.zip)\n",
    "SESSION_PREFIX = \"lab010\"\n",
    "\n",
    "# Update live stats and plots every N episodes\n",
    "CHART_UPDATE_FREQ = 10\n",
    "\n",
    "# Save a training checkpoint every N episodes\n",
    "CHECKPOINT_FREQ_EPISODES = 100\n",
    "\n",
    "# EvalCallback: evaluate the model every N timesteps\n",
    "EVAL_FREQ_TIMESTEPS = 25_000\n",
    "\n",
    "# EvalCallback: number of episodes per evaluation\n",
    "EVAL_N_EPISODES = 20\n",
    "\n",
    "# Solved threshold: only prefer models with mean reward >= this value\n",
    "SOLVED_THRESHOLD = 200\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress decreases from 1 (beginning) to 0 (end)\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "    return func\n",
    "\n",
    "\n",
    "# Per-algorithm hyperparameters\n",
    "ALGO_PARAMS = {\n",
    "    \"dqn\": {\n",
    "        \"policy\": MLP_POLICY,\n",
    "        \n",
    "        # Linear Schedule allows weights to settle perfectly at the end\n",
    "        \"learning_rate\": linear_schedule(6.3e-4), \n",
    "        \n",
    "        \"learning_starts\": 50_000,\n",
    "        \n",
    "        # Massive buffer prevents forgetting recovery maneuvers\n",
    "        \"buffer_size\": 750_000, \n",
    "        \"batch_size\": 128,\n",
    "        \"gamma\": 0.99,\n",
    "        \n",
    "        \"exploration_fraction\": 0.12,  \n",
    "        \n",
    "        # High final epsilon forces the agent to keep learning recoveries\n",
    "        \"exploration_final_eps\": 0.1,  \n",
    "        \n",
    "        # Standard Zoo update mechanics\n",
    "        \"target_update_interval\": 250, \n",
    "        \"train_freq\": 4,\n",
    "        # Takes 4 gradient updates every 4 env steps\n",
    "        \"gradient_steps\": 4,\n",
    "        \n",
    "        \"policy_kwargs\": dict(net_arch=[256, 256]),\n",
    "        \"device\": DEVICE,\n",
    "    },\n",
    "    \"ppo\": {\n",
    "        \"learning_rate\": 2.5e-4,\n",
    "        \"n_steps\": 2048,\n",
    "        \"batch_size\": 64,\n",
    "        \"n_epochs\": 10,\n",
    "        \"gamma\": 0.999,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"clip_range\": 0.2,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Session prefix: {SESSION_PREFIX}\")\n",
    "print(f\"Algorithms: {list(ALGORITHM_MAP.keys())}\")\n",
    "print(f\"Seeds: {SEED_LIST}\")\n",
    "print(f\"Wind enabled: {WIND_ENABLED}\")\n",
    "print(f\"Total timesteps per seed: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"Evaluation episodes per seed: {EVALUATION_EPISODES}\")\n",
    "print(f\"Chart update frequency: every {CHART_UPDATE_FREQ} episodes\")\n",
    "print(f\"Checkpoint frequency: every {CHECKPOINT_FREQ_EPISODES} episodes\")\n",
    "print(f\"Eval callback frequency: every {EVAL_FREQ_TIMESTEPS:,} timesteps ({EVAL_N_EPISODES} episodes)\")\n",
    "print(f\"Solved threshold: {SOLVED_THRESHOLD}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"CUDA:\", torch.version.cuda if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment inspection (run once)\n",
    "env_tmp = gym.make(GYMNASIUM_MODEL)\n",
    "\n",
    "print(\"Observation space:\", env_tmp.observation_space)\n",
    "print(\"Action space:\", env_tmp.action_space)\n",
    "\n",
    "obs, info = env_tmp.reset()\n",
    "print(\"Initial observation:\", obs)\n",
    "\n",
    "env_tmp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, checkpoint_path=None, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.value_loss = []\n",
    "        self.entropy = []\n",
    "        self.mean_q_values = []      # Track Q-value overestimation\n",
    "        self.gradient_updates = 0    # Count gradient updates\n",
    "\n",
    "        self._current_reward = 0.0\n",
    "        self._current_length = 0\n",
    "        self._plot_handle = None\n",
    "        self._stats_handle = None\n",
    "        self._checkpoint_handle = None\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals.get(\"rewards\")\n",
    "        dones = self.locals.get(\"dones\")\n",
    "\n",
    "        if rewards is not None and dones is not None:\n",
    "            reward = rewards[0]\n",
    "            done = dones[0]\n",
    "\n",
    "            self._current_reward += float(reward)\n",
    "            self._current_length += 1\n",
    "\n",
    "            if done:\n",
    "                self.episode_rewards.append(self._current_reward)\n",
    "                self.episode_lengths.append(self._current_length)\n",
    "                ep = len(self.episode_rewards)\n",
    "\n",
    "                # Periodic checkpoint\n",
    "                if self.checkpoint_path and ep % CHECKPOINT_FREQ_EPISODES == 0:\n",
    "                    ckpt_path = os.path.join(self.checkpoint_path, f\"checkpoint_ep{ep}\")\n",
    "                    self.model.save(ckpt_path)\n",
    "                    ckpt_text = f\"[Checkpoint] Episode {ep} saved\"\n",
    "                    if self._checkpoint_handle is None:\n",
    "                        self._checkpoint_handle = display(ckpt_text, display_id=True)\n",
    "                    else:\n",
    "                        self._checkpoint_handle.update(ckpt_text)\n",
    "\n",
    "                if ep % CHART_UPDATE_FREQ == 0:\n",
    "                    recent = np.array(self.episode_rewards[-50:])\n",
    "                    stats_text = (\n",
    "                        f'Episode {ep} | Last {len(recent)} Ep \\u2014 '\n",
    "                        f'Mean: {np.mean(recent):.1f} | Std: {np.std(recent):.1f} | '\n",
    "                        f'Min: {np.min(recent):.1f} | Max: {np.max(recent):.1f} | '\n",
    "                        f'Success: {(recent >= 200).sum() / len(recent) * 100:.0f}%'\n",
    "                    )\n",
    "                    if self._stats_handle is None:\n",
    "                        self._stats_handle = display(stats_text, display_id=True)\n",
    "                    else:\n",
    "                        self._stats_handle.update(stats_text)\n",
    "\n",
    "                if ep % CHART_UPDATE_FREQ == 0:\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "                    ax1.plot(self.episode_rewards, alpha=0.3, color='gray')\n",
    "                    window = min(50, len(self.episode_rewards))\n",
    "                    rolling = pd.Series(self.episode_rewards).rolling(window).mean()\n",
    "                    ax1.plot(rolling, color='blue', linewidth=2)\n",
    "                    ax1.axhline(y=200, color='red', linestyle='--')\n",
    "                    ax1.set_title(f'Episode Reward \\u2014 Ep {ep}')\n",
    "                    ax1.set_xlabel('Episode')\n",
    "                    ax1.set_ylabel('Reward')\n",
    "                    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "                    if self.value_loss:\n",
    "                        ax2.plot(self.value_loss, color='green', alpha=0.7)\n",
    "                        ax2.set_title('Value Loss')\n",
    "                        ax2.set_xlabel('Rollout')\n",
    "                        ax2.set_ylabel('Loss')\n",
    "                        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "                    plt.tight_layout()\n",
    "\n",
    "                    if self._plot_handle is None:\n",
    "                        self._plot_handle = display(fig, display_id=True)\n",
    "                    else:\n",
    "                        self._plot_handle.update(fig)\n",
    "                    plt.close(fig)\n",
    "\n",
    "                self._current_reward = 0.0\n",
    "                self._current_length = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        logger_data = self.model.logger.name_to_value\n",
    "        if \"train/loss\" in logger_data:\n",
    "            self.value_loss.append(logger_data[\"train/loss\"])\n",
    "        if \"rollout/exploration_rate\" in logger_data:\n",
    "            self.entropy.append(logger_data[\"rollout/exploration_rate\"])\n",
    "        if \"train/n_updates\" in logger_data:\n",
    "            self.gradient_updates = int(logger_data[\"train/n_updates\"])\n",
    "\n",
    "        # Sample Q-values from current observation to track overestimation\n",
    "        try:\n",
    "            obs = self.locals.get(\"new_obs\")\n",
    "            if obs is not None:\n",
    "                obs_tensor = torch.as_tensor(obs, device=self.model.device).float()\n",
    "                dqn_model: DQN = self.model  # type: ignore[assignment]\n",
    "                with torch.no_grad():\n",
    "                    q_values = dqn_model.q_net(obs_tensor)\n",
    "                self.mean_q_values.append(float(q_values.max(dim=1).values.mean()))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "class PPOLoggingCallback(BaseCallback):\n",
    "    def __init__(self, checkpoint_path=None, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.policy_loss = []\n",
    "        self.value_loss = []\n",
    "        self.entropy = []\n",
    "        self.clip_fraction = []       # PPO stability: fraction of clipped updates\n",
    "        self.approx_kl = []           # PPO stability: KL divergence\n",
    "        self.explained_variance = []  # PPO stability: value function quality\n",
    "        self.gradient_updates = 0     # Count gradient updates\n",
    "\n",
    "        self._current_rewards: np.ndarray = np.array([])\n",
    "        self._current_lengths: np.ndarray = np.array([])\n",
    "        self._plot_handle = None\n",
    "        self._stats_handle = None\n",
    "        self._checkpoint_handle = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        n_envs = self.training_env.num_envs\n",
    "        self._current_rewards = np.zeros(n_envs, dtype=np.float32)\n",
    "        self._current_lengths = np.zeros(n_envs, dtype=np.int32)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals.get(\"rewards\")\n",
    "        dones = self.locals.get(\"dones\")\n",
    "\n",
    "        if rewards is not None and dones is not None:\n",
    "            self._current_rewards += rewards\n",
    "            self._current_lengths += 1\n",
    "\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:\n",
    "                    self.episode_rewards.append(float(self._current_rewards[i]))\n",
    "                    self.episode_lengths.append(int(self._current_lengths[i]))\n",
    "                    ep = len(self.episode_rewards)\n",
    "\n",
    "                    # Periodic checkpoint\n",
    "                    if self.checkpoint_path and ep % CHECKPOINT_FREQ_EPISODES == 0:\n",
    "                        ckpt_path = os.path.join(self.checkpoint_path, f\"checkpoint_ep{ep}\")\n",
    "                        self.model.save(ckpt_path)\n",
    "                        ckpt_text = f\"[Checkpoint] Episode {ep} saved\"\n",
    "                        if self._checkpoint_handle is None:\n",
    "                            self._checkpoint_handle = display(ckpt_text, display_id=True)\n",
    "                        else:\n",
    "                            self._checkpoint_handle.update(ckpt_text)\n",
    "\n",
    "                    if ep % CHART_UPDATE_FREQ == 0:\n",
    "                        recent = np.array(self.episode_rewards[-50:])\n",
    "                        stats_text = (\n",
    "                            f'Episode {ep} | Last {len(recent)} Ep \\u2014 '\n",
    "                            f'Mean: {np.mean(recent):.1f} | Std: {np.std(recent):.1f} | '\n",
    "                            f'Min: {np.min(recent):.1f} | Max: {np.max(recent):.1f} | '\n",
    "                            f'Success: {(recent >= 200).sum() / len(recent) * 100:.0f}%'\n",
    "                        )\n",
    "                        if self._stats_handle is None:\n",
    "                            self._stats_handle = display(stats_text, display_id=True)\n",
    "                        else:\n",
    "                            self._stats_handle.update(stats_text)\n",
    "\n",
    "                    if ep % CHART_UPDATE_FREQ == 0:\n",
    "                        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "                        ax1.plot(self.episode_rewards, alpha=0.3, color='gray')\n",
    "                        window = min(50, len(self.episode_rewards))\n",
    "                        rolling = pd.Series(self.episode_rewards).rolling(window).mean()\n",
    "                        ax1.plot(rolling, color='blue', linewidth=2)\n",
    "                        ax1.axhline(y=200, color='red', linestyle='--')\n",
    "                        ax1.set_title(f'Episode Reward \\u2014 Ep {ep}')\n",
    "                        ax1.set_xlabel('Episode')\n",
    "                        ax1.set_ylabel('Reward')\n",
    "                        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "                        if self.value_loss:\n",
    "                            ax2.plot(self.value_loss, color='green', alpha=0.7)\n",
    "                            ax2.set_title('Value Loss')\n",
    "                            ax2.set_xlabel('Rollout')\n",
    "                            ax2.set_ylabel('Loss')\n",
    "                            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "                        plt.tight_layout()\n",
    "\n",
    "                        if self._plot_handle is None:\n",
    "                            self._plot_handle = display(fig, display_id=True)\n",
    "                        else:\n",
    "                            self._plot_handle.update(fig)\n",
    "                        plt.close(fig)\n",
    "\n",
    "                    self._current_rewards[i] = 0\n",
    "                    self._current_lengths[i] = 0\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        logger_data = self.model.logger.name_to_value\n",
    "        if \"train/policy_gradient_loss\" in logger_data:\n",
    "            self.policy_loss.append(logger_data[\"train/policy_gradient_loss\"])\n",
    "        if \"train/value_loss\" in logger_data:\n",
    "            self.value_loss.append(logger_data[\"train/value_loss\"])\n",
    "        if \"train/entropy_loss\" in logger_data:\n",
    "            self.entropy.append(-logger_data[\"train/entropy_loss\"])\n",
    "        if \"train/clip_fraction\" in logger_data:\n",
    "            self.clip_fraction.append(logger_data[\"train/clip_fraction\"])\n",
    "        if \"train/approx_kl\" in logger_data:\n",
    "            self.approx_kl.append(logger_data[\"train/approx_kl\"])\n",
    "        if \"train/explained_variance\" in logger_data:\n",
    "            self.explained_variance.append(logger_data[\"train/explained_variance\"])\n",
    "        if \"train/n_updates\" in logger_data:\n",
    "            self.gradient_updates = int(logger_data[\"train/n_updates\"])\n",
    "\n",
    "\n",
    "class CombinedMetricEvalCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    Custom EvalCallback that selects the best model using a combined metric:\n",
    "        score = mean_reward - std_reward\n",
    "    This favors models that are both high-performing and consistent.\n",
    "\n",
    "    Two-tier selection with solved gate:\n",
    "    - Once any evaluation has mean_reward >= SOLVED_THRESHOLD, only solved\n",
    "      models can replace the current best (unsolved fallbacks are discarded).\n",
    "    - Before any model solves, the overall best score is tracked as fallback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.best_combined_score = -np.inf\n",
    "        self.best_std_reward = np.inf\n",
    "        self.best_success_rate = 0.0\n",
    "        self.best_timestep = 0\n",
    "        self._any_solved = False\n",
    "        self._eval_handle = None\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        continue_training = True\n",
    "\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            episode_rewards, episode_lengths = evaluate_policy(\n",
    "                self.model,\n",
    "                self.eval_env,\n",
    "                n_eval_episodes=self.n_eval_episodes,\n",
    "                render=self.render,\n",
    "                deterministic=self.deterministic,\n",
    "                return_episode_rewards=True,\n",
    "            )\n",
    "\n",
    "            mean_reward, std_reward = np.mean(episode_rewards), np.std(episode_rewards)\n",
    "            mean_ep_length = np.mean(episode_lengths)\n",
    "            success_rate = np.sum(np.array(episode_rewards) >= 200) / len(episode_rewards) * 100   # type: ignore\n",
    "            self.last_mean_reward = mean_reward\n",
    "\n",
    "            combined_score = mean_reward - std_reward\n",
    "            is_solved = mean_reward >= SOLVED_THRESHOLD\n",
    "\n",
    "            # Two-tier best model selection:\n",
    "            # 1. If this model is solved, save if it's the first solved or has better score\n",
    "            # 2. If no model has solved yet, save the overall best as fallback\n",
    "            save_new_best = False\n",
    "            if is_solved:\n",
    "                if not self._any_solved:\n",
    "                    # First solved model — always save (replaces any unsolved fallback)\n",
    "                    save_new_best = True\n",
    "                    self._any_solved = True\n",
    "                elif combined_score > self.best_combined_score:\n",
    "                    # Better solved model\n",
    "                    save_new_best = True\n",
    "            elif not self._any_solved:\n",
    "                # No solved model yet — track overall best as fallback\n",
    "                if combined_score > self.best_combined_score:\n",
    "                    save_new_best = True\n",
    "\n",
    "            solved_tag = \" [SOLVED]\" if is_solved else \"\"\n",
    "            eval_text = (\n",
    "                f\"Eval @ {self.num_timesteps} steps | \"\n",
    "                f\"Reward: {mean_reward:.2f} +/- {std_reward:.2f}{solved_tag} | \"\n",
    "                f\"Success: {success_rate:.0f}% | \"\n",
    "                f\"Score (mean-std): {combined_score:.2f} | \"\n",
    "                f\"Best: {self.best_combined_score:.2f}\"\n",
    "            )\n",
    "\n",
    "            if save_new_best:\n",
    "                eval_text += f\" >> New best model!\"\n",
    "                self.best_combined_score = combined_score\n",
    "                self.best_mean_reward = mean_reward\n",
    "                self.best_std_reward = std_reward\n",
    "                self.best_success_rate = success_rate\n",
    "                self.best_timestep = self.num_timesteps\n",
    "                if self.best_model_save_path is not None:\n",
    "                    self.model.save(\n",
    "                        os.path.join(self.best_model_save_path, \"best_model\")\n",
    "                    )\n",
    "\n",
    "            if self._eval_handle is None:\n",
    "                self._eval_handle = display(eval_text, display_id=True)\n",
    "            else:\n",
    "                self._eval_handle.update(eval_text)\n",
    "\n",
    "            if self.log_path is not None:\n",
    "                self.evaluations_timesteps.append(self.num_timesteps)\n",
    "                self.evaluations_results.append(episode_rewards)   # type: ignore\n",
    "                self.evaluations_length.append(episode_lengths)    # type: ignore\n",
    "                np.savez(\n",
    "                    self.log_path,\n",
    "                    timesteps=self.evaluations_timesteps,\n",
    "                    results=self.evaluations_results,\n",
    "                    ep_lengths=self.evaluations_length,\n",
    "                )\n",
    "\n",
    "            self.logger.record(\"eval/mean_reward\", float(mean_reward))\n",
    "            self.logger.record(\"eval/std_reward\", float(std_reward))\n",
    "            self.logger.record(\"eval/mean_ep_length\", float(mean_ep_length))\n",
    "            self.logger.record(\"eval/combined_score\", float(combined_score))\n",
    "            self.logger.record(\"eval/success_rate\", float(success_rate))\n",
    "\n",
    "        return continue_training\n",
    "\n",
    "\n",
    "CALLBACK_MAP = {\n",
    "    \"dqn\": DQNLoggingCallback,\n",
    "    \"ppo\": PPOLoggingCallback,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop: algorithms x seeds\n",
    "\n",
    "training_results = {}  # {algo: {seed: callback}}\n",
    "training_times = {}    # {algo: {seed: seconds}}\n",
    "model_save_paths = {}  # {algo: {seed: {\"run_dir\", \"final\", \"best\"}}}\n",
    "eval_callbacks = {}    # {algo: {seed: eval_cb}} — for best-model summary table\n",
    "\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "\n",
    "    tb_dir = os.path.join(NOTEBOOK_DIR, \"outputs_\" + algo_name, \"tensorboard\")\n",
    "\n",
    "    training_results[algo_name] = {}\n",
    "    training_times[algo_name] = {}\n",
    "    model_save_paths[algo_name] = {}\n",
    "    eval_callbacks[algo_name] = {}\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{algo_name.upper()} | Seed {seed}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        # Create timestamped run directory\n",
    "        run_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "        run_dir = os.path.join(NOTEBOOK_DIR, \"../../../models\", algo_name, run_timestamp)\n",
    "        checkpoints_dir = os.path.join(run_dir, \"checkpoints\")\n",
    "        os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        def make_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        env = DummyVecEnv([make_env])\n",
    "        env.seed(seed)\n",
    "\n",
    "        # Separate eval env for EvalCallback\n",
    "        def make_eval_env(s=seed):\n",
    "            e = Monitor(gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED))\n",
    "            e.reset(seed=s)\n",
    "            return e\n",
    "\n",
    "        eval_env = DummyVecEnv([make_eval_env])\n",
    "\n",
    "        params = {\n",
    "            \"policy\": MLP_POLICY,\n",
    "            \"env\": env,\n",
    "            \"device\": DEVICE,\n",
    "            \"seed\": seed,\n",
    "            \"tensorboard_log\": tb_dir,\n",
    "            **ALGO_PARAMS[algo_name],\n",
    "        }\n",
    "\n",
    "        model = algo_class(**params)\n",
    "\n",
    "        # Setup callbacks\n",
    "        logging_cb = CALLBACK_MAP[algo_name](checkpoint_path=checkpoints_dir)\n",
    "        eval_cb = CombinedMetricEvalCallback(\n",
    "            eval_env=eval_env,\n",
    "            eval_freq=EVAL_FREQ_TIMESTEPS,\n",
    "            n_eval_episodes=EVAL_N_EPISODES,\n",
    "            best_model_save_path=run_dir,\n",
    "            log_path=os.path.join(run_dir, \"eval_log\"),\n",
    "            deterministic=True,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        t_start = time.time()\n",
    "        model.learn(\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            callback=CallbackList([logging_cb, eval_cb]),\n",
    "            progress_bar=True,\n",
    "        )\n",
    "        t_elapsed = time.time() - t_start\n",
    "\n",
    "        training_times[algo_name][seed] = t_elapsed\n",
    "        print(f\"\\nTraining time: {t_elapsed/60:.1f} min ({t_elapsed:.0f} s)\")\n",
    "\n",
    "        # Save final model\n",
    "        final_path = os.path.join(run_dir, f\"{SESSION_PREFIX}_{algo_name}_{seed}\")\n",
    "        model.save(final_path)\n",
    "        print(f\"Final model:  {final_path}\")\n",
    "        print(f\"Best model:   {os.path.join(run_dir, 'best_model')}\")\n",
    "        print(f\"Checkpoints:  {checkpoints_dir}\")\n",
    "        print(\n",
    "            f\"Best model stats: \"\n",
    "            f\"Reward: {eval_cb.best_mean_reward:.2f} +/- {eval_cb.best_std_reward:.2f} | \"\n",
    "            f\"Success: {eval_cb.best_success_rate:.0f}% | \"\n",
    "            f\"Score (mean-std): {eval_cb.best_combined_score:.2f} | \"\n",
    "            f\"@ {eval_cb.best_timestep:,} steps\"\n",
    "        )\n",
    "\n",
    "        model_save_paths[algo_name][seed] = {\n",
    "            \"run_dir\": run_dir,\n",
    "            \"final\": final_path,\n",
    "            \"best\": os.path.join(run_dir, \"best_model\"),\n",
    "        }\n",
    "\n",
    "        training_results[algo_name][seed] = logging_cb\n",
    "        eval_callbacks[algo_name][seed] = eval_cb\n",
    "\n",
    "        env.close()\n",
    "        eval_env.close()\n",
    "\n",
    "    print(f\"\\n{algo_name.upper()}: All {len(SEED_LIST)} seeds trained.\")\n",
    "\n",
    "# Best Model Summary Table\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BEST MODEL SUMMARY (all algorithms x seeds)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "best_rows = []\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    for seed in SEED_LIST:\n",
    "        cb = eval_callbacks[algo_name][seed]\n",
    "        best_rows.append({\n",
    "            \"Algorithm\": algo_name.upper(),\n",
    "            \"Seed\": seed,\n",
    "            \"Mean Reward\": f\"{cb.best_mean_reward:.2f}\",\n",
    "            \"Std Reward\": f\"{cb.best_std_reward:.2f}\",\n",
    "            \"Success\": f\"{cb.best_success_rate:.0f}%\",\n",
    "            \"Score (mean-std)\": f\"{cb.best_combined_score:.2f}\",\n",
    "            \"@ Timestep\": f\"{cb.best_timestep:,}\",\n",
    "        })\n",
    "\n",
    "print(pd.DataFrame(best_rows).to_string(index=False))\n",
    "print(f\"\\nAll training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4plzcl4c9m",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Time Summary\n",
    "\n",
    "rows = []\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    for seed in SEED_LIST:\n",
    "        t = training_times[algo_name][seed]\n",
    "        rows.append({\n",
    "            \"Algorithm\": algo_name.upper(),\n",
    "            \"Seed\": seed,\n",
    "            \"Time (s)\": f\"{t:.0f}\",\n",
    "            \"Time (min)\": f\"{t/60:.1f}\",\n",
    "        })\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    times = list(training_times[algo_name].values())\n",
    "    rows.append({\n",
    "        \"Algorithm\": algo_name.upper(),\n",
    "        \"Seed\": \"Mean\",\n",
    "        \"Time (s)\": f\"{np.mean(times):.0f}\",\n",
    "        \"Time (min)\": f\"{np.mean(times)/60:.1f}\",\n",
    "    })\n",
    "\n",
    "print(\"*** TRAINING TIME SUMMARY ***\")\n",
    "print(f\"Timesteps per seed: {TOTAL_TIMESTEPS:,} | Device: {DEVICE}\")\n",
    "print()\n",
    "print(pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nkvt5nvs81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Updates Summary\n",
    "# DQN: train_freq=4, gradient_steps=4 -> 4 gradient steps every 4 env steps\n",
    "# PPO: n_steps=2048, n_epochs=10, batch_size=64 -> 10 * (2048/64) = 320 updates per rollout\n",
    "\n",
    "rows = []\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    # Use the actual count from the last seed's callback\n",
    "    last_seed = SEED_LIST[-1]\n",
    "    actual_updates = training_results[algo_name][last_seed].gradient_updates\n",
    "\n",
    "    # Also compute analytically for verification\n",
    "    if algo_name == \"dqn\":\n",
    "        p = ALGO_PARAMS[\"dqn\"]\n",
    "        train_freq = p.get(\"train_freq\", 4)\n",
    "        grad_steps = p.get(\"gradient_steps\", -1)\n",
    "        learning_starts = p.get(\"learning_starts\", 0)\n",
    "        effective_steps = TOTAL_TIMESTEPS - learning_starts\n",
    "        steps_per_call = train_freq if grad_steps == -1 else grad_steps\n",
    "        analytical = (effective_steps // train_freq) * steps_per_call\n",
    "    else:\n",
    "        p = ALGO_PARAMS[\"ppo\"]\n",
    "        n_steps = p.get(\"n_steps\", 2048)\n",
    "        n_epochs = p.get(\"n_epochs\", 10)\n",
    "        batch_size = p.get(\"batch_size\", 64)\n",
    "        n_rollouts = TOTAL_TIMESTEPS // n_steps\n",
    "        minibatches_per_epoch = n_steps // batch_size\n",
    "        analytical = n_rollouts * n_epochs * minibatches_per_epoch\n",
    "\n",
    "    rows.append({\n",
    "        \"Algorithm\": algo_name.upper(),\n",
    "        \"Actual (from training)\": f\"{actual_updates:,}\",\n",
    "        \"Analytical (computed)\": f\"{analytical:,}\",\n",
    "        \"Total Env Steps\": f\"{TOTAL_TIMESTEPS:,}\",\n",
    "        \"Ratio (updates/steps)\": f\"{actual_updates / TOTAL_TIMESTEPS:.3f}\",\n",
    "    })\n",
    "\n",
    "print(\"*** GRADIENT UPDATES SUMMARY ***\")\n",
    "print(pd.DataFrame(rows).to_string(index=False))\n",
    "print()\n",
    "print(\"DQN: train_freq=4, gradient_steps=4 -> 4 gradient updates per training call, called every 4 env steps\")\n",
    "print(f\"PPO: n_steps={ALGO_PARAMS['ppo']['n_steps']}, n_epochs={ALGO_PARAMS['ppo']['n_epochs']}, \"\n",
    "      f\"batch_size={ALGO_PARAMS['ppo']['batch_size']} -> \"\n",
    "      f\"{ALGO_PARAMS['ppo']['n_epochs'] * (ALGO_PARAMS['ppo']['n_steps'] // ALGO_PARAMS['ppo']['batch_size'])} \"\n",
    "      f\"updates per rollout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm, Per-Seed: Episode Reward over Training\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "    if len(SEED_LIST) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, seed in zip(axes, SEED_LIST):\n",
    "        ax.plot(training_results[algo_name][seed].episode_rewards, alpha=0.7)\n",
    "        ax.set_title(f\"Seed {seed}\")\n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"Total Reward\")\n",
    "    fig.suptitle(f\"{algo_name.upper()} \\u2014 Episode Reward over Training\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm, Per-Seed: Episode Length over Training\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "    if len(SEED_LIST) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, seed in zip(axes, SEED_LIST):\n",
    "        ax.plot(training_results[algo_name][seed].episode_lengths, alpha=0.7, color=\"orange\")\n",
    "        ax.set_title(f\"Seed {seed}\")\n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"Number of Steps\")\n",
    "    fig.suptitle(f\"{algo_name.upper()} \\u2014 Episode Length over Training\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm, Per-Seed: Value Loss over Rollouts\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "    if len(SEED_LIST) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, seed in zip(axes, SEED_LIST):\n",
    "        ax.plot(training_results[algo_name][seed].value_loss, alpha=0.7, color=\"green\")\n",
    "        ax.set_title(f\"Seed {seed}\")\n",
    "        ax.set_xlabel(\"Rollout\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"Loss Value\")\n",
    "    fig.suptitle(f\"{algo_name.upper()} \\u2014 Value Loss over Rollouts\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm, Per-Seed: Entropy / Exploration Rate over Rollouts\n",
    "\n",
    "entropy_labels = {\"dqn\": (\"Epsilon\", \"Exploration Rate\"), \"ppo\": (\"Entropy (Positive)\", \"Entropy\")}\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    ylabel, title_suffix = entropy_labels[algo_name]\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "    if len(SEED_LIST) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, seed in zip(axes, SEED_LIST):\n",
    "        ax.plot(training_results[algo_name][seed].entropy, alpha=0.7, color=\"purple\")\n",
    "        ax.set_title(f\"Seed {seed}\")\n",
    "        ax.set_xlabel(\"Rollout\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(ylabel)\n",
    "    fig.suptitle(f\"{algo_name.upper()} \\u2014 {title_suffix} over Rollouts\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regiuvw739b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Only: Policy Loss over Rollouts\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    ax.plot(training_results[\"ppo\"][seed].policy_loss, alpha=0.7, color=\"red\")\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.set_xlabel(\"Rollout\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Policy Loss\")\n",
    "fig.suptitle(\"PPO \\u2014 Policy Gradient Loss over Rollouts\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5wjoo9ysenc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Overestimation: Mean Max Q-Value over Training\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    q_vals = training_results[\"dqn\"][seed].mean_q_values\n",
    "    if q_vals:\n",
    "        ax.plot(q_vals, alpha=0.7, color=\"darkorange\")\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.set_xlabel(\"Rollout\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Mean Max Q-Value\")\n",
    "fig.suptitle(\"DQN \\u2014 Q-Value Overestimation Tracking\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Steadily rising Q-values that diverge from actual returns indicate overestimation.\")\n",
    "print(\"A stable or slowly growing curve suggests the target network is controlling overestimation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v772hhtit5s",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Update Stability: Clip Fraction, Approx KL, Explained Variance\n",
    "\n",
    "fig, axes = plt.subplots(3, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 12), sharex=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = axes.reshape(3, 1)\n",
    "\n",
    "metrics = [\n",
    "    (\"clip_fraction\", \"Clip Fraction\", \"tab:red\",\n",
    "     \"Fraction of policy updates clipped by PPO. High values suggest the policy is changing too fast.\"),\n",
    "    (\"approx_kl\", \"Approx KL Divergence\", \"tab:purple\",\n",
    "     \"KL divergence between old and new policy. Spikes indicate large policy shifts.\"),\n",
    "    (\"explained_variance\", \"Explained Variance\", \"tab:cyan\",\n",
    "     \"How well the value function predicts returns. 1.0 = perfect, 0 = no better than mean.\"),\n",
    "]\n",
    "\n",
    "for row, (attr, ylabel, color, _) in enumerate(metrics):\n",
    "    for col, seed in enumerate(SEED_LIST):\n",
    "        data = getattr(training_results[\"ppo\"][seed], attr)\n",
    "        if data:\n",
    "            axes[row][col].plot(data, alpha=0.7, color=color)\n",
    "        if row == 0:\n",
    "            axes[row][col].set_title(f\"Seed {seed}\")\n",
    "        if row == 2:\n",
    "            axes[row][col].set_xlabel(\"Rollout\")\n",
    "        axes[row][col].grid(True, alpha=0.3)\n",
    "    axes[row][0].set_ylabel(ylabel)\n",
    "\n",
    "fig.suptitle(\"PPO \\u2014 Update Stability Metrics over Rollouts\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for _, ylabel, _, description in metrics:\n",
    "    print(f\"  {ylabel}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated: Rolling Reward Overlay — per algorithm (all seeds on one chart)\n",
    "\n",
    "seed_colors = list(plt.colormaps[\"tab10\"](range(10)))  # type: ignore[arg-type]\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for i, seed in enumerate(SEED_LIST):\n",
    "        rewards = training_results[algo_name][seed].episode_rewards\n",
    "        rolling = pd.Series(rewards).rolling(50).mean()\n",
    "        plt.plot(rolling, color=seed_colors[i], linewidth=2, label=f\"Seed {seed}\")\n",
    "\n",
    "    plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "    plt.title(f\"{algo_name.upper()} Training: Rolling Mean Reward (window=50)\", fontsize=14)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y5ge8849z4s",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm: Rolling Success Rate over Training (window=50)\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for i, seed in enumerate(SEED_LIST):\n",
    "        rewards = np.array(training_results[algo_name][seed].episode_rewards)\n",
    "        success = (rewards >= 200).astype(float)\n",
    "        rolling_success = pd.Series(success).rolling(50).mean() * 100\n",
    "        plt.plot(rolling_success, color=seed_colors[i], linewidth=2, label=f\"Seed {seed}\")\n",
    "\n",
    "    plt.axhline(y=100, color='green', linestyle=':', alpha=0.4, label='100%')\n",
    "    plt.axhline(y=50, color='gray', linestyle=':', alpha=0.4, label='50%')\n",
    "    plt.title(f\"{algo_name.upper()} Training: Rolling Success Rate (window=50)\", fontsize=14)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Success Rate (%)\")\n",
    "    plt.ylim(-5, 105)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vwdc82dyk5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm Comparison: Success Rate over Training (averaged across seeds)\n",
    "\n",
    "algo_colors = {\"dqn\": \"tab:blue\", \"ppo\": \"tab:orange\"}\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    min_len = min(len(training_results[algo_name][s].episode_rewards) for s in SEED_LIST)\n",
    "    all_rewards = np.array([training_results[algo_name][s].episode_rewards[:min_len] for s in SEED_LIST])\n",
    "    all_success = (all_rewards >= 200).astype(float)\n",
    "\n",
    "    mean_success = pd.Series(all_success.mean(axis=0)).rolling(50).mean() * 100\n",
    "    std_success = pd.Series(all_success.std(axis=0)).rolling(50).mean() * 100\n",
    "\n",
    "    episodes = np.arange(len(mean_success))\n",
    "    plt.plot(episodes, mean_success, color=algo_colors[algo_name], linewidth=2,\n",
    "             label=f\"{algo_name.upper()} (mean)\")\n",
    "    plt.fill_between(episodes, mean_success - std_success, mean_success + std_success,\n",
    "                     color=algo_colors[algo_name], alpha=0.15)\n",
    "\n",
    "plt.axhline(y=100, color='green', linestyle=':', alpha=0.4, label='100%')\n",
    "plt.axhline(y=50, color='gray', linestyle=':', alpha=0.4, label='50%')\n",
    "plt.title(\"DQN vs PPO: Mean Success Rate across Seeds (window=50)\", fontsize=14)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Success Rate (%)\")\n",
    "plt.ylim(-5, 105)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm Comparison: Rolling Reward (averaged across seeds)\n",
    "\n",
    "algo_colors = {\"dqn\": \"tab:blue\", \"ppo\": \"tab:orange\"}\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    # Find the shortest episode count across seeds for alignment\n",
    "    min_len = min(len(training_results[algo_name][s].episode_rewards) for s in SEED_LIST)\n",
    "    all_rewards = np.array([training_results[algo_name][s].episode_rewards[:min_len] for s in SEED_LIST])\n",
    "    mean_rewards = pd.Series(all_rewards.mean(axis=0)).rolling(50).mean()\n",
    "    std_rewards = pd.Series(all_rewards.std(axis=0)).rolling(50).mean()\n",
    "\n",
    "    episodes = np.arange(len(mean_rewards))\n",
    "    plt.plot(episodes, mean_rewards, color=algo_colors[algo_name], linewidth=2, label=f\"{algo_name.upper()} (mean)\")\n",
    "    plt.fill_between(episodes, mean_rewards - std_rewards, mean_rewards + std_rewards,\n",
    "                     color=algo_colors[algo_name], alpha=0.15)\n",
    "\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.title(\"DQN vs PPO: Mean Rolling Reward across Seeds (window=50)\", fontsize=14)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: deterministic episodes per algorithm per seed (best model)\n",
    "\n",
    "evaluation_results = {}  # {algo: {seed: np.array}}\n",
    "\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "    evaluation_results[algo_name] = {}\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        print(f\"Evaluating {algo_name.upper()} seed {seed} (best model)...\")\n",
    "\n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        best_path = model_save_paths[algo_name][seed][\"best\"]\n",
    "\n",
    "        def make_eval_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        eval_model = algo_class.load(best_path, env=DummyVecEnv([make_eval_env]), device=DEVICE)\n",
    "\n",
    "        eval_env = Monitor(gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED))\n",
    "        eval_env.reset(seed=seed)\n",
    "\n",
    "        rewards, _ = evaluate_policy(\n",
    "            eval_model,\n",
    "            eval_env,\n",
    "            n_eval_episodes=EVALUATION_EPISODES,\n",
    "            deterministic=True,\n",
    "            return_episode_rewards=True\n",
    "        )\n",
    "\n",
    "        evaluation_results[algo_name][seed] = np.array(rewards)\n",
    "        eval_env.close()\n",
    "\n",
    "print(f\"\\nEvaluation complete for all algorithms and seeds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Summary Tables (per algorithm + overall)\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    rows = []\n",
    "    for seed in SEED_LIST:\n",
    "        r = evaluation_results[algo_name][seed]\n",
    "        rows.append({\n",
    "            \"Seed\": seed,\n",
    "            \"Mean Reward\": f\"{np.mean(r):.2f}\",\n",
    "            \"Std Dev\": f\"{np.std(r):.2f}\",\n",
    "            \"Min Reward\": f\"{np.min(r):.2f}\",\n",
    "            \"Max Reward\": f\"{np.max(r):.2f}\",\n",
    "            \"Success Rate\": f\"{(r >= 200).sum() / len(r) * 100:.1f}%\"\n",
    "        })\n",
    "\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "    rows.append({\n",
    "        \"Seed\": \"Overall\",\n",
    "        \"Mean Reward\": f\"{np.mean(all_r):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(all_r):.2f}\",\n",
    "        \"Min Reward\": f\"{np.min(all_r):.2f}\",\n",
    "        \"Max Reward\": f\"{np.max(all_r):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_r >= 200).sum() / len(all_r) * 100:.1f}%\"\n",
    "    })\n",
    "\n",
    "    print(f\"*** {algo_name.upper()} MULTI-SEED EVALUATION SUMMARY ***\")\n",
    "    print(f\"Episodes per seed: {EVALUATION_EPISODES} | Total: {len(all_r)}\")\n",
    "    print(pd.DataFrame(rows).to_string(index=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm Comparison: Bar Chart\n",
    "\n",
    "algo_names = list(ALGORITHM_MAP.keys())\n",
    "n_algos = len(algo_names)\n",
    "n_seeds = len(SEED_LIST)\n",
    "bar_width = 0.8 / n_algos\n",
    "x = np.arange(n_seeds)\n",
    "\n",
    "plt.figure(figsize=(max(10, 3 * n_seeds), 6))\n",
    "for i, algo_name in enumerate(algo_names):\n",
    "    means = [np.mean(evaluation_results[algo_name][s]) for s in SEED_LIST]\n",
    "    stds = [np.std(evaluation_results[algo_name][s]) for s in SEED_LIST]\n",
    "    offset = (i - (n_algos - 1) / 2) * bar_width\n",
    "    plt.bar(x + offset, means, bar_width, yerr=stds, capsize=4,\n",
    "            label=algo_name.upper(), alpha=0.8)\n",
    "\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.xticks(x, [str(s) for s in SEED_LIST])\n",
    "plt.title(f\"DQN vs PPO: Mean Reward per Seed ({EVALUATION_EPISODES} episodes each)\", fontsize=14)\n",
    "plt.xlabel(\"Seed\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm: Evaluation Convergence Plots\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "    if len(SEED_LIST) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, seed in zip(axes, SEED_LIST):\n",
    "        rewards = evaluation_results[algo_name][seed]\n",
    "        episodes = np.arange(1, len(rewards) + 1)\n",
    "        running_mean = np.cumsum(rewards) / episodes\n",
    "\n",
    "        ax.scatter(episodes, rewards, color='gray', alpha=0.4, s=20)\n",
    "        ax.plot(episodes, running_mean, color='blue', linewidth=2)\n",
    "        ax.axhline(y=200, color='red', linestyle='--')\n",
    "        ax.set_title(f\"Seed {seed}\")\n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"Reward\")\n",
    "    fig.suptitle(f\"{algo_name.upper()} Evaluation: {EVALUATION_EPISODES} Episodes per Seed\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIF Visualization (one per algorithm per seed, best model)\n",
    "\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "    output_dir = os.path.join(NOTEBOOK_DIR, \"outputs_\" + algo_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        print(f\"Generating GIF for {algo_name.upper()} seed {seed} (best model)...\")\n",
    "\n",
    "        best_path = model_save_paths[algo_name][seed][\"best\"]\n",
    "\n",
    "        def make_vis_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        vis_model = algo_class.load(best_path, env=DummyVecEnv([make_vis_env]), device=DEVICE)\n",
    "\n",
    "        vis_env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "        frames = []\n",
    "        obs, info = vis_env.reset(seed=seed)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _ = vis_model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = vis_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            frames.append(vis_env.render())\n",
    "\n",
    "        vis_env.close()\n",
    "\n",
    "        gif_path = os.path.join(output_dir, f\"{algo_name}_seed{seed}.gif\")\n",
    "        imageio.mimsave(gif_path, frames, fps=30)\n",
    "        print(f\"  Saved: {gif_path}\")\n",
    "        display(Image(filename=gif_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tables\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    params = ALGO_PARAMS[algo_name]\n",
    "    rows = [{\"Parameter\": k, \"Value\": str(v)} for k, v in params.items()]\n",
    "    rows.append({\"Parameter\": \"total_timesteps\", \"Value\": str(TOTAL_TIMESTEPS)})\n",
    "    rows.append({\"Parameter\": \"device\", \"Value\": DEVICE})\n",
    "    rows.append({\"Parameter\": \"policy\", \"Value\": MLP_POLICY})\n",
    "    rows.append({\"Parameter\": \"checkpoint_freq_episodes\", \"Value\": str(CHECKPOINT_FREQ_EPISODES)})\n",
    "    rows.append({\"Parameter\": \"eval_freq_timesteps\", \"Value\": str(EVAL_FREQ_TIMESTEPS)})\n",
    "    rows.append({\"Parameter\": \"eval_n_episodes\", \"Value\": str(EVAL_N_EPISODES)})\n",
    "\n",
    "    print(f\"*** {algo_name.upper()} Hyperparameters ***\")\n",
    "    print(pd.DataFrame(rows).to_string(index=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7vhi0t10sov",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recovery: Reconstruct Best-Model Summary from saved eval logs\n",
    "# This cell is standalone — it scans models/ folders and rebuilds the table\n",
    "# from evaluations.npz files, without needing a prior training run in memory.\n",
    "#\n",
    "# Results are GROUPED BY SESSION (lab prefix), so seeds trained together\n",
    "# in the same notebook run are shown together.\n",
    "\n",
    "import glob\n",
    "\n",
    "models_root = os.path.join(NOTEBOOK_DIR, \"../../../models\")\n",
    "\n",
    "# Collect all completed run data\n",
    "all_runs = []  # list of dicts with session, algo, seed, scores, folder\n",
    "\n",
    "for algo_name in [\"dqn\", \"ppo\"]:\n",
    "    algo_dir = os.path.join(models_root, algo_name)\n",
    "    if not os.path.isdir(algo_dir):\n",
    "        continue\n",
    "\n",
    "    for run_folder in sorted(glob.glob(os.path.join(algo_dir, \"????-??-??_??_??_??\"))):\n",
    "        best_model_path = os.path.join(run_folder, \"best_model.zip\")\n",
    "        eval_log_path = os.path.join(run_folder, \"eval_log\", \"evaluations.npz\")\n",
    "\n",
    "        if not os.path.isfile(best_model_path):\n",
    "            continue  # incomplete run, skip\n",
    "\n",
    "        # Extract session (lab prefix) and seed from the final model filename\n",
    "        # e.g. lab009_dqn_42.zip -> session=\"lab009\", seed=\"42\"\n",
    "        session = \"unknown\"\n",
    "        seed_str = \"?\"\n",
    "        for f in os.listdir(run_folder):\n",
    "            if f.startswith(\"lab\") and f.endswith(\".zip\") and f != \"best_model.zip\":\n",
    "                parts = f.replace(\".zip\", \"\").split(\"_\")\n",
    "                # parts = [\"lab009\", \"dqn\", \"42\"]\n",
    "                session = parts[0] if len(parts) >= 1 else \"unknown\"\n",
    "                seed_str = parts[-1] if len(parts) >= 3 else \"?\"\n",
    "                break\n",
    "\n",
    "        timestamp = os.path.basename(run_folder)\n",
    "\n",
    "        run_entry = {\n",
    "            \"session\": session,\n",
    "            \"algo\": algo_name.upper(),\n",
    "            \"seed\": seed_str,\n",
    "            \"timestamp\": timestamp,\n",
    "            \"folder\": f\"models/{algo_name}/{timestamp}/\",\n",
    "        }\n",
    "\n",
    "        if os.path.isfile(eval_log_path):\n",
    "            data = np.load(eval_log_path, allow_pickle=True)\n",
    "            timesteps = data[\"timesteps\"]\n",
    "            results = data[\"results\"]\n",
    "\n",
    "            best_score = -np.inf\n",
    "            best_idx = 0\n",
    "            for i in range(len(timesteps)):\n",
    "                ep_rewards = results[i]\n",
    "                score = np.mean(ep_rewards) - np.std(ep_rewards)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_idx = i\n",
    "\n",
    "            ep = results[best_idx]\n",
    "            run_entry.update({   # type: ignore\n",
    "                \"mean_reward\": np.mean(ep),\n",
    "                \"std_reward\": np.std(ep),\n",
    "                \"success\": np.sum(ep >= 200) / len(ep) * 100,\n",
    "                \"score\": best_score,\n",
    "                \"timestep\": int(timesteps[best_idx]),\n",
    "                \"has_eval\": True,\n",
    "            })\n",
    "        else:\n",
    "            run_entry[\"has_eval\"] = False   # type: ignore\n",
    "\n",
    "        all_runs.append(run_entry)\n",
    "\n",
    "# Group by session\n",
    "sessions = sorted(set(r[\"session\"] for r in all_runs))\n",
    "\n",
    "if not all_runs:\n",
    "    print(\"No completed training runs found in models/.\")\n",
    "else:\n",
    "    for session in sessions:\n",
    "        session_runs = [r for r in all_runs if r[\"session\"] == session]\n",
    "\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"SESSION: {session}\")\n",
    "        print(f\"{'='*70}\")\n",
    "\n",
    "        rows = []\n",
    "        for r in sorted(session_runs, key=lambda x: (x[\"algo\"], x[\"seed\"])):\n",
    "            if r[\"has_eval\"]:\n",
    "                rows.append({\n",
    "                    \"Algorithm\": r[\"algo\"],\n",
    "                    \"Seed\": r[\"seed\"],\n",
    "                    \"Mean Reward\": f\"{r['mean_reward']:.2f}\",\n",
    "                    \"Std Reward\": f\"{r['std_reward']:.2f}\",\n",
    "                    \"Success\": f\"{r['success']:.0f}%\",\n",
    "                    \"Score (mean-std)\": f\"{r['score']:.2f}\",\n",
    "                    \"@ Timestep\": f\"{r['timestep']:,}\",\n",
    "                    \"Run Folder\": r[\"folder\"],\n",
    "                })\n",
    "            else:\n",
    "                rows.append({\n",
    "                    \"Algorithm\": r[\"algo\"],\n",
    "                    \"Seed\": r[\"seed\"],\n",
    "                    \"Mean Reward\": \"N/A\",\n",
    "                    \"Std Reward\": \"N/A\",\n",
    "                    \"Success\": \"N/A\",\n",
    "                    \"Score (mean-std)\": \"N/A\",\n",
    "                    \"@ Timestep\": \"N/A\",\n",
    "                    \"Run Folder\": r[\"folder\"] + \" (no eval log)\",\n",
    "                })\n",
    "\n",
    "        print(pd.DataFrame(rows).to_string(index=False))\n",
    "        print(f\"\\nRuns in this session: {len(session_runs)}\")\n",
    "        print(f\"Each 'Run Folder' contains: best_model.zip, checkpoints/, eval_log/\")\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_taap_p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
