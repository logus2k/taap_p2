{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random, time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "import imageio\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED) # to assure reproducibility on numpy (affects functions like np.random.rand, np.random.shuffle, etc.)\n",
    "torch.manual_seed(SEED)  # to assure reproducibility on Torch (affects weight initialization, dropout, data shuffling, etc.)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "torch.cuda.manual_seed_all(SEED) # usefull when using more than one GPT, otherwise torch.manual_seed is enough\n",
    "\n",
    "# Ensure deterministic behavior in CuDNN (NVIDIA backend for deep learning ops).\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = './outputs_DQN/'\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TENSORBOARD_LOGS_DIR = OUT_DIR + \"tensorboard/\"\n",
    "SAVE_MODEL_PATH = OUT_DIR + \"model_dqn.zip\"\n",
    "\n",
    "MODEL_NAME = \"LunarLander-v3\"\n",
    "MLP_POLICY = \"MlpPolicy\"\n",
    "\n",
    "env = gym.make(MODEL_NAME)\n",
    "env.reset(seed=SEED)\n",
    "\n",
    "# Select device: use GPU if available, otherwise fallback to CPU. This will be very important do control in which device the processing will happen\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Device:\", device)\n",
    "print(\"CUDA:\", torch.version.cuda if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    \"\"\"Factory function that creates a fresh LunarLander environment.\"\"\"\n",
    "    return gym.make(MODEL_NAME)\n",
    "\n",
    "env = DummyVecEnv([make_env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddceb419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_dqn = DQN(\n",
    "    policy=MLP_POLICY,\n",
    "    env=env,\n",
    "    exploration_fraction=0.12,\n",
    "    learning_rate=6.3e-4,\n",
    "    buffer_size=50_000,\n",
    "    batch_size=128,\n",
    "    gamma=0.99,\n",
    "    # Number of timesteps to collect before the first training update. \n",
    "    # During the first 500 steps, the agent only explores and fills the replay buffer.\n",
    "    learning_starts=0,\n",
    "    # Frequency (in timesteps) at which the target network is updated copying the weights from the main Q-network\n",
    "    target_update_interval=250,\n",
    "    # How often to perform a gradient update.\n",
    "    # With train_freq=4, the network is updated once every 4 environment steps.\n",
    "    train_freq=4,\n",
    "\n",
    "    verbose=1,  # verbose 0 for disable logs\n",
    "    seed=SEED,\n",
    "    tensorboard_log=TENSORBOARD_LOGS_DIR,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2c9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dqn = DQN(\n",
    "    policy=MLP_POLICY,\n",
    "    env=env,\n",
    "    exploration_fraction=0.12,\n",
    "    exploration_final_eps=0.02,\n",
    "    learning_rate=6.3e-4,\n",
    "    buffer_size=50_000,\n",
    "    batch_size=128,\n",
    "    gamma=0.99,\n",
    "    # Number of timesteps to collect before the first training update. \n",
    "    # During the first 500 steps, the agent only explores and fills the replay buffer.\n",
    "    learning_starts=0,\n",
    "    # Frequency (in timesteps) at which the target network is updated copying the weights from the main Q-network\n",
    "    target_update_interval=250,\n",
    "    # How often to perform a gradient update.\n",
    "    # With train_freq=4, the network is updated once every 4 environment steps.\n",
    "    train_freq=4,\n",
    "\n",
    "    verbose=1,  # verbose 0 for disable logs\n",
    "    seed=SEED,\n",
    "    tensorboard_log=TENSORBOARD_LOGS_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72529bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLoggingCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for logging exploration-related metrics during training.\n",
    "\n",
    "    This callback tracks:\n",
    "    - The exploration rate (epsilon) over time\n",
    "    - Episode rewards (one value per completed episode)\n",
    "    - The number of gradient update steps performed by the agent\n",
    "\n",
    "    The data collected here will later be used to plot training curves directly inside the notebook, complementing (but not replacing) TensorBoard.\n",
    "    \"\"\"\n",
    "    def __init__(self, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        # list to store the metrics\n",
    "        self.epsilon_history = []\n",
    "        self.reward_history = []\n",
    "        self.update_steps = []\n",
    "        self.episode_reward = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # epsilon info\n",
    "        if hasattr(self.model, \"exploration_rate\"):\n",
    "            self.epsilon_history.append(self.model.exploration_rate)\n",
    "\n",
    "        # reward info\n",
    "        reward = self.locals.get(\"rewards\")\n",
    "        if reward is not None:\n",
    "            self.episode_reward += reward[0]\n",
    "\n",
    "        # check if episode has end (done)\n",
    "        done = self.locals.get(\"dones\")\n",
    "        if done is not None and done[0]:\n",
    "            self.reward_history.append(self.episode_reward)\n",
    "            self.episode_reward = 0 # reset for next episode\n",
    "\n",
    "        # number of gradient updates performed\n",
    "        self.update_steps.append(self.model._n_updates)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f879e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#instanciate the callback for training metrics logging\n",
    "callback = DQNLoggingCallback()\n",
    "\n",
    "model_dqn.learn(\n",
    "    # total_timesteps=500_000,\n",
    "    total_timesteps=750_000,\n",
    "    callback = callback,\n",
    "    progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498dc2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# 1. EPSILON EVOLUTION\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(callback.epsilon_history)\n",
    "plt.title(\"Exploration Rate (Epsilon) over Time\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Epsilon\")\n",
    "plt.grid(True)\n",
    "\n",
    "# 2. EPISODE REWARD EVOLUTION\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(callback.reward_history)\n",
    "plt.title(\"Episode Reward over Training\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.grid(True)\n",
    "\n",
    "# 3. NUMBER OF GRADIENT UPDATE STEPS\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(callback.update_steps)\n",
    "plt.title(\"Number of Gradient Updates over Time\")\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Update Steps\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ad705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new, clean, evaluation environment (non-vectorized)\n",
    "eval_env = gym.make(MODEL_NAME)\n",
    "\n",
    "# run some episodes with the trained model, in the new environment\n",
    "n_eval_episodes=50\n",
    "episode_rewards, episode_lengths = evaluate_policy(\n",
    "    model_dqn,\n",
    "    eval_env,\n",
    "    n_eval_episodes=50,\n",
    "    deterministic=True,\n",
    "    return_episode_rewards=True,  # return per-episode returns\n",
    ")\n",
    "\n",
    "# Compute summary statistics manually\n",
    "mean_reward = np.mean(episode_rewards)\n",
    "std_reward = np.std(episode_rewards)\n",
    "\n",
    "print(f\"Mean reward over {n_eval_episodes} episodes: {mean_reward:.2f} Â± {std_reward:.2f}\")\n",
    "print(f\"Episode length range: min = {int(np.min(episode_lengths))}, max = {int(np.max(episode_lengths))}\")\n",
    "\n",
    "# Visualize the distribution of episode returns\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.hist(episode_rewards, bins=10, edgecolor=\"black\")\n",
    "plt.title(\"Distribution of episode returns (DQN on \" + MODEL_NAME + \")\")\n",
    "plt.xlabel(\"Total reward per episode\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Close the evaluation environment\n",
    "eval_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb513a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new environment for visualization only. Render_mode=\"rgb_array\" so that env.render() returns actual image frames.\n",
    "env = gym.make(MODEL_NAME, render_mode=\"rgb_array\")\n",
    "\n",
    "frames = [] # list to store each rendered frame of the episode\n",
    "\n",
    "# Reset the environment to get the initial observation (state)\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "\n",
    "# Run one full episode using the trained DQN agent\n",
    "while not done:\n",
    "    action, _ = model_dqn.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    # The episode ends either naturally (terminated) or by time limit (truncated)\n",
    "    done = terminated or truncated\n",
    "    frames.append(env.render())\n",
    "\n",
    "env.close()\n",
    "\n",
    "# Save all collected frames as an animated GIF\n",
    "imageio.mimsave(\"dqn_lunarlander.gif\", frames, duration=25)\n",
    "\n",
    "\n",
    "# Presenting the image\n",
    "Image(filename=\"dqn_lunarlander.gif\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f9daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving model to: {SAVE_MODEL_PATH}\")\n",
    "model_dqn.save(SAVE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe1cba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading the saved model...\")\n",
    "env2 = DummyVecEnv([make_env])\n",
    "loaded_model = DQN.load(SAVE_MODEL_PATH, env=env2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05224955",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env2.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _ = loaded_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env2.step(action)\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"Total reward by the loaded model: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_taap_p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
