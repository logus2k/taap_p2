{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# PPO Multi-Seed Training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import imageio\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Configuration\n",
    "\n",
    "SEED_LIST = [42, 123, 3407]\n",
    "\n",
    "SELECTED_ALGORITHM = \"ppo\"\n",
    "ALGORITHM_CLASS = PPO\n",
    "\n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "OUTPUT_DIR = os.path.join(NOTEBOOK_DIR, \"outputs_\" + SELECTED_ALGORITHM)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "TENSORBOARD_LOGS_DIR = os.path.join(OUTPUT_DIR, \"tensorboard\")\n",
    "MODELS_DIR = os.path.join(NOTEBOOK_DIR, \"../../../models\", SELECTED_ALGORITHM)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "GYMNASIUM_MODEL = \"LunarLander-v3\"\n",
    "MLP_POLICY = \"MlpPolicy\"\n",
    "\n",
    "WIND_ENABLED = False\n",
    "\n",
    "TOTAL_TIMESTEPS = 1_000_000\n",
    "EVALUATION_EPISODES = 20\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "print(f\"Algorithm: {SELECTED_ALGORITHM.upper()}\")\n",
    "print(f\"Seeds: {SEED_LIST}\")\n",
    "print(f\"Wind enabled: {WIND_ENABLED}\")\n",
    "print(f\"Total timesteps per seed: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"Evaluation episodes per seed: {EVALUATION_EPISODES}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"CUDA:\", torch.version.cuda if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment inspection (run once, not per seed)\n",
    "env_tmp = gym.make(GYMNASIUM_MODEL)\n",
    "\n",
    "print(\"Observation space:\", env_tmp.observation_space)\n",
    "print(\"Action space:\", env_tmp.action_space)\n",
    "\n",
    "obs, info = env_tmp.reset()\n",
    "print(\"Initial observation:\", obs)\n",
    "\n",
    "env_tmp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOLoggingCallback(BaseCallback):\n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.policy_loss = []\n",
    "        self.value_loss = []\n",
    "        self.entropy = []\n",
    "\n",
    "        self._current_rewards: np.ndarray = np.array([])\n",
    "        self._current_lengths: np.ndarray = np.array([])\n",
    "        self._plot_handle = None\n",
    "        self._stats_handle = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        n_envs = self.training_env.num_envs\n",
    "        self._current_rewards = np.zeros(n_envs, dtype=np.float32)\n",
    "        self._current_lengths = np.zeros(n_envs, dtype=np.int32)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals.get(\"rewards\")\n",
    "        dones = self.locals.get(\"dones\")\n",
    "\n",
    "        if rewards is not None and dones is not None:\n",
    "            self._current_rewards += rewards\n",
    "            self._current_lengths += 1\n",
    "\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:\n",
    "                    self.episode_rewards.append(float(self._current_rewards[i]))\n",
    "                    self.episode_lengths.append(int(self._current_lengths[i]))\n",
    "                    ep = len(self.episode_rewards)\n",
    "\n",
    "                    if ep % 10 == 0:\n",
    "                        recent = np.array(self.episode_rewards[-50:])\n",
    "                        stats_text = (\n",
    "                            f'Episode {ep} | Last {len(recent)} Ep \\u2014 '\n",
    "                            f'Mean: {np.mean(recent):.1f} | Std: {np.std(recent):.1f} | '\n",
    "                            f'Min: {np.min(recent):.1f} | Max: {np.max(recent):.1f} | '\n",
    "                            f'Success: {(recent >= 200).sum() / len(recent) * 100:.0f}%'\n",
    "                        )\n",
    "                        if self._stats_handle is None:\n",
    "                            self._stats_handle = display(stats_text, display_id=True)\n",
    "                        else:\n",
    "                            self._stats_handle.update(stats_text)\n",
    "\n",
    "                    if ep % 50 == 0:\n",
    "                        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "                        ax1.plot(self.episode_rewards, alpha=0.3, color='gray')\n",
    "                        window = min(50, len(self.episode_rewards))\n",
    "                        rolling = pd.Series(self.episode_rewards).rolling(window).mean()\n",
    "                        ax1.plot(rolling, color='blue', linewidth=2)\n",
    "                        ax1.axhline(y=200, color='red', linestyle='--')\n",
    "                        ax1.set_title(f'Episode Reward \\u2014 Ep {ep}')\n",
    "                        ax1.set_xlabel('Episode')\n",
    "                        ax1.set_ylabel('Reward')\n",
    "                        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "                        if self.value_loss:\n",
    "                            ax2.plot(self.value_loss, color='green', alpha=0.7)\n",
    "                            ax2.set_title('Value Loss')\n",
    "                            ax2.set_xlabel('Rollout')\n",
    "                            ax2.set_ylabel('Loss')\n",
    "                            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "                        plt.tight_layout()\n",
    "\n",
    "                        if self._plot_handle is None:\n",
    "                            self._plot_handle = display(fig, display_id=True)\n",
    "                        else:\n",
    "                            self._plot_handle.update(fig)\n",
    "                        plt.close(fig)\n",
    "\n",
    "                    self._current_rewards[i] = 0\n",
    "                    self._current_lengths[i] = 0\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        logger_data = self.model.logger.name_to_value\n",
    "        if \"train/policy_loss\" in logger_data:\n",
    "            self.policy_loss.append(logger_data[\"train/policy_loss\"])\n",
    "        if \"train/value_loss\" in logger_data:\n",
    "            self.value_loss.append(logger_data[\"train/value_loss\"])\n",
    "        if \"train/entropy_loss\" in logger_data:\n",
    "            self.entropy.append(-logger_data[\"train/entropy_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop over all seeds\n",
    "\n",
    "training_results = {}\n",
    "\n",
    "for seed in SEED_LIST:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training with seed {seed}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    def make_env(s=seed):\n",
    "        env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "        env.reset(seed=s)\n",
    "        return env\n",
    "\n",
    "    env = DummyVecEnv([make_env])\n",
    "    env.seed(seed)\n",
    "\n",
    "    params = {\n",
    "        \"policy\": MLP_POLICY,\n",
    "        \"env\": env,\n",
    "        \"learning_rate\": 2.5e-4,\n",
    "        \"n_steps\": 2048,\n",
    "        \"batch_size\": 64,\n",
    "        \"n_epochs\": 10,\n",
    "        \"gamma\": 0.999,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"clip_range\": 0.2,\n",
    "        \"device\": DEVICE,\n",
    "        \"seed\": seed,\n",
    "        \"tensorboard_log\": TENSORBOARD_LOGS_DIR,\n",
    "    }\n",
    "\n",
    "    callback = PPOLoggingCallback()\n",
    "    model = ALGORITHM_CLASS(**params)\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=callback, progress_bar=True)\n",
    "\n",
    "    save_path = os.path.join(MODELS_DIR, f\"lab005_{SELECTED_ALGORITHM}_{seed}\")\n",
    "    model.save(save_path)\n",
    "    print(f\"Model saved to: {save_path}\")\n",
    "\n",
    "    training_results[seed] = callback\n",
    "\n",
    "    env.close()\n",
    "\n",
    "print(f\"\\nAll {len(SEED_LIST)} models trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Seed: Episode Reward over Training\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    ax.plot(training_results[seed].episode_rewards, alpha=0.7)\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Total Reward\")\n",
    "fig.suptitle(\"Episode Reward over Training\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Seed: Episode Length over Training\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    ax.plot(training_results[seed].episode_lengths, alpha=0.7, color=\"orange\")\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Number of Steps\")\n",
    "fig.suptitle(\"Episode Length over Training\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Seed: Value Loss over Rollouts\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    ax.plot(training_results[seed].value_loss, alpha=0.7, color=\"green\")\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.set_xlabel(\"Rollout\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Loss Value\")\n",
    "fig.suptitle(\"Value Loss over Rollouts\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Seed: Entropy over Rollouts\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    ax.plot(training_results[seed].entropy, alpha=0.7, color=\"purple\")\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.set_xlabel(\"Rollout\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Entropy (Positive)\")\n",
    "fig.suptitle(\"Entropy over Rollouts\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated: Rolling Reward Overlay (all seeds)\n",
    "\n",
    "colors = plt.cm.tab10.colors\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i, seed in enumerate(SEED_LIST):\n",
    "    rewards = training_results[seed].episode_rewards\n",
    "    rolling = pd.Series(rewards).rolling(50).mean()\n",
    "    plt.plot(rolling, color=colors[i], linewidth=2, label=f\"Seed {seed}\")\n",
    "\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.title(f\"{SELECTED_ALGORITHM.upper()} Training: Rolling Mean Reward (window=50)\", fontsize=14)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: 20 deterministic episodes per seed\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "for seed in SEED_LIST:\n",
    "    print(f\"Evaluating model for seed {seed}...\")\n",
    "\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    load_path = os.path.join(MODELS_DIR, f\"lab005_{SELECTED_ALGORITHM}_{seed}\")\n",
    "\n",
    "    def make_eval_env(s=seed):\n",
    "        env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "        env.reset(seed=s)\n",
    "        return env\n",
    "\n",
    "    eval_model = ALGORITHM_CLASS.load(load_path, env=DummyVecEnv([make_eval_env]), device=DEVICE)\n",
    "\n",
    "    eval_env = Monitor(gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED))\n",
    "    eval_env.reset(seed=seed)\n",
    "\n",
    "    rewards, _ = evaluate_policy(\n",
    "        eval_model,\n",
    "        eval_env,\n",
    "        n_eval_episodes=EVALUATION_EPISODES,\n",
    "        deterministic=True,\n",
    "        return_episode_rewards=True\n",
    "    )\n",
    "\n",
    "    evaluation_results[seed] = np.array(rewards)\n",
    "    eval_env.close()\n",
    "\n",
    "print(f\"\\nEvaluation complete for all {len(SEED_LIST)} seeds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Summary Table\n",
    "\n",
    "rows = []\n",
    "for seed in SEED_LIST:\n",
    "    r = evaluation_results[seed]\n",
    "    rows.append({\n",
    "        \"Seed\": seed,\n",
    "        \"Mean Reward\": f\"{np.mean(r):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(r):.2f}\",\n",
    "        \"Min Reward\": f\"{np.min(r):.2f}\",\n",
    "        \"Max Reward\": f\"{np.max(r):.2f}\",\n",
    "        \"Success Rate\": f\"{(r >= 200).sum() / len(r) * 100:.1f}%\"\n",
    "    })\n",
    "\n",
    "all_rewards = np.concatenate(list(evaluation_results.values()))\n",
    "rows.append({\n",
    "    \"Seed\": \"Overall\",\n",
    "    \"Mean Reward\": f\"{np.mean(all_rewards):.2f}\",\n",
    "    \"Std Dev\": f\"{np.std(all_rewards):.2f}\",\n",
    "    \"Min Reward\": f\"{np.min(all_rewards):.2f}\",\n",
    "    \"Max Reward\": f\"{np.max(all_rewards):.2f}\",\n",
    "    \"Success Rate\": f\"{(all_rewards >= 200).sum() / len(all_rewards) * 100:.1f}%\"\n",
    "})\n",
    "\n",
    "df_summary = pd.DataFrame(rows)\n",
    "print(f\"*** {SELECTED_ALGORITHM.upper()} MULTI-SEED EVALUATION SUMMARY ***\")\n",
    "print(f\"Episodes per seed: {EVALUATION_EPISODES}\")\n",
    "print(f\"Total episodes: {len(all_rewards)}\")\n",
    "print()\n",
    "print(df_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Seed: Evaluation Convergence Plots\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    rewards = evaluation_results[seed]\n",
    "    episodes = np.arange(1, len(rewards) + 1)\n",
    "    running_mean = np.cumsum(rewards) / episodes\n",
    "\n",
    "    ax.scatter(episodes, rewards, color='gray', alpha=0.4, s=20)\n",
    "    ax.plot(episodes, running_mean, color='blue', linewidth=2)\n",
    "    ax.axhline(y=200, color='red', linestyle='--')\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Reward\")\n",
    "fig.suptitle(f\"{SELECTED_ALGORITHM.upper()} Evaluation: {EVALUATION_EPISODES} Episodes per Seed\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated: Evaluation Bar Chart (mean reward per seed with error bars)\n",
    "\n",
    "means = [np.mean(evaluation_results[s]) for s in SEED_LIST]\n",
    "stds = [np.std(evaluation_results[s]) for s in SEED_LIST]\n",
    "labels = [str(s) for s in SEED_LIST]\n",
    "\n",
    "plt.figure(figsize=(max(8, 3 * len(SEED_LIST)), 6))\n",
    "bars = plt.bar(labels, means, yerr=stds, capsize=5, color=colors[:len(SEED_LIST)], alpha=0.8)\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.axhline(y=np.mean(all_rewards), color='blue', linestyle='-', linewidth=2,\n",
    "            label=f'Overall Mean ({np.mean(all_rewards):.1f})')\n",
    "\n",
    "plt.title(f\"{SELECTED_ALGORITHM.upper()} Mean Reward per Seed ({EVALUATION_EPISODES} episodes each)\", fontsize=14)\n",
    "plt.xlabel(\"Seed\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIF Visualization (one per seed)\n",
    "\n",
    "for seed in SEED_LIST:\n",
    "    print(f\"Generating GIF for seed {seed}...\")\n",
    "\n",
    "    load_path = os.path.join(MODELS_DIR, f\"lab005_{SELECTED_ALGORITHM}_{seed}\")\n",
    "\n",
    "    def make_vis_env(s=seed):\n",
    "        env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "        env.reset(seed=s)\n",
    "        return env\n",
    "\n",
    "    vis_model = ALGORITHM_CLASS.load(load_path, env=DummyVecEnv([make_vis_env]), device=DEVICE)\n",
    "\n",
    "    vis_env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "    frames = []\n",
    "    obs, info = vis_env.reset(seed=seed)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action, _ = vis_model.predict(obs, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = vis_env.step(action)\n",
    "        done = terminated or truncated\n",
    "        frames.append(vis_env.render())\n",
    "\n",
    "    vis_env.close()\n",
    "\n",
    "    gif_path = os.path.join(OUTPUT_DIR, f\"{SELECTED_ALGORITHM}_seed{seed}.gif\")\n",
    "    imageio.mimsave(gif_path, frames, fps=30)\n",
    "    print(f\"  Saved: {gif_path}\")\n",
    "    display(Image(filename=gif_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}