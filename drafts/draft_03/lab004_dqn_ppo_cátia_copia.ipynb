{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7699987",
   "metadata": {},
   "source": [
    "# DQN and PPO\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c89ee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import imageio\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae69d77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# \"dqn\" or \"ppo\"\n",
    "SELECTED_ALGORITHM = \"dqn\"\n",
    "\n",
    "OUTPUT_DIR = \"./outputs_\" + SELECTED_ALGORITHM + \"/\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "TENSORBOARD_LOGS_DIR = OUTPUT_DIR + \"tensorboard/\"\n",
    "SAVE_MODEL_PATH = OUTPUT_DIR + \"model_\" + SELECTED_ALGORITHM + \".zip\"\n",
    "\n",
    "GYMNASIUM_MODEL = \"LunarLander-v3\"\n",
    "MLP_POLICY = \"MlpPolicy\"\n",
    "\n",
    "# Environment settings\n",
    "WIND_ENABLED = False\n",
    "\n",
    "MODEL = None\n",
    "CALLBACK = None\n",
    "TOTAL_TIMESTEPS = 1_000_000\n",
    "LEARNING_STARTS = 1000\n",
    "\n",
    "EVALUATION_EPISODES = 100\n",
    "\n",
    "# Use GPU if available, otherwise fallback to CPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657c3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "\n",
    "# Ensure reproducibility on Numpy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Even when running on GPU, many operations still use CPU-side torch RNG\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Ensure reproducibility on Torch\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b84d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"CUDA:\", torch.version.cuda if torch.cuda.is_available() else \"None\")\n",
    "\n",
    "print(\"Base output directory:\", OUTPUT_DIR)\n",
    "print(\"TensorBoard logs directory:\", TENSORBOARD_LOGS_DIR)\n",
    "print(\"Model save path:\", SAVE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6215fe43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary environment instance (for inspection only, not for training)\n",
    "env_tmp = gym.make(GYMNASIUM_MODEL)\n",
    "\n",
    "# Print the observation space: min/max values for position and velocity\n",
    "print(\"Observation space:\", env_tmp.observation_space)\n",
    "# Print the action space: valid range of the continuous acceleration input\n",
    "print(\"Action space:\", env_tmp.action_space)\n",
    "\n",
    "# Reset the environment and display the initial state (position and velocity)\n",
    "obs, info = env_tmp.reset()\n",
    "print(\"Initial observation:\", obs)\n",
    "\n",
    "env_tmp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env_lunarlander():\n",
    "    \n",
    "    # It is a good practice to set the seed inside the factory if not using VecEnv.seed()\n",
    "    env = gym.make(GYMNASIUM_MODEL,\n",
    "                   render_mode=\"rgb_array\",\n",
    "                   enable_wind=WIND_ENABLED)\n",
    "    \n",
    "    # Redundant if env_mc.seed(SEED) is used\n",
    "    env.reset(seed=SEED)\n",
    "    return env\n",
    "\n",
    "env_mc = DummyVecEnv([make_env_lunarlander])\n",
    "env_mc.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6756849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLoggingCallback(BaseCallback):\n",
    "    \n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.value_loss = []\n",
    "        self.entropy = []\n",
    "        \n",
    "        self._current_reward = 0.0\n",
    "        self._current_length = 0\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # 1. Safely get the values\n",
    "        rewards = self.locals.get(\"rewards\")\n",
    "        dones = self.locals.get(\"dones\")\n",
    "\n",
    "        # 2. Type Guard: Only proceed if they are NOT None\n",
    "        if rewards is not None and dones is not None:\n",
    "            # rewards and dones are usually numpy arrays or lists\n",
    "            reward = rewards[0]\n",
    "            done = dones[0]\n",
    "\n",
    "            self._current_reward += float(reward)\n",
    "            self._current_length += 1\n",
    "\n",
    "            if done:\n",
    "                self.episode_rewards.append(self._current_reward)\n",
    "                self.episode_lengths.append(self._current_length)\n",
    "                self._current_reward = 0.0\n",
    "                self._current_length = 0\n",
    "                \n",
    "        return True\n",
    "    \n",
    "    def _on_rollout_end(self) -> None:\n",
    "        logger_data = self.model.logger.name_to_value\n",
    "        \n",
    "        # DQN uses \"train/loss\" instead of \"train/value_loss\"\n",
    "        if \"train/loss\" in logger_data:\n",
    "            self.value_loss.append(logger_data[\"train/loss\"])\n",
    "            \n",
    "        # DQN doesn't have Entropy; it has exploration rate\n",
    "        if \"rollout/exploration_rate\" in logger_data:\n",
    "            self.entropy.append(logger_data[\"rollout/exploration_rate\"])    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72529bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOLoggingCallback(BaseCallback):\n",
    "    def __init__(self, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        # Initialize as empty arrays, not None, to satisfy Pylance\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.policy_loss = []\n",
    "        self.value_loss = []\n",
    "        self.entropy = []\n",
    "        \n",
    "        # Use type hints to stop the \"None is not subscriptable\" error\n",
    "        self._current_rewards: np.ndarray = np.array([])\n",
    "        self._current_lengths: np.ndarray = np.array([])\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        n_envs = self.training_env.num_envs\n",
    "        self._current_rewards = np.zeros(n_envs, dtype=np.float32)\n",
    "        self._current_lengths = np.zeros(n_envs, dtype=np.int32)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        # Pylance fix: Explicitly cast or check if None\n",
    "        rewards = self.locals.get(\"rewards\")\n",
    "        dones = self.locals.get(\"dones\")\n",
    "        \n",
    "        if rewards is not None and dones is not None:\n",
    "            self._current_rewards += rewards\n",
    "            self._current_lengths += 1\n",
    "\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:\n",
    "                    self.episode_rewards.append(float(self._current_rewards[i]))\n",
    "                    self.episode_lengths.append(int(self._current_lengths[i]))\n",
    "                    self._current_rewards[i] = 0\n",
    "                    self._current_lengths[i] = 0\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        # Return nothing to match base class signature\n",
    "        logger_data = self.model.logger.name_to_value\n",
    "        if \"train/policy_loss\" in logger_data:\n",
    "            self.policy_loss.append(logger_data[\"train/policy_loss\"])\n",
    "\n",
    "        # Value loss → critic learning stability\n",
    "        if \"train/value_loss\" in logger_data:\n",
    "            self.value_loss.append(logger_data[\"train/value_loss\"])\n",
    "\n",
    "        # Entropy loss is negative → invert sign to get entropy magnitude\n",
    "        if \"train/entropy_loss\" in logger_data:\n",
    "            self.entropy.append(-logger_data[\"train/entropy_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b97bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping dictionary\n",
    "params = {}\n",
    "ALGORITHM_MAP = {\"ppo\": PPO, \"dqn\": DQN}\n",
    "ALGORITHM_CLASS = ALGORITHM_MAP[SELECTED_ALGORITHM]\n",
    "\n",
    "if SELECTED_ALGORITHM == \"dqn\":\n",
    "    params = {\n",
    "        \"policy\": MLP_POLICY,\n",
    "        \"env\": env_mc,\n",
    "        \"learning_rate\": 5e-4,\n",
    "        \"learning_starts\": LEARNING_STARTS,\n",
    "        \"buffer_size\": 100_000,\n",
    "        \"batch_size\": 128,\n",
    "        \"gamma\": 0.99,\n",
    "        \"device\": DEVICE,\n",
    "        \"seed\": SEED,\n",
    "        \"tensorboard_log\": TENSORBOARD_LOGS_DIR,\n",
    "    }\n",
    "    CALLBACK = DQNLoggingCallback()\n",
    "\n",
    "elif SELECTED_ALGORITHM == \"ppo\":\n",
    "    params = {\n",
    "        \"policy\": MLP_POLICY,\n",
    "        \"env\": env_mc,\n",
    "        \"learning_rate\": 2.5e-4,\n",
    "        \"n_steps\": 2048,\n",
    "        \"batch_size\": 64,\n",
    "        \"n_epochs\": 10,\n",
    "        \"gamma\": 0.999,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"clip_range\": 0.2,\n",
    "        \"device\": DEVICE,\n",
    "        \"seed\": SEED,\n",
    "        \"tensorboard_log\": TENSORBOARD_LOGS_DIR,\n",
    "    }\n",
    "    CALLBACK = PPOLoggingCallback()\n",
    "\n",
    "else:\n",
    "    raise ValueError(\"Invalid algorithm selected.\")\n",
    "\n",
    "# Instantiate using the generic ALGO_CLASS\n",
    "MODEL = ALGORITHM_CLASS(**params)\n",
    "\n",
    "MODEL.learn(\n",
    "    total_timesteps=TOTAL_TIMESTEPS,\n",
    "    callback=CALLBACK,\n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498dc2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(CALLBACK.episode_rewards, label=\"Episode Reward\", alpha=0.7)\n",
    "plt.title(\"Episode Reward over Training\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24451b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(CALLBACK.episode_lengths, label=\"Episode Length\", alpha=0.7, color=\"orange\")\n",
    "plt.title(\"Episode Length over Training\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Number of Steps\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e8abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(CALLBACK.value_loss, label=\"Value Loss\", alpha=0.7, color=\"green\")\n",
    "plt.title(\"Value Loss over Rollouts\")\n",
    "plt.xlabel(\"Rollout\")\n",
    "plt.ylabel(\"Loss Value\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e4d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "if SELECTED_ALGORITHM == \"dqn\":\n",
    "    label = \"Exploration Rate (ε)\"\n",
    "    title = \"Exploration Rate over Rollouts\"\n",
    "    ylabel = \"Epsilon\"\n",
    "else:\n",
    "    label = \"Entropy\"\n",
    "    title = \"Entropy over Rollouts\"\n",
    "    ylabel = \"Entropy (Positive)\"\n",
    "\n",
    "plt.plot(CALLBACK.entropy, label=label, alpha=0.7, color=\"purple\")\n",
    "plt.title(title)\n",
    "plt.xlabel(\"Rollout\")\n",
    "plt.ylabel(ylabel)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new dedicated environment for evaluation\n",
    "evaluation_environment = Monitor(gym.make(GYMNASIUM_MODEL))\n",
    "\n",
    "print(f\"Starting Test: {EVALUATION_EPISODES} episodes...\")\n",
    "\n",
    "episode_rewards, _ = evaluate_policy(\n",
    "    MODEL,\n",
    "    evaluation_environment,\n",
    "    n_eval_episodes=EVALUATION_EPISODES,\n",
    "    deterministic=True,\n",
    "    return_episode_rewards=True\n",
    ")\n",
    "\n",
    "# Show statistics\n",
    "rewards = np.array(episode_rewards)\n",
    "stats = {\n",
    "    \"Metric\": [\"Mean Reward\", \"Standard Deviation\", \"Minimum Reward\", \"Maximum Reward\", \"Success Rate (Score > 200)\"],\n",
    "    \"Value\": [\n",
    "        f\"{np.mean(rewards):.2f}\",\n",
    "        f\"{np.std(rewards):.2f}\",\n",
    "        f\"{np.min(rewards):.2f}\",\n",
    "        f\"{np.max(rewards):.2f}\",\n",
    "        f\"{(rewards >= 200).sum() / EVALUATION_EPISODES * 100:.1f}%\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Display a summary table\n",
    "df_stats = pd.DataFrame(stats)\n",
    "print(\"\\n*** MODEL EVALUATION SUMMARY ***\")\n",
    "print(df_stats.to_string(index=False))\n",
    "\n",
    "# Generate Episodes Convergence Plot\n",
    "episodes = np.arange(1, len(rewards) + 1)\n",
    "running_mean = np.cumsum(rewards) / episodes\n",
    "running_std = np.array([np.std(rewards[:i]) for i in episodes])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(episodes, rewards, color='gray', alpha=0.2, s=8, label='Episode Reward')\n",
    "plt.plot(episodes, running_mean, color='blue', linewidth=2, label='Running Mean')\n",
    "plt.fill_between(episodes, running_mean - running_std, running_mean + running_std, color='blue', alpha=0.15, label='Confidence Interval (Std Dev)')\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "\n",
    "plt.title(f\"{SELECTED_ALGORITHM.upper()} Stability Stress Test: {EVALUATION_EPISODES} Episodes\", fontsize=14)\n",
    "plt.xlabel('Episode', fontsize=12)\n",
    "plt.ylabel('Reward', fontsize=12)\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "evaluation_environment.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb513a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a new dedicated environment for recording a visualization\n",
    "visualization_environment = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\")\n",
    "\n",
    "frames = []\n",
    "obs, info = visualization_environment.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    \n",
    "    # Output a continuous action\n",
    "    action, _ = MODEL.predict(obs, deterministic=True)\n",
    "\n",
    "    obs, reward, terminated, truncated, info = visualization_environment.step(action)\n",
    "    done = terminated or truncated\n",
    "\n",
    "    # Capture frame\n",
    "    frames.append(visualization_environment.render())\n",
    "\n",
    "visualization_environment.close()\n",
    "\n",
    "\n",
    "gif_path = os.path.join(OUTPUT_DIR, SELECTED_ALGORITHM + \"_lunarlander.gif\")\n",
    "\n",
    "# Save a GIF at 30 frames per second\n",
    "imageio.mimsave(gif_path, frames, fps=30)\n",
    "\n",
    "print(f\"Saved visualization to: {gif_path}\")\n",
    "Image(filename=gif_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb684de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving model to: {SAVE_MODEL_PATH}\")\n",
    "MODEL.save(SAVE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798a6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Load the saved {SELECTED_ALGORITHM.upper()} model...\")\n",
    "\n",
    "# Instantiate a new dedicated environment for restoring the saved model\n",
    "restore_environment = DummyVecEnv([make_env_lunarlander])\n",
    "\n",
    "# Call .load() on the CLASS, not the instance\n",
    "restored_model = ALGORITHM_CLASS.load(SAVE_MODEL_PATH, env=restore_environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c1d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = restore_environment.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "\n",
    "while not done:\n",
    "    action, _ = restored_model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = restore_environment.step(action)\n",
    "    total_reward += reward.item()\n",
    "    done = done.item()\n",
    "\n",
    "print(f\"Total reward by the loaded model: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_taap_p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
