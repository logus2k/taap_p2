{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN & PPO Multi-Seed Training & Reporting\n",
    "\n",
    "This notebook trains both DQN and PPO across multiple seeds, with:\n",
    "- **Periodic checkpoints** saved every N episodes\n",
    "- **Best-model tracking** using a combined metric (mean reward - std reward)\n",
    "- **Timestamped run folders** for organized model storage\n",
    "\n",
    "After training, models are **reloaded from disk** and evaluated to produce per-algorithm,\n",
    "per-seed, and aggregated comparison charts, statistical tests, and behavior analysis.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Deep Reinforcement Learning (DRL) has become a central paradigm for solving complex sequential decision-making problems, particularly those involving continuous state spaces, stochastic dynamics, and delayed rewards. Among the wide range of DRL algorithms, Deep Q-Network (DQN) and Proximal Policy Optimization (PPO) represent two fundamentally different approaches: value-based learning and policy-based learning. Comparing these two algorithms provides valuable insight into how different learning mechanisms behave under the same environment conditions.\n",
    "\n",
    "In this project, we investigate the performance of DQN and PPO in the LunarLander-v3 environment from Gymnasium. This environment simulates the control of a lunar lander that must descend and touch down smoothly within a designated landing zone. The agent receives an 8-dimensional continuous state vector and must choose among four discrete actions, making the task suitable for both value-based and policy-based methods. The environment includes dense rewards, penalties for fuel consumption, and strong negative rewards for crashes, creating a challenging control problem that requires stability, precision, and efficient exploration.\n",
    "\n",
    "The goal of this work is to train, evaluate, and compare DQN and PPO under controlled experimental conditions. We analyse their learning curves, sample efficiency, stability across multiple random seeds, and qualitative behaviour through recorded simulations. Additionally, we examine theoretical aspects such as overestimation bias in DQN, the role of clipping and Generalized Advantage Estimation (GAE) in PPO, and the differences between epsilon-greedy and entropy-driven exploration.\n",
    "\n",
    "By combining quantitative metrics with qualitative observations, this study aims to provide a comprehensive understanding of how DQN and PPO behave in the LunarLander-v3 environment, highlighting their strengths, limitations, and the impact of hyperparameter choices on the final performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Analysis\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 State Space\n",
    "\n",
    "The LunarLander-v3 environment provides an 8-dimensional continuous state vector describing the physical configuration of the lander at each timestep:\n",
    "1. Horizontal position (x) relative to the landing zone\n",
    "2. Vertical position (y) above the ground\n",
    "3. Horizontal velocity (vx)\n",
    "4. Vertical velocity (vy)\n",
    "5. Lander angle ($\\theta$)\n",
    "6. Angular velocity ($\\omega$)\n",
    "7. Left leg contact indicator (0 or 1)\n",
    "8. Right leg contact indicator (0 or 1)\n",
    "\n",
    "These variables allow the agent to infer its location, orientation, stability, and motion, which are essential for controlling the descent.\n",
    "\n",
    "### 2.2 Action Space\n",
    "\n",
    "The action space is discrete with four possible actions:\n",
    "\n",
    "* 0 - Do nothing\n",
    "* 1 - Fire left thruster (pushes the lander to the right)\n",
    "* 2 - Fire main thruster (reduces vertical speed)\n",
    "* 3 - Fire right thruster (pushes the lander to the left)\n",
    "\n",
    "These actions allow the agent to control both horizontal movement and vertical descent.\n",
    "\n",
    "### 2.3 Reward Function\n",
    "\n",
    "The reward function is dense and designed to encourage smooth and stable landings while penalizing unsafe or inefficient behaviour.\n",
    "\n",
    "**Positive rewards include:**\n",
    "- Moving closer to the landing zone\n",
    "- Reducing horizontal and vertical velocity\n",
    "- Maintaining a stable angle\n",
    "- Each leg touching the ground (+10 each)\n",
    "- Successful landing (+100 to +140)\n",
    "\n",
    "**Penalties include:**\n",
    "- High velocities\n",
    "- Large tilt angles\n",
    "- Excessive use of the main thruster (fuel cost)\n",
    "- Crashes (around -100)\n",
    "\n",
    "A score of **200 or higher** typically indicates a successful landing.\n",
    "\n",
    "### 2.4 Termination Rules\n",
    "\n",
    "An episode terminates when one of the following occurs:\n",
    "1. **Successful landing** within the designated zone\n",
    "2. **Crash** due to excessive speed or unstable angle\n",
    "3. **Leaving the screen boundaries**\n",
    "4. **Reaching the maximum number of steps** allowed by the environment (1000 timesteps)\n",
    "\n",
    "These termination conditions apply equally to both PPO and DQN agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random, time, glob\n",
    "from datetime import datetime\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import imageio\n",
    "from IPython.display import Image, Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Configuration\n",
    "\n",
    "SEED_LIST = [42, 123, 3407]\n",
    "\n",
    "ALGORITHM_MAP = {\n",
    "    \"dqn\": DQN,\n",
    "    \"ppo\": PPO,\n",
    "}\n",
    "\n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "GYMNASIUM_MODEL = \"LunarLander-v3\"\n",
    "MLP_POLICY = \"MlpPolicy\"\n",
    "\n",
    "WIND_ENABLED = False\n",
    "\n",
    "TOTAL_TIMESTEPS = 1_500_000\n",
    "EVALUATION_EPISODES = 20\n",
    "\n",
    "# Session prefix — used in final model filenames (e.g. lab010_dqn_42.zip)\n",
    "SESSION_PREFIX = \"lab011\"\n",
    "\n",
    "# Update live stats and plots every N episodes\n",
    "CHART_UPDATE_FREQ = 10\n",
    "\n",
    "# Save a training checkpoint every N episodes\n",
    "CHECKPOINT_FREQ_EPISODES = 100\n",
    "\n",
    "# EvalCallback: evaluate the model every N timesteps\n",
    "EVAL_FREQ_TIMESTEPS = 25_000\n",
    "\n",
    "# EvalCallback: number of episodes per evaluation\n",
    "EVAL_N_EPISODES = 20\n",
    "\n",
    "# Solved threshold: only prefer models with mean reward >= this value\n",
    "SOLVED_THRESHOLD = 200\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "\n",
    "def linear_schedule(initial_value: float) -> Callable[[float], float]:\n",
    "    \"\"\"\n",
    "    Linear learning rate schedule.\n",
    "    \"\"\"\n",
    "    def func(progress_remaining: float) -> float:\n",
    "        \"\"\"\n",
    "        Progress decreases from 1 (beginning) to 0 (end)\n",
    "        \"\"\"\n",
    "        return progress_remaining * initial_value\n",
    "    return func\n",
    "\n",
    "\n",
    "# Per-algorithm hyperparameters\n",
    "ALGO_PARAMS = {\n",
    "    \"dqn\": {\n",
    "        \"policy\": MLP_POLICY,\n",
    "        \n",
    "        # Linear Schedule allows weights to settle perfectly at the end\n",
    "        \"learning_rate\": linear_schedule(6.3e-4), \n",
    "        \n",
    "        \"learning_starts\": 50_000,\n",
    "        \n",
    "        # Massive buffer prevents forgetting recovery maneuvers\n",
    "        \"buffer_size\": 750_000, \n",
    "        \"batch_size\": 128,\n",
    "        \"gamma\": 0.99,\n",
    "        \n",
    "        \"exploration_fraction\": 0.12,  \n",
    "        \n",
    "        # High final epsilon forces the agent to keep learning recoveries\n",
    "        \"exploration_final_eps\": 0.1,  \n",
    "        \n",
    "        # Standard Zoo update mechanics\n",
    "        \"target_update_interval\": 250, \n",
    "        \"train_freq\": 4,\n",
    "        # Takes 4 gradient updates every 4 env steps\n",
    "        \"gradient_steps\": 4,\n",
    "        \n",
    "        \"policy_kwargs\": dict(net_arch=[256, 256]),\n",
    "        \"device\": DEVICE,\n",
    "    },\n",
    "    \"ppo\": {\n",
    "        \"learning_rate\": 2.5e-4,\n",
    "        \"n_steps\": 2048,\n",
    "        \"batch_size\": 64,\n",
    "        \"n_epochs\": 10,\n",
    "        \"gamma\": 0.999,\n",
    "        \"gae_lambda\": 0.95,\n",
    "        \"ent_coef\": 0.01,\n",
    "        \"clip_range\": 0.2,\n",
    "    },\n",
    "}\n",
    "\n",
    "print(f\"Session prefix: {SESSION_PREFIX}\")\n",
    "print(f\"Algorithms: {list(ALGORITHM_MAP.keys())}\")\n",
    "print(f\"Seeds: {SEED_LIST}\")\n",
    "print(f\"Wind enabled: {WIND_ENABLED}\")\n",
    "print(f\"Total timesteps per seed: {TOTAL_TIMESTEPS:,}\")\n",
    "print(f\"Evaluation episodes per seed: {EVALUATION_EPISODES}\")\n",
    "print(f\"Chart update frequency: every {CHART_UPDATE_FREQ} episodes\")\n",
    "print(f\"Checkpoint frequency: every {CHECKPOINT_FREQ_EPISODES} episodes\")\n",
    "print(f\"Eval callback frequency: every {EVAL_FREQ_TIMESTEPS:,} timesteps ({EVAL_N_EPISODES} episodes)\")\n",
    "print(f\"Solved threshold: {SOLVED_THRESHOLD}\")\n",
    "print(f\"Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic Experimental Setup — generated from config variables\n",
    "\n",
    "dqn_p = ALGO_PARAMS[\"dqn\"]\n",
    "ppo_p = ALGO_PARAMS[\"ppo\"]\n",
    "\n",
    "seed_bullets = \"\\n\".join(f\"- Seed {i+1}: **{s}**\" for i, s in enumerate(SEED_LIST))\n",
    "\n",
    "setup_md = f\"\"\"## 3. Experimental Setup\n",
    "\n",
    "---\n",
    "\n",
    "This section describes the training and evaluation methodology used to compare DQN and PPO in the LunarLander-v3 environment. All experiments were conducted under controlled and reproducible conditions to ensure a fair comparison between the two algorithms.\n",
    "\n",
    "### 3.1 Total Environment Steps\n",
    "\n",
    "Both DQN and PPO were trained using a total of **{TOTAL_TIMESTEPS:,} environment steps per seed**.\n",
    "Since the project uses {len(SEED_LIST)} seeds, this results in:\n",
    "\n",
    "- {len(SEED_LIST)} seeds x {TOTAL_TIMESTEPS:,} steps = **{len(SEED_LIST) * TOTAL_TIMESTEPS:,} training steps per algorithm**\n",
    "\n",
    "Using the same number of interactions ensures that the comparison between DQN and PPO is based on equal sample budgets.\n",
    "\n",
    "### 3.2 Hyperparameter Configurations\n",
    "\n",
    "The hyperparameters for each algorithm were selected based on Stable-Baselines3 recommendations and refined through empirical testing.\n",
    "Tables 1 and 2 summarize the final configurations used for training.\n",
    "\n",
    "| Hyperparameter          | Value            |\n",
    "|-------------------------|------------------|\n",
    "| Policy                  | {MLP_POLICY}     |\n",
    "| Learning rate           | linear\\_schedule(6.3e-4) |\n",
    "| Buffer size             | {dqn_p['buffer_size']:,} |\n",
    "| Batch size              | {dqn_p['batch_size']} |\n",
    "| Gamma (discount factor) | {dqn_p['gamma']} |\n",
    "| Learning starts         | {dqn_p['learning_starts']:,} |\n",
    "| Exploration strategy    | $\\\\epsilon$-greedy |\n",
    "| Exploration fraction    | {dqn_p['exploration_fraction']} |\n",
    "| Final $\\\\epsilon$       | {dqn_p['exploration_final_eps']} |\n",
    "| Target update interval  | {dqn_p['target_update_interval']} |\n",
    "| Train frequency         | {dqn_p['train_freq']} |\n",
    "| Gradient steps          | {dqn_p['gradient_steps']} |\n",
    "| Network architecture    | {' x '.join(str(x) for x in dqn_p['policy_kwargs']['net_arch'])} MLP |\n",
    "\n",
    "**Table 1** - DQN Hyperparameters\n",
    "\n",
    "| Hyperparameter          | Value            |\n",
    "|-------------------------|------------------|\n",
    "| Policy                  | {MLP_POLICY}     |\n",
    "| Learning rate           | {ppo_p['learning_rate']} |\n",
    "| n\\_steps                | {ppo_p['n_steps']} |\n",
    "| Batch size              | {ppo_p['batch_size']} |\n",
    "| n\\_epochs               | {ppo_p['n_epochs']} |\n",
    "| Gamma (discount factor) | {ppo_p['gamma']} |\n",
    "| GAE $\\\\lambda$          | {ppo_p['gae_lambda']} |\n",
    "| Clip range              | {ppo_p['clip_range']} |\n",
    "| Entropy coefficient     | {ppo_p['ent_coef']} |\n",
    "| Network architecture    | {' x '.join(str(x) for x in dqn_p['policy_kwargs']['net_arch'])} MLP |\n",
    "\n",
    "**Table 2** - PPO Hyperparameters\n",
    "\n",
    "These hyperparameters were kept constant across all seeds to isolate the effect of stochasticity in training.\n",
    "\n",
    "### 3.3 Random Seeds\n",
    "\n",
    "To evaluate robustness and training stability, both algorithms were trained using **{len(SEED_LIST)} different random seeds**:\n",
    "{seed_bullets}\n",
    "\n",
    "Each seed corresponds to a full independent training run of {TOTAL_TIMESTEPS:,} steps.\n",
    "\n",
    "### 3.4 Evaluation Protocol\n",
    "\n",
    "Each trained model was evaluated over **{EVALUATION_EPISODES} deterministic episodes per seed**, for a total of {EVALUATION_EPISODES * len(SEED_LIST)} evaluation episodes per algorithm. Deterministic evaluation ensures that the policy is tested without exploration noise, providing a fair assessment of the learned behaviour.\n",
    "\n",
    "### 3.5 Best-Model Selection\n",
    "\n",
    "During training, an evaluation callback assessed the model every {EVAL_FREQ_TIMESTEPS:,} environment steps using {EVAL_N_EPISODES} deterministic episodes. The best model was selected based on a combined metric: **mean reward - standard deviation**. This metric favours models that are both high-performing and consistent, rather than models that achieve high mean reward with large variance.\n",
    "\n",
    "A two-tier selection gate ensures that once any model crosses a mean reward of {SOLVED_THRESHOLD}, only solved models can replace the current best.\n",
    "\n",
    "### 3.6 Reproducibility\n",
    "\n",
    "All experiments used fixed random seeds for Python, NumPy, PyTorch, and the Gymnasium environment. Deterministic algorithms were enabled in PyTorch to minimize non-determinism from GPU operations.\n",
    "\"\"\"\n",
    "\n",
    "display(Markdown(setup_md))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"CUDA:\", torch.version.cuda if torch.cuda.is_available() else \"None\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment inspection (run once)\n",
    "env_tmp = gym.make(GYMNASIUM_MODEL)\n",
    "\n",
    "print(\"Observation space:\", env_tmp.observation_space)\n",
    "print(\"Action space:\", env_tmp.action_space)\n",
    "\n",
    "obs, info = env_tmp.reset()\n",
    "print(\"Initial observation:\", obs)\n",
    "\n",
    "env_tmp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, checkpoint_path=None, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.value_loss = []\n",
    "        self.entropy = []\n",
    "        self.mean_q_values = []      # Track Q-value overestimation\n",
    "        self.gradient_updates = 0    # Count gradient updates\n",
    "\n",
    "        self._current_reward = 0.0\n",
    "        self._current_length = 0\n",
    "        self._plot_handle = None\n",
    "        self._stats_handle = None\n",
    "        self._checkpoint_handle = None\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals.get(\"rewards\")\n",
    "        dones = self.locals.get(\"dones\")\n",
    "\n",
    "        if rewards is not None and dones is not None:\n",
    "            reward = rewards[0]\n",
    "            done = dones[0]\n",
    "\n",
    "            self._current_reward += float(reward)\n",
    "            self._current_length += 1\n",
    "\n",
    "            if done:\n",
    "                self.episode_rewards.append(self._current_reward)\n",
    "                self.episode_lengths.append(self._current_length)\n",
    "                ep = len(self.episode_rewards)\n",
    "\n",
    "                # Periodic checkpoint\n",
    "                if self.checkpoint_path and ep % CHECKPOINT_FREQ_EPISODES == 0:\n",
    "                    ckpt_path = os.path.join(self.checkpoint_path, f\"checkpoint_ep{ep}\")\n",
    "                    self.model.save(ckpt_path)\n",
    "                    ckpt_text = f\"[Checkpoint] Episode {ep} saved\"\n",
    "                    if self._checkpoint_handle is None:\n",
    "                        self._checkpoint_handle = display(ckpt_text, display_id=True)\n",
    "                    else:\n",
    "                        self._checkpoint_handle.update(ckpt_text)\n",
    "\n",
    "                if ep % CHART_UPDATE_FREQ == 0:\n",
    "                    recent = np.array(self.episode_rewards[-50:])\n",
    "                    stats_text = (\n",
    "                        f'Episode {ep} | Last {len(recent)} Ep \\u2014 '\n",
    "                        f'Mean: {np.mean(recent):.1f} | Std: {np.std(recent):.1f} | '\n",
    "                        f'Min: {np.min(recent):.1f} | Max: {np.max(recent):.1f} | '\n",
    "                        f'Success: {(recent >= 200).sum() / len(recent) * 100:.0f}%'\n",
    "                    )\n",
    "                    if self._stats_handle is None:\n",
    "                        self._stats_handle = display(stats_text, display_id=True)\n",
    "                    else:\n",
    "                        self._stats_handle.update(stats_text)\n",
    "\n",
    "                if ep % CHART_UPDATE_FREQ == 0:\n",
    "                    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "                    ax1.plot(self.episode_rewards, alpha=0.3, color='gray')\n",
    "                    window = min(50, len(self.episode_rewards))\n",
    "                    rolling = pd.Series(self.episode_rewards).rolling(window).mean()\n",
    "                    ax1.plot(rolling, color='blue', linewidth=2)\n",
    "                    ax1.axhline(y=200, color='red', linestyle='--')\n",
    "                    ax1.set_title(f'Episode Reward \\u2014 Ep {ep}')\n",
    "                    ax1.set_xlabel('Episode')\n",
    "                    ax1.set_ylabel('Reward')\n",
    "                    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "                    if self.value_loss:\n",
    "                        ax2.plot(self.value_loss, color='green', alpha=0.7)\n",
    "                        ax2.set_title('Value Loss')\n",
    "                        ax2.set_xlabel('Rollout')\n",
    "                        ax2.set_ylabel('Loss')\n",
    "                        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "                    plt.tight_layout()\n",
    "\n",
    "                    if self._plot_handle is None:\n",
    "                        self._plot_handle = display(fig, display_id=True)\n",
    "                    else:\n",
    "                        self._plot_handle.update(fig)\n",
    "                    plt.close(fig)\n",
    "\n",
    "                self._current_reward = 0.0\n",
    "                self._current_length = 0\n",
    "\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        logger_data = self.model.logger.name_to_value\n",
    "        if \"train/loss\" in logger_data:\n",
    "            self.value_loss.append(logger_data[\"train/loss\"])\n",
    "        if \"rollout/exploration_rate\" in logger_data:\n",
    "            self.entropy.append(logger_data[\"rollout/exploration_rate\"])\n",
    "        if \"train/n_updates\" in logger_data:\n",
    "            self.gradient_updates = int(logger_data[\"train/n_updates\"])\n",
    "\n",
    "        # Sample Q-values from current observation to track overestimation\n",
    "        try:\n",
    "            obs = self.locals.get(\"new_obs\")\n",
    "            if obs is not None:\n",
    "                obs_tensor = torch.as_tensor(obs, device=self.model.device).float()\n",
    "                dqn_model: DQN = self.model  # type: ignore[assignment]\n",
    "                with torch.no_grad():\n",
    "                    q_values = dqn_model.q_net(obs_tensor)\n",
    "                self.mean_q_values.append(float(q_values.max(dim=1).values.mean()))\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "\n",
    "class PPOLoggingCallback(BaseCallback):\n",
    "    def __init__(self, checkpoint_path=None, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.policy_loss = []\n",
    "        self.value_loss = []\n",
    "        self.entropy = []\n",
    "        self.clip_fraction = []       # PPO stability: fraction of clipped updates\n",
    "        self.approx_kl = []           # PPO stability: KL divergence\n",
    "        self.explained_variance = []  # PPO stability: value function quality\n",
    "        self.gradient_updates = 0     # Count gradient updates\n",
    "\n",
    "        self._current_rewards: np.ndarray = np.array([])\n",
    "        self._current_lengths: np.ndarray = np.array([])\n",
    "        self._plot_handle = None\n",
    "        self._stats_handle = None\n",
    "        self._checkpoint_handle = None\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        n_envs = self.training_env.num_envs\n",
    "        self._current_rewards = np.zeros(n_envs, dtype=np.float32)\n",
    "        self._current_lengths = np.zeros(n_envs, dtype=np.int32)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        rewards = self.locals.get(\"rewards\")\n",
    "        dones = self.locals.get(\"dones\")\n",
    "\n",
    "        if rewards is not None and dones is not None:\n",
    "            self._current_rewards += rewards\n",
    "            self._current_lengths += 1\n",
    "\n",
    "            for i, done in enumerate(dones):\n",
    "                if done:\n",
    "                    self.episode_rewards.append(float(self._current_rewards[i]))\n",
    "                    self.episode_lengths.append(int(self._current_lengths[i]))\n",
    "                    ep = len(self.episode_rewards)\n",
    "\n",
    "                    # Periodic checkpoint\n",
    "                    if self.checkpoint_path and ep % CHECKPOINT_FREQ_EPISODES == 0:\n",
    "                        ckpt_path = os.path.join(self.checkpoint_path, f\"checkpoint_ep{ep}\")\n",
    "                        self.model.save(ckpt_path)\n",
    "                        ckpt_text = f\"[Checkpoint] Episode {ep} saved\"\n",
    "                        if self._checkpoint_handle is None:\n",
    "                            self._checkpoint_handle = display(ckpt_text, display_id=True)\n",
    "                        else:\n",
    "                            self._checkpoint_handle.update(ckpt_text)\n",
    "\n",
    "                    if ep % CHART_UPDATE_FREQ == 0:\n",
    "                        recent = np.array(self.episode_rewards[-50:])\n",
    "                        stats_text = (\n",
    "                            f'Episode {ep} | Last {len(recent)} Ep \\u2014 '\n",
    "                            f'Mean: {np.mean(recent):.1f} | Std: {np.std(recent):.1f} | '\n",
    "                            f'Min: {np.min(recent):.1f} | Max: {np.max(recent):.1f} | '\n",
    "                            f'Success: {(recent >= 200).sum() / len(recent) * 100:.0f}%'\n",
    "                        )\n",
    "                        if self._stats_handle is None:\n",
    "                            self._stats_handle = display(stats_text, display_id=True)\n",
    "                        else:\n",
    "                            self._stats_handle.update(stats_text)\n",
    "\n",
    "                    if ep % CHART_UPDATE_FREQ == 0:\n",
    "                        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "                        ax1.plot(self.episode_rewards, alpha=0.3, color='gray')\n",
    "                        window = min(50, len(self.episode_rewards))\n",
    "                        rolling = pd.Series(self.episode_rewards).rolling(window).mean()\n",
    "                        ax1.plot(rolling, color='blue', linewidth=2)\n",
    "                        ax1.axhline(y=200, color='red', linestyle='--')\n",
    "                        ax1.set_title(f'Episode Reward \\u2014 Ep {ep}')\n",
    "                        ax1.set_xlabel('Episode')\n",
    "                        ax1.set_ylabel('Reward')\n",
    "                        ax1.grid(True, alpha=0.3)\n",
    "\n",
    "                        if self.value_loss:\n",
    "                            ax2.plot(self.value_loss, color='green', alpha=0.7)\n",
    "                            ax2.set_title('Value Loss')\n",
    "                            ax2.set_xlabel('Rollout')\n",
    "                            ax2.set_ylabel('Loss')\n",
    "                            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "                        plt.tight_layout()\n",
    "\n",
    "                        if self._plot_handle is None:\n",
    "                            self._plot_handle = display(fig, display_id=True)\n",
    "                        else:\n",
    "                            self._plot_handle.update(fig)\n",
    "                        plt.close(fig)\n",
    "\n",
    "                    self._current_rewards[i] = 0\n",
    "                    self._current_lengths[i] = 0\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        logger_data = self.model.logger.name_to_value\n",
    "        if \"train/policy_gradient_loss\" in logger_data:\n",
    "            self.policy_loss.append(logger_data[\"train/policy_gradient_loss\"])\n",
    "        if \"train/value_loss\" in logger_data:\n",
    "            self.value_loss.append(logger_data[\"train/value_loss\"])\n",
    "        if \"train/entropy_loss\" in logger_data:\n",
    "            self.entropy.append(-logger_data[\"train/entropy_loss\"])\n",
    "        if \"train/clip_fraction\" in logger_data:\n",
    "            self.clip_fraction.append(logger_data[\"train/clip_fraction\"])\n",
    "        if \"train/approx_kl\" in logger_data:\n",
    "            self.approx_kl.append(logger_data[\"train/approx_kl\"])\n",
    "        if \"train/explained_variance\" in logger_data:\n",
    "            self.explained_variance.append(logger_data[\"train/explained_variance\"])\n",
    "        if \"train/n_updates\" in logger_data:\n",
    "            self.gradient_updates = int(logger_data[\"train/n_updates\"])\n",
    "\n",
    "\n",
    "class CombinedMetricEvalCallback(EvalCallback):\n",
    "    \"\"\"\n",
    "    Custom EvalCallback that selects the best model using a combined metric:\n",
    "        score = mean_reward - std_reward\n",
    "    This favors models that are both high-performing and consistent.\n",
    "\n",
    "    Two-tier selection with solved gate:\n",
    "    - Once any evaluation has mean_reward >= SOLVED_THRESHOLD, only solved\n",
    "      models can replace the current best (unsolved fallbacks are discarded).\n",
    "    - Before any model solves, the overall best score is tracked as fallback.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.best_combined_score = -np.inf\n",
    "        self.best_std_reward = np.inf\n",
    "        self.best_success_rate = 0.0\n",
    "        self.best_timestep = 0\n",
    "        self._any_solved = False\n",
    "        self._eval_handle = None\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        continue_training = True\n",
    "\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            episode_rewards, episode_lengths = evaluate_policy(\n",
    "                self.model,\n",
    "                self.eval_env,\n",
    "                n_eval_episodes=self.n_eval_episodes,\n",
    "                render=self.render,\n",
    "                deterministic=self.deterministic,\n",
    "                return_episode_rewards=True,\n",
    "            )\n",
    "\n",
    "            mean_reward, std_reward = np.mean(episode_rewards), np.std(episode_rewards)\n",
    "            mean_ep_length = np.mean(episode_lengths)\n",
    "            success_rate = np.sum(np.array(episode_rewards) >= 200) / len(episode_rewards) * 100   # type: ignore\n",
    "            self.last_mean_reward = mean_reward\n",
    "\n",
    "            combined_score = mean_reward - std_reward\n",
    "            is_solved = mean_reward >= SOLVED_THRESHOLD\n",
    "\n",
    "            # Two-tier best model selection:\n",
    "            # 1. If this model is solved, save if it's the first solved or has better score\n",
    "            # 2. If no model has solved yet, save the overall best as fallback\n",
    "            save_new_best = False\n",
    "            if is_solved:\n",
    "                if not self._any_solved:\n",
    "                    # First solved model — always save (replaces any unsolved fallback)\n",
    "                    save_new_best = True\n",
    "                    self._any_solved = True\n",
    "                elif combined_score > self.best_combined_score:\n",
    "                    # Better solved model\n",
    "                    save_new_best = True\n",
    "            elif not self._any_solved:\n",
    "                # No solved model yet — track overall best as fallback\n",
    "                if combined_score > self.best_combined_score:\n",
    "                    save_new_best = True\n",
    "\n",
    "            solved_tag = \" [SOLVED]\" if is_solved else \"\"\n",
    "            eval_text = (\n",
    "                f\"Eval @ {self.num_timesteps} steps | \"\n",
    "                f\"Reward: {mean_reward:.2f} +/- {std_reward:.2f}{solved_tag} | \"\n",
    "                f\"Success: {success_rate:.0f}% | \"\n",
    "                f\"Score (mean-std): {combined_score:.2f} | \"\n",
    "                f\"Best: {self.best_combined_score:.2f}\"\n",
    "            )\n",
    "\n",
    "            if save_new_best:\n",
    "                eval_text += f\" >> New best model!\"\n",
    "                self.best_combined_score = combined_score\n",
    "                self.best_mean_reward = mean_reward\n",
    "                self.best_std_reward = std_reward\n",
    "                self.best_success_rate = success_rate\n",
    "                self.best_timestep = self.num_timesteps\n",
    "                if self.best_model_save_path is not None:\n",
    "                    self.model.save(\n",
    "                        os.path.join(self.best_model_save_path, \"best_model\")\n",
    "                    )\n",
    "\n",
    "            if self._eval_handle is None:\n",
    "                self._eval_handle = display(eval_text, display_id=True)\n",
    "            else:\n",
    "                self._eval_handle.update(eval_text)\n",
    "\n",
    "            if self.log_path is not None:\n",
    "                self.evaluations_timesteps.append(self.num_timesteps)\n",
    "                self.evaluations_results.append(episode_rewards)   # type: ignore\n",
    "                self.evaluations_length.append(episode_lengths)    # type: ignore\n",
    "                np.savez(\n",
    "                    self.log_path,\n",
    "                    timesteps=self.evaluations_timesteps,\n",
    "                    results=self.evaluations_results,\n",
    "                    ep_lengths=self.evaluations_length,\n",
    "                )\n",
    "\n",
    "            self.logger.record(\"eval/mean_reward\", float(mean_reward))\n",
    "            self.logger.record(\"eval/std_reward\", float(std_reward))\n",
    "            self.logger.record(\"eval/mean_ep_length\", float(mean_ep_length))\n",
    "            self.logger.record(\"eval/combined_score\", float(combined_score))\n",
    "            self.logger.record(\"eval/success_rate\", float(success_rate))\n",
    "\n",
    "        return continue_training\n",
    "\n",
    "\n",
    "CALLBACK_MAP = {\n",
    "    \"dqn\": DQNLoggingCallback,\n",
    "    \"ppo\": PPOLoggingCallback,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop: algorithms x seeds\n",
    "\n",
    "training_results = {}  # {algo: {seed: callback}}\n",
    "training_times = {}    # {algo: {seed: seconds}}\n",
    "model_save_paths = {}  # {algo: {seed: {\"run_dir\", \"final\", \"best\"}}}\n",
    "eval_callbacks = {}    # {algo: {seed: eval_cb}} — for best-model summary table\n",
    "\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "\n",
    "    tb_dir = os.path.join(NOTEBOOK_DIR, \"outputs_\" + algo_name, \"tensorboard\")\n",
    "\n",
    "    training_results[algo_name] = {}\n",
    "    training_times[algo_name] = {}\n",
    "    model_save_paths[algo_name] = {}\n",
    "    eval_callbacks[algo_name] = {}\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{algo_name.upper()} | Seed {seed}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        # Create timestamped run directory\n",
    "        run_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "        run_dir = os.path.join(NOTEBOOK_DIR, \"../models\", algo_name, run_timestamp)\n",
    "        checkpoints_dir = os.path.join(run_dir, \"checkpoints\")\n",
    "        os.makedirs(checkpoints_dir, exist_ok=True)\n",
    "\n",
    "        set_all_seeds(seed)\n",
    "\n",
    "        def make_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        env = DummyVecEnv([make_env])\n",
    "        env.seed(seed)\n",
    "\n",
    "        # Separate eval env for EvalCallback\n",
    "        def make_eval_env(s=seed):\n",
    "            e = Monitor(gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED))\n",
    "            e.reset(seed=s)\n",
    "            return e\n",
    "\n",
    "        eval_env = DummyVecEnv([make_eval_env])\n",
    "\n",
    "        params = {\n",
    "            \"policy\": MLP_POLICY,\n",
    "            \"env\": env,\n",
    "            \"device\": DEVICE,\n",
    "            \"seed\": seed,\n",
    "            \"tensorboard_log\": tb_dir,\n",
    "            **ALGO_PARAMS[algo_name],\n",
    "        }\n",
    "\n",
    "        model = algo_class(**params)\n",
    "\n",
    "        # Setup callbacks\n",
    "        logging_cb = CALLBACK_MAP[algo_name](checkpoint_path=checkpoints_dir)\n",
    "        eval_cb = CombinedMetricEvalCallback(\n",
    "            eval_env=eval_env,\n",
    "            eval_freq=EVAL_FREQ_TIMESTEPS,\n",
    "            n_eval_episodes=EVAL_N_EPISODES,\n",
    "            best_model_save_path=run_dir,\n",
    "            log_path=os.path.join(run_dir, \"eval_log\"),\n",
    "            deterministic=True,\n",
    "            verbose=1,\n",
    "        )\n",
    "\n",
    "        t_start = time.time()\n",
    "        model.learn(\n",
    "            total_timesteps=TOTAL_TIMESTEPS,\n",
    "            callback=CallbackList([logging_cb, eval_cb]),\n",
    "            progress_bar=True,\n",
    "        )\n",
    "        t_elapsed = time.time() - t_start\n",
    "\n",
    "        training_times[algo_name][seed] = t_elapsed\n",
    "        print(f\"\\nTraining time: {t_elapsed/60:.1f} min ({t_elapsed:.0f} s)\")\n",
    "\n",
    "        # Save final model\n",
    "        final_path = os.path.join(run_dir, f\"{SESSION_PREFIX}_{algo_name}_{seed}\")\n",
    "        model.save(final_path)\n",
    "        print(f\"Final model:  {final_path}\")\n",
    "        print(f\"Best model:   {os.path.join(run_dir, 'best_model')}\")\n",
    "        print(f\"Checkpoints:  {checkpoints_dir}\")\n",
    "        print(\n",
    "            f\"Best model stats: \"\n",
    "            f\"Reward: {eval_cb.best_mean_reward:.2f} +/- {eval_cb.best_std_reward:.2f} | \"\n",
    "            f\"Success: {eval_cb.best_success_rate:.0f}% | \"\n",
    "            f\"Score (mean-std): {eval_cb.best_combined_score:.2f} | \"\n",
    "            f\"@ {eval_cb.best_timestep:,} steps\"\n",
    "        )\n",
    "\n",
    "        model_save_paths[algo_name][seed] = {\n",
    "            \"run_dir\": run_dir,\n",
    "            \"final\": final_path,\n",
    "            \"best\": os.path.join(run_dir, \"best_model\"),\n",
    "        }\n",
    "\n",
    "        training_results[algo_name][seed] = logging_cb\n",
    "        eval_callbacks[algo_name][seed] = eval_cb\n",
    "\n",
    "        env.close()\n",
    "        eval_env.close()\n",
    "\n",
    "    print(f\"\\n{algo_name.upper()}: All {len(SEED_LIST)} seeds trained.\")\n",
    "\n",
    "# Best Model Summary Table\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BEST MODEL SUMMARY (all algorithms x seeds)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "best_rows = []\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    for seed in SEED_LIST:\n",
    "        cb = eval_callbacks[algo_name][seed]\n",
    "        best_rows.append({\n",
    "            \"Algorithm\": algo_name.upper(),\n",
    "            \"Seed\": seed,\n",
    "            \"Mean Reward\": f\"{cb.best_mean_reward:.2f}\",\n",
    "            \"Std Reward\": f\"{cb.best_std_reward:.2f}\",\n",
    "            \"Success\": f\"{cb.best_success_rate:.0f}%\",\n",
    "            \"Score (mean-std)\": f\"{cb.best_combined_score:.2f}\",\n",
    "            \"@ Timestep\": f\"{cb.best_timestep:,}\",\n",
    "        })\n",
    "\n",
    "print(pd.DataFrame(best_rows).to_string(index=False))\n",
    "print(f\"\\nAll training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist training episode data for post-hoc analysis\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    for seed in SEED_LIST:\n",
    "        cb = training_results[algo_name][seed]\n",
    "        save_path = os.path.join(model_save_paths[algo_name][seed][\"run_dir\"], \"training_log.npz\")\n",
    "        np.savez(\n",
    "            save_path,\n",
    "            episode_rewards=np.array(cb.episode_rewards),\n",
    "            episode_lengths=np.array(cb.episode_lengths),\n",
    "        )\n",
    "        print(f\"Saved {algo_name.upper()} seed {seed}: {len(cb.episode_rewards)} episodes -> {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Time Summary\n",
    "\n",
    "rows = []\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    for seed in SEED_LIST:\n",
    "        t = training_times[algo_name][seed]\n",
    "        rows.append({\n",
    "            \"Algorithm\": algo_name.upper(),\n",
    "            \"Seed\": seed,\n",
    "            \"Time (s)\": f\"{t:.0f}\",\n",
    "            \"Time (min)\": f\"{t/60:.1f}\",\n",
    "        })\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    times = list(training_times[algo_name].values())\n",
    "    rows.append({\n",
    "        \"Algorithm\": algo_name.upper(),\n",
    "        \"Seed\": \"Mean\",\n",
    "        \"Time (s)\": f\"{np.mean(times):.0f}\",\n",
    "        \"Time (min)\": f\"{np.mean(times)/60:.1f}\",\n",
    "    })\n",
    "\n",
    "print(\"*** TRAINING TIME SUMMARY ***\")\n",
    "print(f\"Timesteps per seed: {TOTAL_TIMESTEPS:,} | Device: {DEVICE}\")\n",
    "print()\n",
    "print(pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Updates Summary\n",
    "# DQN: train_freq=4, gradient_steps=4 -> 4 gradient steps every 4 env steps\n",
    "# PPO: n_steps=2048, n_epochs=10, batch_size=64 -> 10 * (2048/64) = 320 updates per rollout\n",
    "\n",
    "rows = []\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    # Use the actual count from the last seed's callback\n",
    "    last_seed = SEED_LIST[-1]\n",
    "    actual_updates = training_results[algo_name][last_seed].gradient_updates\n",
    "\n",
    "    # Also compute analytically for verification\n",
    "    if algo_name == \"dqn\":\n",
    "        p = ALGO_PARAMS[\"dqn\"]\n",
    "        train_freq = p.get(\"train_freq\", 4)\n",
    "        grad_steps = p.get(\"gradient_steps\", -1)\n",
    "        learning_starts = p.get(\"learning_starts\", 0)\n",
    "        effective_steps = TOTAL_TIMESTEPS - learning_starts\n",
    "        steps_per_call = train_freq if grad_steps == -1 else grad_steps\n",
    "        analytical = (effective_steps // train_freq) * steps_per_call\n",
    "    else:\n",
    "        p = ALGO_PARAMS[\"ppo\"]\n",
    "        n_steps = p.get(\"n_steps\", 2048)\n",
    "        n_epochs = p.get(\"n_epochs\", 10)\n",
    "        batch_size = p.get(\"batch_size\", 64)\n",
    "        n_rollouts = TOTAL_TIMESTEPS // n_steps\n",
    "        minibatches_per_epoch = n_steps // batch_size\n",
    "        analytical = n_rollouts * n_epochs * minibatches_per_epoch\n",
    "\n",
    "    rows.append({\n",
    "        \"Algorithm\": algo_name.upper(),\n",
    "        \"Actual (from training)\": f\"{actual_updates:,}\",\n",
    "        \"Analytical (computed)\": f\"{analytical:,}\",\n",
    "        \"Total Env Steps\": f\"{TOTAL_TIMESTEPS:,}\",\n",
    "        \"Ratio (updates/steps)\": f\"{actual_updates / TOTAL_TIMESTEPS:.3f}\",\n",
    "    })\n",
    "\n",
    "print(\"*** GRADIENT UPDATES SUMMARY ***\")\n",
    "print(pd.DataFrame(rows).to_string(index=False))\n",
    "print()\n",
    "print(\"DQN: train_freq=4, gradient_steps=4 -> 4 gradient updates per training call, called every 4 env steps\")\n",
    "print(f\"PPO: n_steps={ALGO_PARAMS['ppo']['n_steps']}, n_epochs={ALGO_PARAMS['ppo']['n_epochs']}, \"\n",
    "      f\"batch_size={ALGO_PARAMS['ppo']['batch_size']} -> \"\n",
    "      f\"{ALGO_PARAMS['ppo']['n_epochs'] * (ALGO_PARAMS['ppo']['n_steps'] // ALGO_PARAMS['ppo']['batch_size'])} \"\n",
    "      f\"updates per rollout\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm, Per-Seed: Episode Reward over Training\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "    if len(SEED_LIST) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, seed in zip(axes, SEED_LIST):\n",
    "        ax.plot(training_results[algo_name][seed].episode_rewards, alpha=0.7)\n",
    "        ax.set_title(f\"Seed {seed}\")\n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"Total Reward\")\n",
    "    fig.suptitle(f\"{algo_name.upper()} \\u2014 Episode Reward over Training\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm, Per-Seed: Episode Length over Training\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "    if len(SEED_LIST) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, seed in zip(axes, SEED_LIST):\n",
    "        ax.plot(training_results[algo_name][seed].episode_lengths, alpha=0.7, color=\"orange\")\n",
    "        ax.set_title(f\"Seed {seed}\")\n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"Number of Steps\")\n",
    "    fig.suptitle(f\"{algo_name.upper()} \\u2014 Episode Length over Training\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm, Per-Seed: Value Loss over Rollouts\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "    if len(SEED_LIST) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, seed in zip(axes, SEED_LIST):\n",
    "        ax.plot(training_results[algo_name][seed].value_loss, alpha=0.7, color=\"green\")\n",
    "        ax.set_title(f\"Seed {seed}\")\n",
    "        ax.set_xlabel(\"Rollout\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"Loss Value\")\n",
    "    fig.suptitle(f\"{algo_name.upper()} \\u2014 Value Loss over Rollouts\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm, Per-Seed: Entropy / Exploration Rate over Rollouts\n",
    "\n",
    "entropy_labels = {\"dqn\": (\"Epsilon\", \"Exploration Rate\"), \"ppo\": (\"Entropy (Positive)\", \"Entropy\")}\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    ylabel, title_suffix = entropy_labels[algo_name]\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "    if len(SEED_LIST) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, seed in zip(axes, SEED_LIST):\n",
    "        ax.plot(training_results[algo_name][seed].entropy, alpha=0.7, color=\"purple\")\n",
    "        ax.set_title(f\"Seed {seed}\")\n",
    "        ax.set_xlabel(\"Rollout\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(ylabel)\n",
    "    fig.suptitle(f\"{algo_name.upper()} \\u2014 {title_suffix} over Rollouts\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Only: Policy Loss over Rollouts\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    ax.plot(training_results[\"ppo\"][seed].policy_loss, alpha=0.7, color=\"red\")\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.set_xlabel(\"Rollout\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Policy Loss\")\n",
    "fig.suptitle(\"PPO \\u2014 Policy Gradient Loss over Rollouts\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Overestimation: Mean Max Q-Value over Training\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    q_vals = training_results[\"dqn\"][seed].mean_q_values\n",
    "    if q_vals:\n",
    "        ax.plot(q_vals, alpha=0.7, color=\"darkorange\")\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.set_xlabel(\"Rollout\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Mean Max Q-Value\")\n",
    "fig.suptitle(\"DQN \\u2014 Q-Value Overestimation Tracking\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Note: Steadily rising Q-values that diverge from actual returns indicate overestimation.\")\n",
    "print(\"A stable or slowly growing curve suggests the target network is controlling overestimation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Update Stability: Clip Fraction, Approx KL, Explained Variance\n",
    "\n",
    "fig, axes = plt.subplots(3, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 12), sharex=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = axes.reshape(3, 1)\n",
    "\n",
    "metrics = [\n",
    "    (\"clip_fraction\", \"Clip Fraction\", \"tab:red\",\n",
    "     \"Fraction of policy updates clipped by PPO. High values suggest the policy is changing too fast.\"),\n",
    "    (\"approx_kl\", \"Approx KL Divergence\", \"tab:purple\",\n",
    "     \"KL divergence between old and new policy. Spikes indicate large policy shifts.\"),\n",
    "    (\"explained_variance\", \"Explained Variance\", \"tab:cyan\",\n",
    "     \"How well the value function predicts returns. 1.0 = perfect, 0 = no better than mean.\"),\n",
    "]\n",
    "\n",
    "for row, (attr, ylabel, color, _) in enumerate(metrics):\n",
    "    for col, seed in enumerate(SEED_LIST):\n",
    "        data = getattr(training_results[\"ppo\"][seed], attr)\n",
    "        if data:\n",
    "            axes[row][col].plot(data, alpha=0.7, color=color)\n",
    "        if row == 0:\n",
    "            axes[row][col].set_title(f\"Seed {seed}\")\n",
    "        if row == 2:\n",
    "            axes[row][col].set_xlabel(\"Rollout\")\n",
    "        axes[row][col].grid(True, alpha=0.3)\n",
    "    axes[row][0].set_ylabel(ylabel)\n",
    "\n",
    "fig.suptitle(\"PPO \\u2014 Update Stability Metrics over Rollouts\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for _, ylabel, _, description in metrics:\n",
    "    print(f\"  {ylabel}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated: Rolling Reward Overlay — per algorithm (all seeds on one chart)\n",
    "\n",
    "seed_colors = list(plt.colormaps[\"tab10\"](range(10)))  # type: ignore[arg-type]\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for i, seed in enumerate(SEED_LIST):\n",
    "        rewards = training_results[algo_name][seed].episode_rewards\n",
    "        rolling = pd.Series(rewards).rolling(50).mean()\n",
    "        plt.plot(rolling, color=seed_colors[i], linewidth=2, label=f\"Seed {seed}\")\n",
    "\n",
    "    plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "    plt.title(f\"{algo_name.upper()} Training: Rolling Mean Reward (window=50)\", fontsize=14)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm: Rolling Success Rate over Training (window=50)\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    for i, seed in enumerate(SEED_LIST):\n",
    "        rewards = np.array(training_results[algo_name][seed].episode_rewards)\n",
    "        success = (rewards >= 200).astype(float)\n",
    "        rolling_success = pd.Series(success).rolling(50).mean() * 100\n",
    "        plt.plot(rolling_success, color=seed_colors[i], linewidth=2, label=f\"Seed {seed}\")\n",
    "\n",
    "    plt.axhline(y=100, color='green', linestyle=':', alpha=0.4, label='100%')\n",
    "    plt.axhline(y=50, color='gray', linestyle=':', alpha=0.4, label='50%')\n",
    "    plt.title(f\"{algo_name.upper()} Training: Rolling Success Rate (window=50)\", fontsize=14)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Success Rate (%)\")\n",
    "    plt.ylim(-5, 105)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm Comparison: Success Rate over Training (averaged across seeds)\n",
    "\n",
    "algo_colors = {\"dqn\": \"tab:blue\", \"ppo\": \"tab:orange\"}\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    min_len = min(len(training_results[algo_name][s].episode_rewards) for s in SEED_LIST)\n",
    "    all_rewards = np.array([training_results[algo_name][s].episode_rewards[:min_len] for s in SEED_LIST])\n",
    "    all_success = (all_rewards >= 200).astype(float)\n",
    "\n",
    "    mean_success = pd.Series(all_success.mean(axis=0)).rolling(50).mean() * 100\n",
    "    std_success = pd.Series(all_success.std(axis=0)).rolling(50).mean() * 100\n",
    "\n",
    "    episodes = np.arange(len(mean_success))\n",
    "    plt.plot(episodes, mean_success, color=algo_colors[algo_name], linewidth=2,\n",
    "             label=f\"{algo_name.upper()} (mean)\")\n",
    "    plt.fill_between(episodes, mean_success - std_success, mean_success + std_success,\n",
    "                     color=algo_colors[algo_name], alpha=0.15)\n",
    "\n",
    "plt.axhline(y=100, color='green', linestyle=':', alpha=0.4, label='100%')\n",
    "plt.axhline(y=50, color='gray', linestyle=':', alpha=0.4, label='50%')\n",
    "plt.title(\"DQN vs PPO: Mean Success Rate across Seeds (window=50)\", fontsize=14)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Success Rate (%)\")\n",
    "plt.ylim(-5, 105)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm Comparison: Rolling Reward (averaged across seeds)\n",
    "\n",
    "algo_colors = {\"dqn\": \"tab:blue\", \"ppo\": \"tab:orange\"}\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    # Find the shortest episode count across seeds for alignment\n",
    "    min_len = min(len(training_results[algo_name][s].episode_rewards) for s in SEED_LIST)\n",
    "    all_rewards = np.array([training_results[algo_name][s].episode_rewards[:min_len] for s in SEED_LIST])\n",
    "    mean_rewards = pd.Series(all_rewards.mean(axis=0)).rolling(50).mean()\n",
    "    std_rewards = pd.Series(all_rewards.std(axis=0)).rolling(50).mean()\n",
    "\n",
    "    episodes = np.arange(len(mean_rewards))\n",
    "    plt.plot(episodes, mean_rewards, color=algo_colors[algo_name], linewidth=2, label=f\"{algo_name.upper()} (mean)\")\n",
    "    plt.fill_between(episodes, mean_rewards - std_rewards, mean_rewards + std_rewards,\n",
    "                     color=algo_colors[algo_name], alpha=0.15)\n",
    "\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.title(\"DQN vs PPO: Mean Rolling Reward across Seeds (window=50)\", fontsize=14)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation & Reporting (loaded from disk)\n",
    "\n",
    "The following sections reload the best models from disk and run evaluation independently of the training session.\n",
    "This proves that the saved artifacts are complete and that reporting is fully reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"\"\"The following tables summarize the evaluation performance of each algorithm across all seeds. Each model was evaluated over **{EVALUATION_EPISODES} deterministic episodes**. The overall row aggregates all episodes across seeds.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discovery functions — scan models/ folders for saved artifacts\n",
    "\n",
    "TRAJECTORY_EPISODES = 3\n",
    "\n",
    "# LunarLander-v3 action labels\n",
    "ACTION_LABELS = [\"Do Nothing\", \"Fire Left\", \"Fire Main\", \"Fire Right\"]\n",
    "\n",
    "algo_colors = {\"dqn\": \"tab:blue\", \"ppo\": \"tab:orange\"}\n",
    "algo_names = list(ALGORITHM_MAP.keys())\n",
    "seed_colors = list(plt.colormaps[\"tab10\"](range(10)))\n",
    "\n",
    "\n",
    "def discover_best_models(session_prefix):\n",
    "    \"\"\"\n",
    "    Scan models/{algo}/{timestamp}/ folders and return a dict:\n",
    "        {algo: {seed: path_to_best_model}}\n",
    "    Only considers runs whose final model filename starts with session_prefix.\n",
    "    \"\"\"\n",
    "    models_root = os.path.join(NOTEBOOK_DIR, \"../models\")\n",
    "    best_models = {}\n",
    "\n",
    "    for algo_name in ALGORITHM_MAP:\n",
    "        best_models[algo_name] = {}\n",
    "        algo_dir = os.path.join(models_root, algo_name)\n",
    "        if not os.path.isdir(algo_dir):\n",
    "            continue\n",
    "\n",
    "        for run_folder in sorted(glob.glob(os.path.join(algo_dir, \"????-??-??_??_??_??\"))):\n",
    "            best_model_path = os.path.join(run_folder, \"best_model.zip\")\n",
    "            if not os.path.isfile(best_model_path):\n",
    "                continue\n",
    "\n",
    "            for f in os.listdir(run_folder):\n",
    "                if f.startswith(session_prefix) and f.endswith(\".zip\") and f != \"best_model.zip\":\n",
    "                    seed_str = f.replace(\".zip\", \"\").split(\"_\")[-1]\n",
    "                    if seed_str.isdigit():\n",
    "                        seed_int = int(seed_str)\n",
    "                        if seed_int in SEED_LIST:\n",
    "                            best_models[algo_name][seed_int] = best_model_path\n",
    "                    break\n",
    "\n",
    "    return best_models\n",
    "\n",
    "\n",
    "def discover_final_models(session_prefix):\n",
    "    \"\"\"\n",
    "    Scan models/{algo}/{timestamp}/ folders and return a dict:\n",
    "        {algo: {seed: path_to_final_model}}\n",
    "    \"\"\"\n",
    "    models_root = os.path.join(NOTEBOOK_DIR, \"../models\")\n",
    "    final_models = {}\n",
    "\n",
    "    for algo_name in ALGORITHM_MAP:\n",
    "        final_models[algo_name] = {}\n",
    "        algo_dir = os.path.join(models_root, algo_name)\n",
    "        if not os.path.isdir(algo_dir):\n",
    "            continue\n",
    "\n",
    "        for run_folder in sorted(glob.glob(os.path.join(algo_dir, \"????-??-??_??_??_??\"))):\n",
    "            for f in os.listdir(run_folder):\n",
    "                if f.startswith(session_prefix) and f.endswith(\".zip\") and f != \"best_model.zip\":\n",
    "                    seed_str = f.replace(\".zip\", \"\").split(\"_\")[-1]\n",
    "                    if seed_str.isdigit():\n",
    "                        seed_int = int(seed_str)\n",
    "                        if seed_int in SEED_LIST:\n",
    "                            final_models[algo_name][seed_int] = os.path.join(run_folder, f)\n",
    "                    break\n",
    "\n",
    "    return final_models\n",
    "\n",
    "\n",
    "def discover_eval_logs(session_prefix):\n",
    "    \"\"\"\n",
    "    Scan models/{algo}/{timestamp}/eval_log/evaluations.npz and return a dict:\n",
    "        {algo: {seed: path_to_evaluations_npz}}\n",
    "    \"\"\"\n",
    "    models_root = os.path.join(NOTEBOOK_DIR, \"../models\")\n",
    "    eval_logs = {}\n",
    "\n",
    "    for algo_name in ALGORITHM_MAP:\n",
    "        eval_logs[algo_name] = {}\n",
    "        algo_dir = os.path.join(models_root, algo_name)\n",
    "        if not os.path.isdir(algo_dir):\n",
    "            continue\n",
    "\n",
    "        for run_folder in sorted(glob.glob(os.path.join(algo_dir, \"????-??-??_??_??_??\"))):\n",
    "            eval_log_path = os.path.join(run_folder, \"eval_log\", \"evaluations.npz\")\n",
    "            if not os.path.isfile(eval_log_path):\n",
    "                continue\n",
    "\n",
    "            for f in os.listdir(run_folder):\n",
    "                if f.startswith(session_prefix) and f.endswith(\".zip\") and f != \"best_model.zip\":\n",
    "                    seed_str = f.replace(\".zip\", \"\").split(\"_\")[-1]\n",
    "                    if seed_str.isdigit():\n",
    "                        seed_int = int(seed_str)\n",
    "                        if seed_int in SEED_LIST:\n",
    "                            eval_logs[algo_name][seed_int] = eval_log_path\n",
    "                    break\n",
    "\n",
    "    return eval_logs\n",
    "\n",
    "\n",
    "def discover_training_logs(session_prefix):\n",
    "    \"\"\"\n",
    "    Scan models/{algo}/{timestamp}/training_log.npz and return a dict:\n",
    "        {algo: {seed: path_to_training_log}}\n",
    "    \"\"\"\n",
    "    models_root = os.path.join(NOTEBOOK_DIR, \"../models\")\n",
    "    logs = {}\n",
    "\n",
    "    for algo_name in ALGORITHM_MAP:\n",
    "        logs[algo_name] = {}\n",
    "        algo_dir = os.path.join(models_root, algo_name)\n",
    "        if not os.path.isdir(algo_dir):\n",
    "            continue\n",
    "\n",
    "        for run_folder in sorted(glob.glob(os.path.join(algo_dir, \"????-??-??_??_??_??\"))):\n",
    "            log_path = os.path.join(run_folder, \"training_log.npz\")\n",
    "            if not os.path.isfile(log_path):\n",
    "                continue\n",
    "\n",
    "            for f in os.listdir(run_folder):\n",
    "                if f.startswith(session_prefix) and f.endswith(\".zip\") and f != \"best_model.zip\":\n",
    "                    seed_str = f.replace(\".zip\", \"\").split(\"_\")[-1]\n",
    "                    if seed_str.isdigit():\n",
    "                        seed_int = int(seed_str)\n",
    "                        if seed_int in SEED_LIST:\n",
    "                            logs[algo_name][seed_int] = log_path\n",
    "                    break\n",
    "\n",
    "    return logs\n",
    "\n",
    "\n",
    "# Discover models and logs\n",
    "best_model_paths = discover_best_models(SESSION_PREFIX)\n",
    "final_model_paths = discover_final_models(SESSION_PREFIX)\n",
    "eval_log_paths = discover_eval_logs(SESSION_PREFIX)\n",
    "training_log_paths = discover_training_logs(SESSION_PREFIX)\n",
    "\n",
    "print(f\"Session: {SESSION_PREFIX}\")\n",
    "print(f\"Algorithms: {list(ALGORITHM_MAP.keys())}\")\n",
    "print(f\"Seeds: {SEED_LIST}\")\n",
    "print()\n",
    "print(\"Discovered best models:\")\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    for seed in SEED_LIST:\n",
    "        path = best_model_paths.get(algo_name, {}).get(seed)\n",
    "        status = path if path else \"NOT FOUND\"\n",
    "        print(f\"  {algo_name.upper()} seed {seed}: {status}\")\n",
    "print()\n",
    "print(\"Discovered eval logs:\")\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    for seed in SEED_LIST:\n",
    "        path = eval_log_paths.get(algo_name, {}).get(seed)\n",
    "        status = path if path else \"NOT FOUND\"\n",
    "        print(f\"  {algo_name.upper()} seed {seed}: {status}\")\n",
    "print()\n",
    "print(\"Discovered training logs:\")\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    for seed in SEED_LIST:\n",
    "        path = training_log_paths.get(algo_name, {}).get(seed)\n",
    "        status = path if path else \"NOT FOUND\"\n",
    "        print(f\"  {algo_name.upper()} seed {seed}: {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all best models and evaluate\n",
    "\n",
    "evaluation_results = {}  # {algo: {seed: np.array}}\n",
    "\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "    evaluation_results[algo_name] = {}\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        load_path = best_model_paths.get(algo_name, {}).get(seed)\n",
    "        if load_path is None:\n",
    "            print(f\"SKIPPING {algo_name.upper()} seed {seed} - best model not found\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Loading and evaluating {algo_name.upper()} seed {seed} (best model)...\")\n",
    "\n",
    "        def make_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        model = algo_class.load(load_path, env=DummyVecEnv([make_env]), device=DEVICE)\n",
    "\n",
    "        eval_env = Monitor(gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED))\n",
    "        eval_env.reset(seed=seed)\n",
    "\n",
    "        rewards, _ = evaluate_policy(\n",
    "            model,\n",
    "            eval_env,\n",
    "            n_eval_episodes=EVALUATION_EPISODES,\n",
    "            deterministic=True,\n",
    "            return_episode_rewards=True\n",
    "        )\n",
    "\n",
    "        evaluation_results[algo_name][seed] = np.array(rewards)\n",
    "        eval_env.close()\n",
    "\n",
    "    print(f\"{algo_name.upper()}: evaluation complete.\\n\")\n",
    "\n",
    "print(f\"All evaluations complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Per-Algorithm Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm: Evaluation Summary Tables\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    rows = []\n",
    "    for seed in SEED_LIST:\n",
    "        r = evaluation_results[algo_name][seed]\n",
    "        rows.append({\n",
    "            \"Seed\": seed,\n",
    "            \"Mean Reward\": f\"{np.mean(r):.2f}\",\n",
    "            \"Std Dev\": f\"{np.std(r):.2f}\",\n",
    "            \"Min Reward\": f\"{np.min(r):.2f}\",\n",
    "            \"Max Reward\": f\"{np.max(r):.2f}\",\n",
    "            \"Success Rate\": f\"{(r >= 200).sum() / len(r) * 100:.1f}%\"\n",
    "        })\n",
    "\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "    rows.append({\n",
    "        \"Seed\": \"Overall\",\n",
    "        \"Mean Reward\": f\"{np.mean(all_r):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(all_r):.2f}\",\n",
    "        \"Min Reward\": f\"{np.min(all_r):.2f}\",\n",
    "        \"Max Reward\": f\"{np.max(all_r):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_r >= 200).sum() / len(all_r) * 100:.1f}%\"\n",
    "    })\n",
    "\n",
    "    print(f\"*** {algo_name.upper()} MULTI-SEED EVALUATION SUMMARY ***\")\n",
    "    print(f\"Episodes per seed: {EVALUATION_EPISODES} | Total: {len(all_r)}\")\n",
    "    print(pd.DataFrame(rows).to_string(index=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"\"\"These convergence plots show the running mean reward across the {EVALUATION_EPISODES} evaluation episodes for each seed. The shaded region represents one standard deviation. A flat, high running mean indicates consistent performance.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm, Per-Seed: Evaluation Convergence Plots\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "    if len(SEED_LIST) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, seed in zip(axes, SEED_LIST):\n",
    "        rewards = evaluation_results[algo_name][seed]\n",
    "        episodes = np.arange(1, len(rewards) + 1)\n",
    "        running_mean = np.cumsum(rewards) / episodes\n",
    "        running_std = np.array([np.std(rewards[:i]) for i in episodes])\n",
    "\n",
    "        ax.scatter(episodes, rewards, color='gray', alpha=0.4, s=20, label='Episode Reward')\n",
    "        ax.plot(episodes, running_mean, color='blue', linewidth=2, label='Running Mean')\n",
    "        ax.fill_between(episodes, running_mean - running_std, running_mean + running_std,\n",
    "                        color='blue', alpha=0.15)\n",
    "        ax.axhline(y=200, color='red', linestyle='--')\n",
    "        ax.set_title(f\"Seed {seed}\")\n",
    "        ax.set_xlabel(\"Episode\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0].set_ylabel(\"Reward\")\n",
    "    fig.suptitle(f\"{algo_name.upper()} Evaluation: {EVALUATION_EPISODES} Episodes per Seed\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"\"\"**Figure 1** - Per-seed evaluation convergence showing running mean reward and standard deviation across {EVALUATION_EPISODES} deterministic episodes. The dashed red line marks the solved threshold ({SOLVED_THRESHOLD}).\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm: Evaluation Bar Chart (mean reward per seed with error bars)\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "    means = [np.mean(evaluation_results[algo_name][s]) for s in SEED_LIST]\n",
    "    stds = [np.std(evaluation_results[algo_name][s]) for s in SEED_LIST]\n",
    "    labels = [str(s) for s in SEED_LIST]\n",
    "\n",
    "    plt.figure(figsize=(max(8, 3 * len(SEED_LIST)), 6))\n",
    "    plt.bar(labels, means, yerr=stds, capsize=5, color=seed_colors[:len(SEED_LIST)], alpha=0.8)\n",
    "    plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "    plt.axhline(y=float(np.mean(all_r)), color='blue', linestyle='-', linewidth=2,\n",
    "                label=f'Overall Mean ({np.mean(all_r):.1f})')\n",
    "\n",
    "    plt.title(f\"{algo_name.upper()} Mean Reward per Seed ({EVALUATION_EPISODES} episodes each)\", fontsize=14)\n",
    "    plt.xlabel(\"Seed\")\n",
    "    plt.ylabel(\"Mean Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm: Reward Distribution Histograms (overlaid per seed)\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    for i, seed in enumerate(SEED_LIST):\n",
    "        plt.hist(evaluation_results[algo_name][seed], bins=10, alpha=0.5,\n",
    "                 color=seed_colors[i], edgecolor='black', label=f\"Seed {seed}\")\n",
    "\n",
    "    plt.axvline(x=float(np.mean(all_r)), color='blue', linestyle='-', linewidth=2,\n",
    "                label=f'Overall Mean ({np.mean(all_r):.1f})')\n",
    "    plt.axvline(x=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "    plt.title(f'{algo_name.upper()} Reward Distribution across Seeds')\n",
    "    plt.xlabel('Reward')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cross-Algorithm Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm: Combined Summary Table\n",
    "\n",
    "rows = []\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "    rows.append({\n",
    "        \"Algorithm\": algo_name.upper(),\n",
    "        \"Mean Reward\": f\"{np.mean(all_r):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(all_r):.2f}\",\n",
    "        \"Min Reward\": f\"{np.min(all_r):.2f}\",\n",
    "        \"Max Reward\": f\"{np.max(all_r):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_r >= 200).sum() / len(all_r) * 100:.1f}%\"\n",
    "    })\n",
    "\n",
    "print(f\"*** CROSS-ALGORITHM EVALUATION SUMMARY ***\")\n",
    "print(f\"Seeds: {SEED_LIST} | Episodes per seed: {EVALUATION_EPISODES}\")\n",
    "print(f\"Total episodes per algorithm: {EVALUATION_EPISODES * len(SEED_LIST)}\")\n",
    "print()\n",
    "print(pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm: Grouped Bar Chart (DQN vs PPO per seed)\n",
    "\n",
    "n_algos = len(algo_names)\n",
    "n_seeds = len(SEED_LIST)\n",
    "bar_width = 0.8 / n_algos\n",
    "x = np.arange(n_seeds)\n",
    "\n",
    "plt.figure(figsize=(max(10, 3 * n_seeds), 6))\n",
    "for i, algo_name in enumerate(algo_names):\n",
    "    means = [np.mean(evaluation_results[algo_name][s]) for s in SEED_LIST]\n",
    "    stds = [np.std(evaluation_results[algo_name][s]) for s in SEED_LIST]\n",
    "    offset = (i - (n_algos - 1) / 2) * bar_width\n",
    "    plt.bar(x + offset, means, bar_width, yerr=stds, capsize=4,\n",
    "            label=algo_name.upper(), alpha=0.8)\n",
    "\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.xticks(x, [str(s) for s in SEED_LIST])\n",
    "plt.title(f\"DQN vs PPO: Mean Reward per Seed ({EVALUATION_EPISODES} episodes each)\", fontsize=14)\n",
    "plt.xlabel(\"Seed\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm: Overall Mean Reward Bar Chart\n",
    "\n",
    "overall_means = []\n",
    "overall_stds = []\n",
    "for algo_name in algo_names:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "    overall_means.append(np.mean(all_r))\n",
    "    overall_stds.append(np.std(all_r))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar([a.upper() for a in algo_names], overall_means, yerr=overall_stds,\n",
    "               capsize=6, color=[algo_colors[a] for a in algo_names], alpha=0.8)\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "\n",
    "for bar, mean in zip(bars, overall_means):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5,\n",
    "             f'{mean:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.title(f\"Overall Mean Reward: DQN vs PPO ({EVALUATION_EPISODES * len(SEED_LIST)} episodes each)\", fontsize=14)\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm: Reward Distribution Comparison (overlaid histograms)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for algo_name in algo_names:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "    plt.hist(all_r, bins=15, alpha=0.5, color=algo_colors[algo_name],\n",
    "             edgecolor='black', label=f\"{algo_name.upper()} (mean={np.mean(all_r):.1f})\")\n",
    "\n",
    "plt.axvline(x=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.title('Reward Distribution: DQN vs PPO (all seeds combined)', fontsize=14)\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm: Box Plot Comparison per Seed\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    data = [evaluation_results[algo_name][seed] for algo_name in algo_names]\n",
    "    bp = ax.boxplot(data, labels=[a.upper() for a in algo_names], patch_artist=True)\n",
    "    for patch, algo_name in zip(bp['boxes'], algo_names):\n",
    "        patch.set_facecolor(algo_colors[algo_name])\n",
    "        patch.set_alpha(0.6)\n",
    "    ax.axhline(y=200, color='red', linestyle='--')\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Reward\")\n",
    "fig.suptitle(f\"DQN vs PPO: Reward Distribution per Seed ({EVALUATION_EPISODES} episodes each)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Statistical Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two statistical tests assess whether the observed performance difference between DQN and PPO is significant. The Mann-Whitney U test compares reward distributions (non-parametric, no normality assumption). The Chi-squared test compares success rates as a proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Significance: Mann-Whitney U Tests\n",
    "\n",
    "algo_all_rewards = {}\n",
    "for algo_name in algo_names:\n",
    "    algo_all_rewards[algo_name] = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST])\n",
    "\n",
    "# Reward comparison (Mann-Whitney U)\n",
    "mwu_result = stats.mannwhitneyu(\n",
    "    algo_all_rewards[algo_names[0]],\n",
    "    algo_all_rewards[algo_names[1]],\n",
    "    alternative='two-sided'\n",
    ")\n",
    "stat_reward = float(mwu_result.statistic)\n",
    "p_reward = float(mwu_result.pvalue)\n",
    "\n",
    "# Success rate comparison (Chi-squared)\n",
    "successes = []\n",
    "totals = []\n",
    "for algo_name in algo_names:\n",
    "    r = algo_all_rewards[algo_name]\n",
    "    successes.append(int((r >= 200).sum()))\n",
    "    totals.append(len(r))\n",
    "\n",
    "failures = [t - s for t, s in zip(totals, successes)]\n",
    "contingency = np.array([successes, failures])\n",
    "\n",
    "if np.all(contingency.sum(axis=1) > 0) and np.all(contingency.sum(axis=0) > 0):\n",
    "    chi2_result = stats.chi2_contingency(contingency)\n",
    "    chi2 = float(chi2_result[0])\n",
    "    p_success = float(chi2_result[1])\n",
    "    chi2_valid = True\n",
    "else:\n",
    "    chi2, p_success = 0.0, 1.0\n",
    "    chi2_valid = False\n",
    "\n",
    "chi2_note = \"\" if chi2_valid else \" (skipped: zero row/col)\"\n",
    "rows = [\n",
    "    {\n",
    "        \"Metric\": \"Mean Reward\",\n",
    "        f\"{algo_names[0].upper()} Value\": f\"{np.mean(algo_all_rewards[algo_names[0]]):.2f}\",\n",
    "        f\"{algo_names[1].upper()} Value\": f\"{np.mean(algo_all_rewards[algo_names[1]]):.2f}\",\n",
    "        \"Test\": \"Mann-Whitney U\",\n",
    "        \"Statistic\": f\"{stat_reward:.1f}\",\n",
    "        \"p-value\": f\"{p_reward:.4f}\",\n",
    "        \"Significant (p<0.05)\": \"Yes\" if p_reward < 0.05 else \"No\"\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Success Rate (>=200)\",\n",
    "        f\"{algo_names[0].upper()} Value\": f\"{successes[0]/totals[0]*100:.1f}%\",\n",
    "        f\"{algo_names[1].upper()} Value\": f\"{successes[1]/totals[1]*100:.1f}%\",\n",
    "        \"Test\": f\"Chi-squared{chi2_note}\",\n",
    "        \"Statistic\": f\"{chi2:.2f}\",\n",
    "        \"p-value\": f\"{p_success:.4f}\",\n",
    "        \"Significant (p<0.05)\": \"Yes\" if (chi2_valid and p_success < 0.05) else \"No\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"*** STATISTICAL SIGNIFICANCE TESTS ***\")\n",
    "print(f\"Sample size per algorithm: {totals[0]} episodes ({EVALUATION_EPISODES} episodes x {len(SEED_LIST)} seeds)\")\n",
    "print()\n",
    "print(pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Agent Baseline Evaluation\n",
    "\n",
    "random_results = {}\n",
    "\n",
    "for seed in SEED_LIST:\n",
    "    print(f\"Running random agent with seed {seed}...\")\n",
    "    env = gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED)\n",
    "    env.action_space.seed(seed)\n",
    "    episode_rewards = []\n",
    "\n",
    "    for ep in range(EVALUATION_EPISODES):\n",
    "        obs, info = env.reset(seed=seed + ep)\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += float(reward)\n",
    "            done = terminated or truncated\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "    random_results[seed] = np.array(episode_rewards)\n",
    "    env.close()\n",
    "\n",
    "print(\"Random baseline evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Comparison: Table + Chart\n",
    "\n",
    "algo_all_rewards = {a: np.concatenate([evaluation_results[a][s] for s in SEED_LIST]) for a in algo_names}\n",
    "all_random = np.concatenate([random_results[s] for s in SEED_LIST])\n",
    "\n",
    "rows = [\n",
    "    {\n",
    "        \"Agent\": \"Random\",\n",
    "        \"Mean Reward\": f\"{np.mean(all_random):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(all_random):.2f}\",\n",
    "        \"Min\": f\"{np.min(all_random):.2f}\",\n",
    "        \"Max\": f\"{np.max(all_random):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_random >= 200).sum() / len(all_random) * 100:.1f}%\"\n",
    "    }\n",
    "]\n",
    "for algo_name in algo_names:\n",
    "    all_r = algo_all_rewards[algo_name]\n",
    "    rows.append({\n",
    "        \"Agent\": algo_name.upper(),\n",
    "        \"Mean Reward\": f\"{np.mean(all_r):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(all_r):.2f}\",\n",
    "        \"Min\": f\"{np.min(all_r):.2f}\",\n",
    "        \"Max\": f\"{np.max(all_r):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_r >= 200).sum() / len(all_r) * 100:.1f}%\"\n",
    "    })\n",
    "\n",
    "print(\"*** BASELINE COMPARISON ***\")\n",
    "print(pd.DataFrame(rows).to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Bar chart\n",
    "agent_labels = [\"Random\"] + [a.upper() for a in algo_names]\n",
    "agent_means = [np.mean(all_random)] + [np.mean(algo_all_rewards[a]) for a in algo_names]\n",
    "agent_stds = [np.std(all_random)] + [np.std(algo_all_rewards[a]) for a in algo_names]\n",
    "bar_colors = [\"gray\"] + [algo_colors[a] for a in algo_names]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(agent_labels, agent_means, yerr=agent_stds, capsize=6,\n",
    "               color=bar_colors, alpha=0.8)\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "\n",
    "for bar, mean in zip(bars, agent_means):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5,\n",
    "             f'{mean:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.title(f\"Baseline Comparison: Random vs DQN vs PPO ({EVALUATION_EPISODES * len(SEED_LIST)} episodes each)\", fontsize=14)\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Agent Behavior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect per-step data: actions and trajectories\n",
    "\n",
    "action_counts = {}       # {algo: np.array of shape (4,)} total action counts\n",
    "trajectory_data = {}     # {algo: list of (x_positions, y_positions)} one per TRAJECTORY_EPISODES\n",
    "\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "    action_counts[algo_name] = np.zeros(len(ACTION_LABELS), dtype=int)\n",
    "    trajectory_data[algo_name] = []\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        load_path = best_model_paths.get(algo_name, {}).get(seed)\n",
    "        if load_path is None:\n",
    "            continue\n",
    "\n",
    "        def make_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        model = algo_class.load(load_path, env=DummyVecEnv([make_env]), device=DEVICE)\n",
    "        env = gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED)\n",
    "\n",
    "        for ep in range(EVALUATION_EPISODES):\n",
    "            obs, info = env.reset(seed=seed + ep)\n",
    "            done = False\n",
    "            x_pos, y_pos = [obs[0]], [obs[1]]\n",
    "\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                action_int = int(action)\n",
    "                action_counts[algo_name][action_int] += 1\n",
    "\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                x_pos.append(obs[0])\n",
    "                y_pos.append(obs[1])\n",
    "\n",
    "            if seed == SEED_LIST[0] and ep < TRAJECTORY_EPISODES:\n",
    "                trajectory_data[algo_name].append((np.array(x_pos), np.array(y_pos)))\n",
    "\n",
    "        env.close()\n",
    "\n",
    "    total_actions = action_counts[algo_name].sum()\n",
    "    print(f\"{algo_name.upper()}: {total_actions:,} total actions collected across {EVALUATION_EPISODES * len(SEED_LIST)} episodes\")\n",
    "\n",
    "print(\"\\nBehavior data collection complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Distribution: DQN vs PPO\n",
    "\n",
    "n_actions = len(ACTION_LABELS)\n",
    "x = np.arange(n_actions)\n",
    "bar_width = 0.35\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Absolute counts\n",
    "for i, algo_name in enumerate(algo_names):\n",
    "    offset = (i - 0.5) * bar_width\n",
    "    ax1.bar(x + offset, action_counts[algo_name], bar_width,\n",
    "            label=algo_name.upper(), color=algo_colors[algo_name], alpha=0.8)\n",
    "\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(ACTION_LABELS, rotation=15)\n",
    "ax1.set_title(\"Action Counts (Absolute)\", fontsize=13)\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Percentage distribution\n",
    "for i, algo_name in enumerate(algo_names):\n",
    "    pcts = action_counts[algo_name] / action_counts[algo_name].sum() * 100\n",
    "    offset = (i - 0.5) * bar_width\n",
    "    bars = ax2.bar(x + offset, pcts, bar_width,\n",
    "                   label=algo_name.upper(), color=algo_colors[algo_name], alpha=0.8)\n",
    "    for bar, pct in zip(bars, pcts):\n",
    "        ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,\n",
    "                 f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(ACTION_LABELS, rotation=15)\n",
    "ax2.set_title(\"Action Distribution (Percentage)\", fontsize=13)\n",
    "ax2.set_ylabel(\"Percentage (%)\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "fig.suptitle(f\"DQN vs PPO: Action Distribution ({EVALUATION_EPISODES * len(SEED_LIST)} episodes each)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory Plots: x-y paths of the lander\n",
    "\n",
    "fig, axes = plt.subplots(1, len(algo_names), figsize=(8 * len(algo_names), 6), sharey=True)\n",
    "if len(algo_names) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "traj_colors = list(plt.colormaps[\"Set2\"](range(8)))\n",
    "\n",
    "for ax, algo_name in zip(axes, algo_names):\n",
    "    for i, (x_pos, y_pos) in enumerate(trajectory_data[algo_name]):\n",
    "        ax.plot(x_pos, y_pos, color=traj_colors[i], linewidth=1.5, alpha=0.8,\n",
    "                label=f\"Episode {i+1}\")\n",
    "        ax.scatter(x_pos[0], y_pos[0], color=traj_colors[i], marker='o', s=60, zorder=5)\n",
    "        ax.scatter(x_pos[-1], y_pos[-1], color=traj_colors[i], marker='x', s=80, zorder=5)\n",
    "\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    ax.scatter(0, 0, color='red', marker='^', s=120, zorder=10, label='Landing Pad')\n",
    "\n",
    "    ax.set_title(f\"{algo_name.upper()} Trajectories (seed {SEED_LIST[0]})\", fontsize=13)\n",
    "    ax.set_xlabel(\"X Position\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Y Position\")\n",
    "fig.suptitle(f\"Lander Trajectories: DQN vs PPO ({TRAJECTORY_EPISODES} episodes each)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory Comparison: DQN vs PPO overlaid on one chart\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for algo_name in algo_names:\n",
    "    x_pos, y_pos = trajectory_data[algo_name][0]\n",
    "    plt.plot(x_pos, y_pos, color=algo_colors[algo_name], linewidth=2, alpha=0.8,\n",
    "             label=f\"{algo_name.upper()}\")\n",
    "    plt.scatter(x_pos[0], y_pos[0], color=algo_colors[algo_name], marker='o', s=80, zorder=5)\n",
    "    plt.scatter(x_pos[-1], y_pos[-1], color=algo_colors[algo_name], marker='x', s=100, zorder=5)\n",
    "\n",
    "plt.scatter(0, 0, color='red', marker='^', s=150, zorder=10, label='Landing Pad')\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "plt.title(f\"DQN vs PPO: Landing Trajectory Comparison (seed {SEED_LIST[0]}, episode 1)\", fontsize=14)\n",
    "plt.xlabel(\"X Position\")\n",
    "plt.ylabel(\"Y Position\")\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sample Efficiency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Efficiency: Mean Reward and Success Rate vs Training Timesteps\n",
    "# Extracted from the EvalCallback logs (evaluations.npz)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "for algo_name in algo_names:\n",
    "    all_timesteps = None\n",
    "    all_mean_rewards = []\n",
    "    all_success_rates = []\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        log_path = eval_log_paths.get(algo_name, {}).get(seed)\n",
    "        if log_path is None:\n",
    "            continue\n",
    "\n",
    "        data = np.load(log_path, allow_pickle=True)\n",
    "        timesteps = data[\"timesteps\"]\n",
    "        results = data[\"results\"]\n",
    "\n",
    "        mean_rewards = np.array([np.mean(r) for r in results])\n",
    "        success_rates = np.array([np.sum(r >= 200) / len(r) * 100 for r in results])\n",
    "\n",
    "        if all_timesteps is None:\n",
    "            all_timesteps = timesteps\n",
    "        all_mean_rewards.append(mean_rewards[:len(all_timesteps)])\n",
    "        all_success_rates.append(success_rates[:len(all_timesteps)])\n",
    "\n",
    "    if all_timesteps is None or not all_mean_rewards:\n",
    "        continue\n",
    "\n",
    "    # Align to shortest\n",
    "    min_len = min(len(a) for a in all_mean_rewards)\n",
    "    all_timesteps = all_timesteps[:min_len]\n",
    "    all_mean_rewards = np.array([a[:min_len] for a in all_mean_rewards])\n",
    "    all_success_rates = np.array([a[:min_len] for a in all_success_rates])\n",
    "\n",
    "    mean_r = all_mean_rewards.mean(axis=0)\n",
    "    std_r = all_mean_rewards.std(axis=0)\n",
    "    mean_s = all_success_rates.mean(axis=0)\n",
    "    std_s = all_success_rates.std(axis=0)\n",
    "\n",
    "    ax1.plot(all_timesteps, mean_r, color=algo_colors[algo_name], linewidth=2, label=algo_name.upper())\n",
    "    ax1.fill_between(all_timesteps, mean_r - std_r, mean_r + std_r, color=algo_colors[algo_name], alpha=0.15)\n",
    "\n",
    "    ax2.plot(all_timesteps, mean_s, color=algo_colors[algo_name], linewidth=2, label=algo_name.upper())\n",
    "    ax2.fill_between(all_timesteps, mean_s - std_s, mean_s + std_s, color=algo_colors[algo_name], alpha=0.15)\n",
    "\n",
    "ax1.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "ax1.set_title(\"Mean Reward vs Environment Steps\", fontsize=13)\n",
    "ax1.set_xlabel(\"Environment Steps\")\n",
    "ax1.set_ylabel(\"Mean Reward\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.axhline(y=100, color='green', linestyle=':', alpha=0.4)\n",
    "ax2.set_title(\"Success Rate vs Environment Steps\", fontsize=13)\n",
    "ax2.set_xlabel(\"Environment Steps\")\n",
    "ax2.set_ylabel(\"Success Rate (%)\")\n",
    "ax2.set_ylim(-5, 105)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "fig.suptitle(f\"Sample Efficiency: DQN vs PPO (mean +/- std across {len(SEED_LIST)} seeds)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Efficiency: Timestep at which each algorithm first crossed the solved threshold\n",
    "# per seed and on average\n",
    "\n",
    "solved_threshold = 200\n",
    "\n",
    "rows = []\n",
    "for algo_name in algo_names:\n",
    "    first_solved_steps = []\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        log_path = eval_log_paths.get(algo_name, {}).get(seed)\n",
    "        if log_path is None:\n",
    "            first_solved_steps.append(None)\n",
    "            continue\n",
    "\n",
    "        data = np.load(log_path, allow_pickle=True)\n",
    "        timesteps = data[\"timesteps\"]\n",
    "        results = data[\"results\"]\n",
    "\n",
    "        found = False\n",
    "        for i, r in enumerate(results):\n",
    "            if np.mean(r) >= solved_threshold:\n",
    "                first_solved_steps.append(int(timesteps[i]))\n",
    "                rows.append({\n",
    "                    \"Algorithm\": algo_name.upper(),\n",
    "                    \"Seed\": seed,\n",
    "                    \"First Solved at Step\": f\"{int(timesteps[i]):,}\",\n",
    "                })\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "        if not found:\n",
    "            first_solved_steps.append(None)\n",
    "            rows.append({\n",
    "                \"Algorithm\": algo_name.upper(),\n",
    "                \"Seed\": seed,\n",
    "                \"First Solved at Step\": \"Not reached\",\n",
    "            })\n",
    "\n",
    "    valid = [s for s in first_solved_steps if s is not None]\n",
    "    if valid:\n",
    "        rows.append({\n",
    "            \"Algorithm\": algo_name.upper(),\n",
    "            \"Seed\": \"Mean\",\n",
    "            \"First Solved at Step\": f\"{int(np.mean(valid)):,}\",\n",
    "        })\n",
    "\n",
    "print(\"*** SAMPLE EFFICIENCY: FIRST SOLVED TIMESTEP ***\")\n",
    "print(f\"Solved = mean evaluation reward >= {solved_threshold}\")\n",
    "print(f\"Evaluated every {25_000:,} timesteps with {20} episodes\")\n",
    "print()\n",
    "print(pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROLLING_WINDOW = 100\n",
    "\n",
    "display(Markdown(f\"\"\"### Training Milestone Timeline\n",
    "\n",
    "This chart overlays the training reward curves with vertical milestone markers showing when each\n",
    "algorithm reached key performance thresholds. The background curves show the rolling mean reward\n",
    "(window={ROLLING_WINDOW} episodes) plotted against cumulative environment steps, reconstructed from the\n",
    "per-episode training logs. Three milestones are marked per algorithm:\n",
    "- **Best model** (solid line): the checkpoint selected by the combined metric (mean - std) during training\n",
    "- **First solved** (dashed line): first point where the rolling {ROLLING_WINDOW}-episode mean reward >= {SOLVED_THRESHOLD}\n",
    "- **First all-pass** (dotted line): first point where all {ROLLING_WINDOW} consecutive episodes scored >= {SOLVED_THRESHOLD}\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Milestone Timeline\n",
    "# Requires: training_log.npz (episode_rewards, episode_lengths) per run\n",
    "#           evaluations.npz (timesteps, results) per run\n",
    "\n",
    "def discover_training_logs(session_prefix):\n",
    "    \"\"\"\n",
    "    Scan models/{algo}/{timestamp}/training_log.npz and return a dict:\n",
    "        {algo: {seed: path_to_training_log}}\n",
    "    \"\"\"\n",
    "    models_root = os.path.join(NOTEBOOK_DIR, \"../models\")\n",
    "    logs = {}\n",
    "\n",
    "    for algo_name in ALGORITHM_MAP:\n",
    "        logs[algo_name] = {}\n",
    "        algo_dir = os.path.join(models_root, algo_name)\n",
    "        if not os.path.isdir(algo_dir):\n",
    "            continue\n",
    "\n",
    "        for run_folder in sorted(glob.glob(os.path.join(algo_dir, \"????-??-??_??_??_??\"))):\n",
    "            log_path = os.path.join(run_folder, \"training_log.npz\")\n",
    "            if not os.path.isfile(log_path):\n",
    "                continue\n",
    "\n",
    "            for f in os.listdir(run_folder):\n",
    "                if f.startswith(session_prefix) and f.endswith(\".zip\") and f != \"best_model.zip\":\n",
    "                    seed_str = f.replace(\".zip\", \"\").split(\"_\")[-1]\n",
    "                    if seed_str.isdigit():\n",
    "                        seed_int = int(seed_str)\n",
    "                        if seed_int in SEED_LIST:\n",
    "                            logs[algo_name][seed_int] = log_path\n",
    "                    break\n",
    "\n",
    "    return logs\n",
    "\n",
    "\n",
    "training_log_paths = discover_training_logs(SESSION_PREFIX)\n",
    "\n",
    "print(\"Discovered training logs:\")\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    for seed in SEED_LIST:\n",
    "        path = training_log_paths.get(algo_name, {}).get(seed)\n",
    "        status = path if path else \"NOT FOUND\"\n",
    "        print(f\"  {algo_name.upper()} seed {seed}: {status}\")\n",
    "\n",
    "# --- Compute milestones and build chart ---\n",
    "\n",
    "ROLLING_WINDOW = 100\n",
    "SOLVED_THRESHOLD = 200\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "# Lighter per-seed colors\n",
    "seed_alpha = 0.25\n",
    "seed_line_styles = ['-', '--', ':']\n",
    "\n",
    "milestone_summary = []  # for the summary table\n",
    "\n",
    "for algo_name in algo_names:\n",
    "    color = algo_colors[algo_name]\n",
    "\n",
    "    for si, seed in enumerate(SEED_LIST):\n",
    "        log_path = training_log_paths.get(algo_name, {}).get(seed)\n",
    "        if log_path is None:\n",
    "            continue\n",
    "\n",
    "        data = np.load(log_path)\n",
    "        ep_rewards = data['episode_rewards']\n",
    "        ep_lengths = data['episode_lengths']\n",
    "\n",
    "        # Cumulative environment steps at end of each episode\n",
    "        cum_steps = np.cumsum(ep_lengths)\n",
    "\n",
    "        # Rolling mean reward (window=100)\n",
    "        rolling_mean = pd.Series(ep_rewards).rolling(ROLLING_WINDOW).mean().values\n",
    "\n",
    "        # Plot per-seed curve (light)\n",
    "        ax.plot(cum_steps, rolling_mean, color=color, alpha=seed_alpha,\n",
    "                linestyle=seed_line_styles[si], linewidth=1)\n",
    "\n",
    "        # --- Milestone: first rolling mean >= 200 ---\n",
    "        first_solved_step = None\n",
    "        for j in range(ROLLING_WINDOW, len(ep_rewards)):\n",
    "            if rolling_mean[j] >= SOLVED_THRESHOLD:\n",
    "                first_solved_step = int(cum_steps[j])\n",
    "                break\n",
    "\n",
    "        # --- Milestone: first 100 consecutive all >= 200 ---\n",
    "        first_allpass_step = None\n",
    "        for j in range(ROLLING_WINDOW, len(ep_rewards)):\n",
    "            window = ep_rewards[j - ROLLING_WINDOW + 1 : j + 1]\n",
    "            if np.all(window >= SOLVED_THRESHOLD):\n",
    "                first_allpass_step = int(cum_steps[j])\n",
    "                break\n",
    "\n",
    "        milestone_summary.append({\n",
    "            'Algorithm': algo_name.upper(),\n",
    "            'Seed': seed,\n",
    "            'First Solved (rolling mean >= 200)': f'{first_solved_step:,}' if first_solved_step else 'Not reached',\n",
    "            'First All-Pass (all 100 >= 200)': f'{first_allpass_step:,}' if first_allpass_step else 'Not reached',\n",
    "        })\n",
    "\n",
    "    # --- Best model milestone (from evaluations.npz, averaged across seeds) ---\n",
    "    best_steps = []\n",
    "    for seed in SEED_LIST:\n",
    "        eval_path = eval_log_paths.get(algo_name, {}).get(seed)\n",
    "        if eval_path is None:\n",
    "            continue\n",
    "        edata = np.load(eval_path, allow_pickle=True)\n",
    "        timesteps = edata['timesteps']\n",
    "        results = edata['results']\n",
    "        best_score = -np.inf\n",
    "        best_step = 0\n",
    "        for k in range(len(timesteps)):\n",
    "            score = np.mean(results[k]) - np.std(results[k])\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_step = int(timesteps[k])\n",
    "        best_steps.append(best_step)\n",
    "\n",
    "    # --- Draw aggregate milestone lines ---\n",
    "    # Collect per-seed first-solved steps for this algorithm\n",
    "    algo_milestones = [m for m in milestone_summary if m['Algorithm'] == algo_name.upper()]\n",
    "\n",
    "    solved_steps = []\n",
    "    allpass_steps = []\n",
    "    for m in algo_milestones:\n",
    "        v = m['First Solved (rolling mean >= 200)']\n",
    "        if v != 'Not reached':\n",
    "            solved_steps.append(int(v.replace(',', '')))\n",
    "        v = m['First All-Pass (all 100 >= 200)']\n",
    "        if v != 'Not reached':\n",
    "            allpass_steps.append(int(v.replace(',', '')))\n",
    "\n",
    "    label_prefix = algo_name.upper()\n",
    "\n",
    "    if best_steps:\n",
    "        mean_best = int(np.mean(best_steps))\n",
    "        ax.axvline(x=mean_best, color=color, linestyle='-', linewidth=2, alpha=0.8,\n",
    "                   label=f'{label_prefix} best model (mean step {mean_best:,})')\n",
    "\n",
    "    if solved_steps:\n",
    "        mean_solved = int(np.mean(solved_steps))\n",
    "        ax.axvline(x=mean_solved, color=color, linestyle='--', linewidth=2, alpha=0.8,\n",
    "                   label=f'{label_prefix} first solved (mean step {mean_solved:,})')\n",
    "\n",
    "    if allpass_steps:\n",
    "        mean_allpass = int(np.mean(allpass_steps))\n",
    "        ax.axvline(x=mean_allpass, color=color, linestyle=':', linewidth=2, alpha=0.8,\n",
    "                   label=f'{label_prefix} first all-pass (mean step {mean_allpass:,})')\n",
    "\n",
    "# Solved threshold line\n",
    "ax.axhline(y=SOLVED_THRESHOLD, color='red', linestyle='--', linewidth=1, alpha=0.5, label='Solved Threshold (200)')\n",
    "\n",
    "ax.set_title('Training Milestone Timeline: DQN vs PPO', fontsize=14)\n",
    "ax.set_xlabel('Environment Steps')\n",
    "ax.set_ylabel('Rolling Mean Reward (window=100)')\n",
    "ax.legend(loc='lower right', fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(f\"\"\"**Figure 14** - Training milestone timeline showing rolling mean reward curves (per-seed, light lines)\n",
    "with vertical markers for best model selection, first solved (rolling {ROLLING_WINDOW}-episode mean >= {SOLVED_THRESHOLD}),\n",
    "and first all-pass (all {ROLLING_WINDOW} consecutive episodes >= {SOLVED_THRESHOLD}) milestones.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Milestone Summary Table\n",
    "\n",
    "print('*** TRAINING MILESTONES PER SEED ***')\n",
    "print(f'Rolling window: {ROLLING_WINDOW} episodes')\n",
    "print(f'Solved threshold: mean reward >= {SOLVED_THRESHOLD}')\n",
    "print(f'All-pass threshold: every episode in window >= {SOLVED_THRESHOLD}')\n",
    "print()\n",
    "print(pd.DataFrame(milestone_summary).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Final vs Best Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate final models (end-of-training) and compare against best models\n",
    "\n",
    "final_evaluation_results = {}\n",
    "\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "    final_evaluation_results[algo_name] = {}\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        load_path = final_model_paths.get(algo_name, {}).get(seed)\n",
    "        if load_path is None:\n",
    "            print(f\"SKIPPING {algo_name.upper()} seed {seed} - final model not found\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Evaluating {algo_name.upper()} seed {seed} (final model)...\")\n",
    "\n",
    "        def make_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        model = algo_class.load(load_path, env=DummyVecEnv([make_env]), device=DEVICE)\n",
    "\n",
    "        eval_env = Monitor(gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED))\n",
    "        eval_env.reset(seed=seed)\n",
    "\n",
    "        rewards, _ = evaluate_policy(\n",
    "            model, eval_env,\n",
    "            n_eval_episodes=EVALUATION_EPISODES,\n",
    "            deterministic=True,\n",
    "            return_episode_rewards=True\n",
    "        )\n",
    "        final_evaluation_results[algo_name][seed] = np.array(rewards)\n",
    "        eval_env.close()\n",
    "\n",
    "print(\"\\nFinal model evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final vs Best Model Comparison Table\n",
    "\n",
    "rows = []\n",
    "for algo_name in algo_names:\n",
    "    for seed in SEED_LIST:\n",
    "        best_r = evaluation_results.get(algo_name, {}).get(seed)\n",
    "        final_r = final_evaluation_results.get(algo_name, {}).get(seed)\n",
    "\n",
    "        if best_r is None or final_r is None:\n",
    "            continue\n",
    "\n",
    "        rows.append({\n",
    "            \"Algorithm\": algo_name.upper(),\n",
    "            \"Seed\": seed,\n",
    "            \"Best Mean\": f\"{np.mean(best_r):.2f}\",\n",
    "            \"Best Success\": f\"{(best_r >= 200).sum() / len(best_r) * 100:.0f}%\",\n",
    "            \"Final Mean\": f\"{np.mean(final_r):.2f}\",\n",
    "            \"Final Success\": f\"{(final_r >= 200).sum() / len(final_r) * 100:.0f}%\",\n",
    "            \"Delta Mean\": f\"{np.mean(best_r) - np.mean(final_r):+.2f}\",\n",
    "        })\n",
    "\n",
    "print(\"*** BEST MODEL vs FINAL MODEL ***\")\n",
    "print(f\"Episodes per evaluation: {EVALUATION_EPISODES}\")\n",
    "print()\n",
    "print(pd.DataFrame(rows).to_string(index=False))\n",
    "print()\n",
    "print(\"Positive Delta Mean = best model selection improved over end-of-training snapshot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameter Exploration Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tables: Final Configuration\n",
    "\n",
    "for algo_name in ALGORITHM_MAP:\n",
    "    params = ALGO_PARAMS[algo_name]\n",
    "    rows = [{\"Parameter\": k, \"Value\": str(v)} for k, v in params.items()]\n",
    "    rows.append({\"Parameter\": \"total_timesteps\", \"Value\": f\"{TOTAL_TIMESTEPS:,}\"})\n",
    "    rows.append({\"Parameter\": \"device\", \"Value\": DEVICE})\n",
    "    rows.append({\"Parameter\": \"policy\", \"Value\": \"MlpPolicy\"})\n",
    "\n",
    "    print(f\"*** {algo_name.upper()} Final Hyperparameters ***\")\n",
    "    print(pd.DataFrame(rows).to_string(index=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Exploration Journey\n",
    "# Summary of configurations tested across lab sessions and their results.\n",
    "# Fill in the actual results from your earlier experiments (lab004, lab009, etc.)\n",
    "\n",
    "print(\"*** DQN HYPERPARAMETER EXPLORATION ***\")\n",
    "print()\n",
    "\n",
    "dqn_configs = [\n",
    "    {\n",
    "        \"Config\": \"A (baseline)\",\n",
    "        \"Key Changes\": \"lr=6.3e-4, buffer=100k, eps_final=0.01, explore=0.5, soft updates (tau=0.005)\",\n",
    "        \"Success Rate\": \"84.0%\",\n",
    "        \"Mean Reward\": \"245.47\",\n",
    "        \"Notes\": \"Slow exploration decay, soft target updates\",\n",
    "    },\n",
    "    {\n",
    "        \"Config\": \"B (Zoo-style)\",\n",
    "        \"Key Changes\": \"lr=linear(6.3e-4), buffer=1M, eps_final=0.1, explore=0.12, hard updates (250)\",\n",
    "        \"Success Rate\": \"96.0%\",\n",
    "        \"Mean Reward\": \"263.02\",\n",
    "        \"Notes\": \"Short exploration, high final epsilon, hard target updates\",\n",
    "    },\n",
    "    {\n",
    "        \"Config\": \"C (final)\",\n",
    "        \"Key Changes\": \"lr=linear(6.3e-4), buffer=750k, eps_final=0.1, explore=0.12, hard updates (250), learning_starts=50k\",\n",
    "        \"Success Rate\": \"TBD (lab011)\",\n",
    "        \"Mean Reward\": \"TBD (lab011)\",\n",
    "        \"Notes\": \"Added learning_starts buffer fill, gradient_steps=4 explicit\",\n",
    "    },\n",
    "]\n",
    "print(pd.DataFrame(dqn_configs).to_string(index=False))\n",
    "\n",
    "print()\n",
    "print(\"*** PPO HYPERPARAMETER EXPLORATION ***\")\n",
    "print()\n",
    "\n",
    "ppo_configs = [\n",
    "    {\n",
    "        \"Config\": \"A (final)\",\n",
    "        \"Key Changes\": \"lr=2.5e-4, gamma=0.999, n_steps=2048, batch=64, n_epochs=10, ent_coef=0.01\",\n",
    "        \"Success Rate\": \"98.0%\",\n",
    "        \"Mean Reward\": \"269.21\",\n",
    "        \"Notes\": \"Standard config, single environment\",\n",
    "    },\n",
    "]\n",
    "print(pd.DataFrame(ppo_configs).to_string(index=False))\n",
    "print()\n",
    "print(\"Note: Update Config C results and add any additional configs tested during lab011 runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GIF Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIF Visualizations (one per algorithm per seed, best model)\n",
    "\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "    output_dir = os.path.join(NOTEBOOK_DIR, \"outputs_\" + algo_name)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        load_path = best_model_paths.get(algo_name, {}).get(seed)\n",
    "        if load_path is None:\n",
    "            print(f\"SKIPPING GIF for {algo_name.upper()} seed {seed} - best model not found\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Generating GIF for {algo_name.upper()} seed {seed} (best model)...\")\n",
    "\n",
    "        def make_vis_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        vis_model = algo_class.load(load_path, env=DummyVecEnv([make_vis_env]), device=DEVICE)\n",
    "\n",
    "        vis_env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "        frames = []\n",
    "        obs, info = vis_env.reset(seed=seed)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _ = vis_model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, info = vis_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            frames.append(vis_env.render())\n",
    "\n",
    "        vis_env.close()\n",
    "\n",
    "        gif_path = os.path.join(output_dir, f\"{algo_name}_seed{seed}.gif\")\n",
    "        imageio.mimsave(gif_path, frames, fps=30)\n",
    "        print(f\"  Saved: {gif_path}\")\n",
    "        display(Image(filename=gif_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Experimental Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment inspection\n",
    "\n",
    "env_tmp = gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED)\n",
    "print(f\"Environment: {GYMNASIUM_MODEL}\")\n",
    "print(f\"Observation space: {env_tmp.observation_space}\")\n",
    "print(f\"Action space: {env_tmp.action_space}\")\n",
    "print(f\"Wind enabled: {WIND_ENABLED}\")\n",
    "\n",
    "obs, info = env_tmp.reset(seed=42)\n",
    "print(f\"\\nSample observation: {obs}\")\n",
    "print(f\"Observation labels: [x, y, vx, vy, angle, angular_vel, left_leg, right_leg]\")\n",
    "env_tmp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and library versions\n",
    "\n",
    "import stable_baselines3\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Stable-Baselines3: {stable_baselines3.__version__}\")\n",
    "print(f\"Gymnasium: {gym.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"CUDA: {torch.version.cuda if torch.cuda.is_available() else 'Not available'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conceptual Discussion\n",
    "\n",
    "This section presents a theoretical discussion of the core conceptual differences between value-based and policy-based reinforcement learning methods, the sources of instability in deep RL, and the exploration mechanisms used in DQN and PPO.\n",
    "\n",
    "---\n",
    "\n",
    "### 7.1 Value-Based vs Policy-Based Learning\n",
    "\n",
    "Reinforcement Learning algorithms can be categorised according to what is parameterised and optimised.\n",
    "\n",
    "#### Value-Based Learning\n",
    "\n",
    "Value-based methods approximate a value function, typically the action-value function:\n",
    "\n",
    "$$\n",
    "Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0 = s, a_0 = a \\right]\n",
    "$$\n",
    "\n",
    "The optimal policy is derived implicitly:\n",
    "\n",
    "$$\n",
    "\\pi(s) = \\arg\\max_a Q(s,a)\n",
    "$$\n",
    "\n",
    "Deep Q-Networks (DQN) approximate $Q(s,a; \\theta)$ and update parameters using the Bellman optimality equation:\n",
    "\n",
    "$$\n",
    "Q(s,a) = r + \\gamma \\max_{a'} Q(s',a')\n",
    "$$\n",
    "\n",
    "This introduces **bootstrapping**, since the target depends on the model's own predictions.\n",
    "\n",
    "Key characteristics:\n",
    "\n",
    "- Off-policy learning\n",
    "- Bootstrapping\n",
    "- Policy extracted via maximisation\n",
    "\n",
    "However, the use of the max operator combined with function approximation introduces instability.\n",
    "\n",
    "#### Policy-Based Learning\n",
    "\n",
    "Policy-based methods parameterise the policy directly:\n",
    "\n",
    "$$\n",
    "\\pi_\\theta(a|s)\n",
    "$$\n",
    "\n",
    "and optimise the expected return:\n",
    "\n",
    "$$\n",
    "J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[R]\n",
    "$$\n",
    "\n",
    "Using the policy gradient theorem:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) =\n",
    "\\mathbb{E}_{\\pi_\\theta}\n",
    "\\left[\n",
    "\\nabla_\\theta \\log \\pi_\\theta(a|s) A^\\pi(s,a)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "These methods:\n",
    "\n",
    "- Learn the policy directly\n",
    "- Avoid explicit maximisation over value estimates\n",
    "- Naturally handle continuous action spaces\n",
    "\n",
    "Because they do not rely on a max operator over Q-values, they tend to be structurally more stable in deep settings.\n",
    "\n",
    "---\n",
    "\n",
    "### 7.2 Overestimation and Instability in Value-Based Algorithms\n",
    "\n",
    "#### Overestimation Bias\n",
    "\n",
    "Overestimation arises from the maximisation step $\\max_a Q(s,a)$. If Q-value estimates contain zero-mean noise:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\max(X_i)] \\ge \\max(\\mathbb{E}[X_i])\n",
    "$$\n",
    "\n",
    "Thus, even unbiased estimators lead to positively biased maximum estimates.\n",
    "\n",
    "In DQN, the same network selects the action (argmax) and evaluates the action. This coupling amplifies overestimation bias.\n",
    "\n",
    "#### Structural Instability: The Deadly Triad\n",
    "\n",
    "Deep value-based RL combines:\n",
    "\n",
    "1. Function approximation\n",
    "2. Bootstrapping\n",
    "3. Off-policy learning\n",
    "\n",
    "This combination is known as the **deadly triad**, which may cause divergence or unstable oscillations.\n",
    "\n",
    "PPO avoids these structural issues because:\n",
    "\n",
    "- It does not use a max over Q-values\n",
    "- It performs on-policy updates\n",
    "- Value estimates are auxiliary rather than directly driving action selection\n",
    "\n",
    "---\n",
    "\n",
    "### 7.3 PPO Clipping and Generalised Advantage Estimation (GAE)\n",
    "\n",
    "#### PPO Clipped Objective\n",
    "\n",
    "PPO constrains policy updates through the clipped surrogate objective:\n",
    "\n",
    "$$\n",
    "L^{CLIP}(\\theta) =\n",
    "\\mathbb{E}\n",
    "\\left[\n",
    "\\min\n",
    "\\left(\n",
    "r_t(\\theta) A_t,\n",
    "\\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t\n",
    "\\right)\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "r_t(\\theta) =\n",
    "\\frac{\\pi_\\theta(a_t|s_t)}\n",
    "{\\pi_{\\theta_{old}}(a_t|s_t)}\n",
    "$$\n",
    "\n",
    "This objective:\n",
    "\n",
    "- Approximates a trust-region method\n",
    "- Prevents excessively large updates\n",
    "- Improves training stability\n",
    "\n",
    "#### Generalised Advantage Estimation (GAE)\n",
    "\n",
    "The advantage function is defined as:\n",
    "\n",
    "$$\n",
    "A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s)\n",
    "$$\n",
    "\n",
    "GAE computes:\n",
    "\n",
    "$$\n",
    "A_t^{GAE(\\lambda)} =\n",
    "\\sum_{l=0}^{\\infty}\n",
    "(\\gamma \\lambda)^l \\delta_{t+l}\n",
    "$$\n",
    "\n",
    "where the temporal-difference residual is:\n",
    "\n",
    "$$\n",
    "\\delta_t =\n",
    "r_t + \\gamma V(s_{t+1}) - V(s_t)\n",
    "$$\n",
    "\n",
    "The parameter $\\lambda \\in [0,1]$ controls the bias-variance trade-off:\n",
    "\n",
    "- $\\lambda = 0$: lower variance, higher bias\n",
    "- $\\lambda = 1$: lower bias, higher variance\n",
    "\n",
    "GAE reduces variance in policy gradient updates while maintaining acceptable bias, improving sample efficiency and stability.\n",
    "\n",
    "---\n",
    "\n",
    "### 7.4 Epsilon-Greedy vs Entropy-Driven Exploration\n",
    "\n",
    "#### Epsilon-Greedy Exploration (DQN)\n",
    "\n",
    "The behaviour policy is defined as:\n",
    "\n",
    "$$\n",
    "\\pi(a|s) =\n",
    "\\begin{cases}\n",
    "\\text{random action} & \\text{with probability } \\epsilon \\\\\n",
    "\\arg\\max_a Q(s,a) & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Limitations:\n",
    "\n",
    "- Uniform and uninformed exploration\n",
    "- Independent of uncertainty\n",
    "- Becomes fully greedy as $\\epsilon \\to 0$\n",
    "\n",
    "Exploration is externally imposed rather than integrated into optimisation.\n",
    "\n",
    "#### Entropy-Regularised Exploration (PPO)\n",
    "\n",
    "PPO includes an entropy bonus in the objective:\n",
    "\n",
    "$$\n",
    "L =\n",
    "L^{CLIP}\n",
    "+\n",
    "\\beta\n",
    "\\mathbb{E}[H(\\pi_\\theta(\\cdot|s))]\n",
    "$$\n",
    "\n",
    "where entropy is defined as:\n",
    "\n",
    "$$\n",
    "H(\\pi) =\n",
    "-\n",
    "\\sum_a\n",
    "\\pi(a|s)\n",
    "\\log \\pi(a|s)\n",
    "$$\n",
    "\n",
    "This mechanism:\n",
    "\n",
    "- Encourages stochastic policies early in training\n",
    "- Gradually reduces randomness\n",
    "- Couples exploration with optimisation\n",
    "\n",
    "Unlike epsilon-greedy, exploration is intrinsic to the learning objective.\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptual Summary\n",
    "\n",
    "In summary, value-based methods optimise an approximation of the value function and derive the policy implicitly through action maximisation. This structural reliance on the max operator and bootstrapping introduces overestimation bias and makes deep value-based methods particularly sensitive to instability when combined with function approximation and off-policy learning. In contrast, policy-based methods directly parameterise and optimise the policy, avoiding explicit maximisation over value estimates and thereby reducing structural sources of instability. PPO further improves stability through its clipped surrogate objective, which constrains policy updates, and through Generalised Advantage Estimation, which manages the bias-variance trade-off in gradient estimates. Regarding exploration, epsilon-greedy strategies impose external and uninformed randomness that vanishes as epsilon decays, whereas entropy-regularised exploration integrates stochasticity directly into the optimisation objective, allowing exploration to adapt naturally as learning progresses. Overall, these theoretical differences explain why PPO typically exhibits smoother and more stable learning dynamics compared to DQN in complex environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_taap_p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
