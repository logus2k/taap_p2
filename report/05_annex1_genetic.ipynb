{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm for Neural Network Optimization\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN and PPO are deep reinforcement learning methods that optimize neural network weights through gradient descent, computing how each weight should change to improve performance, then applying small, calculated adjustments through backpropagation. This process requires differentiable loss functions, careful learning rate tuning, and mechanisms like experience replay or policy clipping to remain stable.\n",
    "\n",
    "A genetic algorithm (GA) approaches the same reinforcement learning problem from a fundamentally different angle. It treats the neural network's weights as a flat array of numbers, called a \"genome\", and applies principles borrowed from biological evolution to find good values, without ever computing a gradient. While still reinforcement learning (the agent learns from environment interaction and reward signals), it is not deep learning, as the optimization is evolutionary rather than gradient-based.\n",
    "\n",
    "The process works as follows:\n",
    "\n",
    "1. **Initialization**: Create a population of 50 random genomes, each representing a complete neural network with random weights.\n",
    "\n",
    "2. **Evaluation**: Each genome is decoded into a neural network and tested directly in the LunarLander environment. The network receives the same 8 observations (position, velocity, angle, leg contacts) and outputs one of 4 actions, identical to the DQN/PPO networks. Its fitness score is simply the total reward accumulated during the episode.\n",
    "\n",
    "3. **Selection**: Genomes are ranked by fitness. The top 20% become eligible parents, and the top 3 survive unchanged into the next generation (elitism).\n",
    "\n",
    "4. **Crossover**: Two parents are selected and their weights are combined to produce a child genome, mixing genetic material from both successful individuals.\n",
    "\n",
    "5. **Mutation**: Small random perturbations are applied to the child's weights, introducing variation. The mutation rate decays linearly over generations, with large perturbations early for exploration and smaller ones later for refinement.\n",
    "\n",
    "6. **Repeat**: The new population replaces the old one, and the process repeats for thousands of generations.\n",
    "\n",
    "The key distinction is what drives learning. In DQN/PPO, the network receives precise mathematical feedback about which direction to adjust each weight. In the GA, there is no such signal. The algorithm only knows \"this set of weights scored 280, that one scored 150.\" It discovers good weights purely through trial, selection, and incremental refinement over generations.\n",
    "\n",
    "This makes the GA less sample-efficient, requiring roughly 150x more environment interactions than DQN or PPO to reach comparable performance, but also entirely gradient-free. It requires no loss function design, no replay buffer, no policy clipping, and no learning rate schedule. The neural network architecture itself is a simple feedforward network with two hidden layers of 10 neurons each using tanh activations, far smaller than the 256x256 networks used by DQN and PPO, yet sufficient for the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, random, time, glob\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import gymnasium as gym\n",
    "import imageio.v2 as imageio\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9607cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from genetic.evolution.genetic_algorithm import GeneticAlgorithm\n",
    "from genetic.evolution.neural_network import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Configuration\n",
    "\n",
    "SEED_LIST = [42, 123, 3407]\n",
    "\n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "GYMNASIUM_MODEL = \"LunarLander-v3\"\n",
    "\n",
    "WIND_ENABLED = False\n",
    "\n",
    "# Session prefix — used in final genome filenames (e.g. annex1_ga_42.npy)\n",
    "SESSION_PREFIX = \"annex1\"\n",
    "\n",
    "# GA hyperparameters\n",
    "INPUT_SIZE = 8\n",
    "HIDDEN1_SIZE = 10\n",
    "HIDDEN2_SIZE = 10\n",
    "OUTPUT_SIZE = 4\n",
    "\n",
    "POPULATION_SIZE = 50\n",
    "MUTATION_RATE = 0.05\n",
    "GENERATIONS = 5000\n",
    "EVAL_SEEDS_PER_GEN = 3  # seeds used to evaluate each genome during training\n",
    "\n",
    "# Evaluation\n",
    "EVALUATION_EPISODES = 20\n",
    "\n",
    "# Trajectory visualization episodes\n",
    "TRAJECTORY_EPISODES = 3\n",
    "\n",
    "# Parallelization\n",
    "MAX_WORKERS = 20\n",
    "\n",
    "# LunarLander-v3 action labels\n",
    "ACTION_LABELS = [\"Do Nothing\", \"Fire Left\", \"Fire Main\", \"Fire Right\"]\n",
    "\n",
    "print(f\"Session prefix: {SESSION_PREFIX}\")\n",
    "print(f\"Seeds: {SEED_LIST}\")\n",
    "print(f\"Wind enabled: {WIND_ENABLED}\")\n",
    "print(f\"Generations: {GENERATIONS}\")\n",
    "print(f\"Population size: {POPULATION_SIZE}\")\n",
    "print(f\"Mutation rate: {MUTATION_RATE}\")\n",
    "print(f\"Eval seeds per generation: {EVAL_SEEDS_PER_GEN}\")\n",
    "print(f\"Evaluation episodes per seed: {EVALUATION_EPISODES}\")\n",
    "print(f\"Max workers: {MAX_WORKERS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"NumPy:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment inspection\n",
    "env_tmp = gym.make(GYMNASIUM_MODEL)\n",
    "\n",
    "print(\"Observation space:\", env_tmp.observation_space)\n",
    "print(\"Action space:\", env_tmp.action_space)\n",
    "\n",
    "obs, info = env_tmp.reset()\n",
    "print(\"Initial observation:\", obs)\n",
    "\n",
    "env_tmp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "def evaluate_genome_deterministic(genome, n_episodes, seed=None):\n",
    "    \"\"\"\n",
    "    Evaluate a genome deterministically over n_episodes.\n",
    "    No early termination. Returns list of per-episode rewards.\n",
    "    \"\"\"\n",
    "    nn = NeuralNetwork(INPUT_SIZE, HIDDEN1_SIZE, HIDDEN2_SIZE, OUTPUT_SIZE, genome)\n",
    "    env = gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED)\n",
    "    rewards = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        ep_seed = seed + ep if seed is not None else None\n",
    "        obs, _ = env.reset(seed=ep_seed)\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            output = nn.forward(obs)\n",
    "            action = np.argmax(output)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += float(reward)\n",
    "\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    env.close()\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def record_genome_gif(genome, seed, output_path):\n",
    "    \"\"\"\n",
    "    Record a single episode GIF of a genome.\n",
    "    \"\"\"\n",
    "    nn = NeuralNetwork(INPUT_SIZE, HIDDEN1_SIZE, HIDDEN2_SIZE, OUTPUT_SIZE, genome)\n",
    "    env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    frames = []\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        frames.append(env.render())\n",
    "        output = nn.forward(obs)\n",
    "        action = np.argmax(output)\n",
    "        obs, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    env.close()\n",
    "    imageio.mimsave(output_path, frames, fps=30, loop=0)\n",
    "    return output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop: GA x seeds\n",
    "\n",
    "from IPython.display import display as ipy_display\n",
    "\n",
    "training_results = {}   # {seed: {\"best\": [], \"avg\": [], \"worst\": []}}\n",
    "training_times = {}     # {seed: seconds}\n",
    "best_genomes = {}       # {seed: genome}\n",
    "best_genome_paths = {}  # {seed: path}\n",
    "total_env_steps = {}    # {seed: int}\n",
    "eval_histories = {}     # {seed: list of eval dicts}\n",
    "\n",
    "CHART_UPDATE_FREQ = 100\n",
    "EVAL_FREQ_GENS = 100\n",
    "EVAL_N_EPISODES = 20\n",
    "SOLVED_THRESHOLD = 200\n",
    "\n",
    "for seed in SEED_LIST:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"GA | Seed {seed}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    # Create timestamped run directory\n",
    "    run_timestamp = datetime.now().strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "    run_dir = os.path.join(NOTEBOOK_DIR, \"../models\", \"ga\", run_timestamp)\n",
    "    os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "    set_all_seeds(seed)\n",
    "\n",
    "    ga = GeneticAlgorithm(\n",
    "        INPUT_SIZE, HIDDEN1_SIZE, HIDDEN2_SIZE, OUTPUT_SIZE,\n",
    "        POPULATION_SIZE, MUTATION_RATE, render=False, max_workers=MAX_WORKERS,\n",
    "        generations=GENERATIONS\n",
    "    )\n",
    "\n",
    "    best_fitness_history = []\n",
    "    avg_fitness_history = []\n",
    "    worst_fitness_history = []\n",
    "    env_steps = 0\n",
    "    sorted_population = []\n",
    "\n",
    "    # Best model tracking (matches DQN/PPO two-tier selection)\n",
    "    best_combined_score = -np.inf\n",
    "    best_genome = np.array([])\n",
    "    best_eval_mean = 0.0\n",
    "    best_eval_std = np.inf\n",
    "    best_eval_success = 0.0\n",
    "    best_gen = 0\n",
    "    any_solved = False\n",
    "    eval_history = []\n",
    "\n",
    "    plot_handle = None\n",
    "    stats_handle = None\n",
    "    eval_handle = None\n",
    "\n",
    "    t_start = time.time()\n",
    "\n",
    "    for gen in range(GENERATIONS):\n",
    "        sorted_population = ga.evaluate_population(gen)\n",
    "        fitness_values = [fitness for _, fitness in sorted_population]\n",
    "        best_fitness = fitness_values[0]\n",
    "        worst_fitness = fitness_values[-1]\n",
    "        avg_fitness = sum(fitness_values) / len(fitness_values)\n",
    "\n",
    "        best_fitness_history.append(best_fitness)\n",
    "        avg_fitness_history.append(avg_fitness)\n",
    "        worst_fitness_history.append(worst_fitness)\n",
    "\n",
    "        env_steps += POPULATION_SIZE * EVAL_SEEDS_PER_GEN * 300\n",
    "\n",
    "        # Periodic deterministic evaluation of generation's best genome\n",
    "        if (gen + 1) % EVAL_FREQ_GENS == 0:\n",
    "            candidate_genome = sorted_population[0][0]\n",
    "            eval_rewards = evaluate_genome_deterministic(\n",
    "                candidate_genome, EVAL_N_EPISODES, seed=seed\n",
    "            )\n",
    "            eval_mean = np.mean(eval_rewards)\n",
    "            eval_std = np.std(eval_rewards)\n",
    "            eval_success = np.sum(np.array(eval_rewards) >= SOLVED_THRESHOLD) / len(eval_rewards) * 100\n",
    "            combined_score = eval_mean - eval_std\n",
    "            is_solved = eval_mean >= SOLVED_THRESHOLD\n",
    "\n",
    "            eval_history.append({\n",
    "                \"generation\": gen + 1,\n",
    "                \"mean\": eval_mean,\n",
    "                \"std\": eval_std,\n",
    "                \"success\": eval_success,\n",
    "                \"score\": combined_score,\n",
    "                \"solved\": is_solved,\n",
    "            })\n",
    "\n",
    "            # Two-tier best model selection (matches DQN/PPO logic)\n",
    "            save_new_best = False\n",
    "            if is_solved:\n",
    "                if not any_solved:\n",
    "                    save_new_best = True\n",
    "                    any_solved = True\n",
    "                elif combined_score > best_combined_score:\n",
    "                    save_new_best = True\n",
    "            elif not any_solved:\n",
    "                if combined_score > best_combined_score:\n",
    "                    save_new_best = True\n",
    "\n",
    "            solved_tag = \" [SOLVED]\" if is_solved else \"\"\n",
    "            eval_text = (\n",
    "                f\"Eval @ Gen {gen + 1} | \"\n",
    "                f\"Reward: {eval_mean:.2f} +/- {eval_std:.2f}{solved_tag} | \"\n",
    "                f\"Success: {eval_success:.0f}% | \"\n",
    "                f\"Score (mean-std): {combined_score:.2f} | \"\n",
    "                f\"Best: {best_combined_score:.2f}\"\n",
    "            )\n",
    "\n",
    "            if save_new_best:\n",
    "                eval_text += \" >> New best genome!\"\n",
    "                best_combined_score = combined_score\n",
    "                best_eval_mean = eval_mean\n",
    "                best_eval_std = eval_std\n",
    "                best_eval_success = eval_success\n",
    "                best_gen = gen + 1\n",
    "                best_genome = candidate_genome.copy()\n",
    "                np.save(os.path.join(run_dir, \"best_genome.npy\"), best_genome)\n",
    "\n",
    "            if eval_handle is None:\n",
    "                eval_handle = ipy_display(eval_text, display_id=True)\n",
    "            else:\n",
    "                eval_handle.update(eval_text)\n",
    "\n",
    "        if (gen + 1) % CHART_UPDATE_FREQ == 0:\n",
    "            recent_avg = np.mean(avg_fitness_history[-100:])\n",
    "            stats_text = (\n",
    "                f\"Generation {gen + 1} | \"\n",
    "                f\"Best: {best_fitness:>8.2f} | \"\n",
    "                f\"Avg: {avg_fitness:>8.2f} | \"\n",
    "                f\"Worst: {worst_fitness:>8.2f} | \"\n",
    "                f\"Best Score: {best_combined_score:>8.2f} | \"\n",
    "                f\"Recent Avg(100): {recent_avg:.1f} | \"\n",
    "                f\"MR: {ga.mutation_rate:.4f}\"\n",
    "            )\n",
    "            if stats_handle is None:\n",
    "                stats_handle = ipy_display(stats_text, display_id=True)\n",
    "            else:\n",
    "                stats_handle.update(stats_text)\n",
    "\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "            ax1.plot(best_fitness_history, alpha=0.3, color='blue')\n",
    "            ax1.plot(avg_fitness_history, alpha=0.3, color='orange')\n",
    "            window = min(50, len(best_fitness_history))\n",
    "            rolling_best = pd.Series(best_fitness_history).rolling(window).mean()\n",
    "            rolling_avg = pd.Series(avg_fitness_history).rolling(window).mean()\n",
    "            ax1.plot(rolling_best, color='blue', linewidth=2, label='Best (rolling)')\n",
    "            ax1.plot(rolling_avg, color='orange', linewidth=2, label='Avg (rolling)')\n",
    "            ax1.axhline(y=200, color='red', linestyle='--', alpha=0.5)\n",
    "            ax1.set_title(f\"Fitness - Gen {gen + 1}\")\n",
    "            ax1.set_xlabel(\"Generation\")\n",
    "            ax1.set_ylabel(\"Fitness\")\n",
    "            ax1.legend(fontsize=8)\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "\n",
    "            success = np.array(best_fitness_history) >= 200\n",
    "            rolling_success = pd.Series(success.astype(float)).rolling(window).mean() * 100\n",
    "            ax2.plot(rolling_success, color='green', linewidth=2)\n",
    "            ax2.axhline(y=100, color='green', linestyle=':', alpha=0.4)\n",
    "            ax2.set_title(f\"Success Rate (best >= 200)\")\n",
    "            ax2.set_xlabel(\"Generation\")\n",
    "            ax2.set_ylabel(\"Success Rate (%)\")\n",
    "            ax2.set_ylim(-5, 105)\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "\n",
    "            plt.tight_layout()\n",
    "\n",
    "            if plot_handle is None:\n",
    "                plot_handle = ipy_display(fig, display_id=True)\n",
    "            else:\n",
    "                plot_handle.update(fig)\n",
    "            plt.close(fig)\n",
    "\n",
    "        ga.next_generation(sorted_population)\n",
    "\n",
    "    t_elapsed = time.time() - t_start\n",
    "\n",
    "    # Save final genome (best of last generation)\n",
    "    final_genome = sorted_population[0][0]\n",
    "    final_path = os.path.join(run_dir, f\"{SESSION_PREFIX}_ga_{seed}.npy\")\n",
    "    np.save(final_path, final_genome)\n",
    "\n",
    "    # Save eval log\n",
    "    eval_log_dir = os.path.join(run_dir, \"eval_log\")\n",
    "    os.makedirs(eval_log_dir, exist_ok=True)\n",
    "    if eval_history:\n",
    "        np.savez(\n",
    "            os.path.join(eval_log_dir, \"evaluations\"),\n",
    "            generations=np.array([e[\"generation\"] for e in eval_history]),\n",
    "            means=np.array([e[\"mean\"] for e in eval_history]),\n",
    "            stds=np.array([e[\"std\"] for e in eval_history]),\n",
    "            scores=np.array([e[\"score\"] for e in eval_history]),\n",
    "            success_rates=np.array([e[\"success\"] for e in eval_history]),\n",
    "        )\n",
    "\n",
    "    # Save fitness history\n",
    "    history_path = os.path.join(run_dir, f\"fitness_history_seed{seed}.npz\")\n",
    "    np.savez(history_path,\n",
    "             best=best_fitness_history,\n",
    "             avg=avg_fitness_history,\n",
    "             worst=worst_fitness_history)\n",
    "\n",
    "    training_results[seed] = {\n",
    "        \"best\": best_fitness_history,\n",
    "        \"avg\": avg_fitness_history,\n",
    "        \"worst\": worst_fitness_history,\n",
    "    }\n",
    "    training_times[seed] = t_elapsed\n",
    "    best_genomes[seed] = best_genome\n",
    "    best_genome_paths[seed] = {\n",
    "        \"run_dir\": run_dir,\n",
    "        \"final\": final_path,\n",
    "        \"best\": os.path.join(run_dir, \"best_genome.npy\"),\n",
    "    }\n",
    "    total_env_steps[seed] = env_steps\n",
    "    eval_histories[seed] = eval_history\n",
    "\n",
    "    print(f\"\\nTraining time: {t_elapsed/60:.1f} min ({t_elapsed:.0f} s)\")\n",
    "    print(f\"Final genome:  {final_path}\")\n",
    "    print(f\"Best genome:   {os.path.join(run_dir, 'best_genome.npy')}\")\n",
    "    print(f\"Eval log:      {eval_log_dir}\")\n",
    "    print(\n",
    "        f\"Best genome stats: \"\n",
    "        f\"Reward: {best_eval_mean:.2f} +/- {best_eval_std:.2f} | \"\n",
    "        f\"Success: {best_eval_success:.0f}% | \"\n",
    "        f\"Score (mean-std): {best_combined_score:.2f} | \"\n",
    "        f\"@ Gen {best_gen}\"\n",
    "    )\n",
    "    print(f\"Estimated env steps: {env_steps:,}\")\n",
    "\n",
    "print(f\"\\nGA: All {len(SEED_LIST)} seeds trained.\")\n",
    "\n",
    "# Training Summary Table\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BEST MODEL SUMMARY (all seeds)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "rows = []\n",
    "for seed in SEED_LIST:\n",
    "    best_eval = max(eval_histories[seed], key=lambda x: x[\"score\"])\n",
    "    rows.append({\n",
    "        \"Seed\": seed,\n",
    "        \"Mean Reward\": f\"{best_eval['mean']:.2f}\",\n",
    "        \"Std Reward\": f\"{best_eval['std']:.2f}\",\n",
    "        \"Success\": f\"{best_eval['success']:.0f}%\",\n",
    "        \"Score (mean-std)\": f\"{best_eval['score']:.2f}\",\n",
    "        \"@ Generation\": best_eval[\"generation\"],\n",
    "        \"Time (min)\": f\"{training_times[seed]/60:.1f}\",\n",
    "    })\n",
    "\n",
    "print(pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Time Summary\n",
    "\n",
    "rows = []\n",
    "for seed in SEED_LIST:\n",
    "    t = training_times[seed]\n",
    "    rows.append({\n",
    "        \"Algorithm\": \"GA\",\n",
    "        \"Seed\": seed,\n",
    "        \"Time (s)\": f\"{t:.0f}\",\n",
    "        \"Time (min)\": f\"{t/60:.1f}\",\n",
    "    })\n",
    "\n",
    "times = list(training_times.values())\n",
    "rows.append({\n",
    "    \"Algorithm\": \"GA\",\n",
    "    \"Seed\": \"Mean\",\n",
    "    \"Time (s)\": f\"{np.mean(times):.0f}\",\n",
    "    \"Time (min)\": f\"{np.mean(times)/60:.1f}\",\n",
    "})\n",
    "\n",
    "print(\"*** TRAINING TIME SUMMARY ***\")\n",
    "print(f\"Generations: {GENERATIONS} | Population: {POPULATION_SIZE} | Workers: {MAX_WORKERS}\")\n",
    "print()\n",
    "print(pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Seed: Fitness over Generations\n",
    "\n",
    "seed_colors = list(plt.colormaps[\"tab10\"](range(10)))\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    ax.plot(training_results[seed][\"best\"], alpha=0.7, label=\"Best\")\n",
    "    ax.plot(training_results[seed][\"avg\"], alpha=0.7, label=\"Average\")\n",
    "    ax.plot(training_results[seed][\"worst\"], alpha=0.5, label=\"Worst\")\n",
    "    ax.axhline(y=200, color='red', linestyle='--', alpha=0.5)\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.set_xlabel(\"Generation\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "axes[0].set_ylabel(\"Fitness\")\n",
    "fig.suptitle(\"GA - Fitness over Generations\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated: Rolling Best Fitness Overlay (all seeds on one chart)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i, seed in enumerate(SEED_LIST):\n",
    "    rolling = pd.Series(training_results[seed][\"best\"]).rolling(50).mean()\n",
    "    plt.plot(rolling, color=seed_colors[i], linewidth=2, label=f\"Seed {seed}\")\n",
    "\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.title(\"GA Training: Rolling Best Fitness (window=50)\", fontsize=14)\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregated: Rolling Average Fitness Overlay (all seeds on one chart)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i, seed in enumerate(SEED_LIST):\n",
    "    rolling = pd.Series(training_results[seed][\"avg\"]).rolling(50).mean()\n",
    "    plt.plot(rolling, color=seed_colors[i], linewidth=2, label=f\"Seed {seed}\")\n",
    "\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.title(\"GA Training: Rolling Average Fitness (window=50)\", fontsize=14)\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Fitness\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Success Rate over Training (best fitness >= 200)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i, seed in enumerate(SEED_LIST):\n",
    "    success = np.array(training_results[seed][\"best\"]) >= 200\n",
    "    rolling_success = pd.Series(success.astype(float)).rolling(50).mean() * 100\n",
    "    plt.plot(rolling_success, color=seed_colors[i], linewidth=2, label=f\"Seed {seed}\")\n",
    "\n",
    "plt.axhline(y=100, color='green', linestyle=':', alpha=0.4, label='100%')\n",
    "plt.axhline(y=50, color='gray', linestyle=':', alpha=0.4, label='50%')\n",
    "plt.title(\"GA Training: Rolling Success Rate - Best Genome (window=50)\", fontsize=14)\n",
    "plt.xlabel(\"Generation\")\n",
    "plt.ylabel(\"Success Rate (%)\")\n",
    "plt.ylim(-5, 105)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Score Progression during Training (from periodic evaluations)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    history = eval_histories[seed]\n",
    "    if history:\n",
    "        gens = [e[\"generation\"] for e in history]\n",
    "        means = np.array([e[\"mean\"] for e in history])\n",
    "        stds = np.array([e[\"std\"] for e in history])\n",
    "        scores = [e[\"score\"] for e in history]\n",
    "\n",
    "        ax.plot(gens, means, color='blue', linewidth=2, label='Mean Reward')\n",
    "        ax.fill_between(gens, means - stds, means + stds, color='blue', alpha=0.15)\n",
    "        ax.plot(gens, scores, color='green', linewidth=1.5, linestyle='--', label='Score (mean-std)')\n",
    "        ax.axhline(y=200, color='red', linestyle='--', alpha=0.5)\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.set_xlabel(\"Generation\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Reward\")\n",
    "fig.suptitle(\"GA - Deterministic Eval Score during Training\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation: deterministic episodes per seed (best genome)\n",
    "\n",
    "evaluation_results = {}  # {seed: np.array}\n",
    "\n",
    "for seed in SEED_LIST:\n",
    "    print(f\"Evaluating GA seed {seed} (best genome)...\")\n",
    "\n",
    "    set_all_seeds(seed)\n",
    "    genome = best_genomes[seed]\n",
    "\n",
    "    rewards = evaluate_genome_deterministic(genome, EVALUATION_EPISODES, seed=seed)\n",
    "    evaluation_results[seed] = np.array(rewards)\n",
    "\n",
    "    mean_r = np.mean(rewards)\n",
    "    std_r = np.std(rewards)\n",
    "    success = np.sum(np.array(rewards) >= 200) / len(rewards) * 100\n",
    "    print(f\"  Reward: {mean_r:.2f} +/- {std_r:.2f} | Success: {success:.0f}%\")\n",
    "\n",
    "print(f\"\\nEvaluation complete for all seeds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Summary Table\n",
    "\n",
    "rows = []\n",
    "for seed in SEED_LIST:\n",
    "    r = evaluation_results[seed]\n",
    "    rows.append({\n",
    "        \"Seed\": seed,\n",
    "        \"Mean Reward\": f\"{np.mean(r):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(r):.2f}\",\n",
    "        \"Min Reward\": f\"{np.min(r):.2f}\",\n",
    "        \"Max Reward\": f\"{np.max(r):.2f}\",\n",
    "        \"Success Rate\": f\"{(r >= 200).sum() / len(r) * 100:.1f}%\"\n",
    "    })\n",
    "\n",
    "all_r = np.concatenate([evaluation_results[s] for s in SEED_LIST])\n",
    "rows.append({\n",
    "    \"Seed\": \"Overall\",\n",
    "    \"Mean Reward\": f\"{np.mean(all_r):.2f}\",\n",
    "    \"Std Dev\": f\"{np.std(all_r):.2f}\",\n",
    "    \"Min Reward\": f\"{np.min(all_r):.2f}\",\n",
    "    \"Max Reward\": f\"{np.max(all_r):.2f}\",\n",
    "    \"Success Rate\": f\"{(all_r >= 200).sum() / len(all_r) * 100:.1f}%\"\n",
    "})\n",
    "\n",
    "print(f\"*** GA MULTI-SEED EVALUATION SUMMARY ***\")\n",
    "print(f\"Episodes per seed: {EVALUATION_EPISODES} | Total: {len(all_r)}\")\n",
    "print(pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Convergence Plots (per seed)\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    rewards = evaluation_results[seed]\n",
    "    episodes = np.arange(1, len(rewards) + 1)\n",
    "    running_mean = np.cumsum(rewards) / episodes\n",
    "    running_std = np.array([np.std(rewards[:i]) for i in episodes])\n",
    "\n",
    "    ax.scatter(episodes, rewards, color='gray', alpha=0.4, s=20, label='Episode Reward')\n",
    "    ax.plot(episodes, running_mean, color='blue', linewidth=2, label='Running Mean')\n",
    "    ax.fill_between(episodes, running_mean - running_std, running_mean + running_std,\n",
    "                    color='blue', alpha=0.15)\n",
    "    ax.axhline(y=200, color='red', linestyle='--')\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.set_xlabel(\"Episode\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Reward\")\n",
    "fig.suptitle(f\"GA Evaluation: {EVALUATION_EPISODES} Episodes per Seed\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Bar Chart (mean reward per seed with error bars)\n",
    "\n",
    "all_r = np.concatenate([evaluation_results[s] for s in SEED_LIST])\n",
    "means = [np.mean(evaluation_results[s]) for s in SEED_LIST]\n",
    "stds_vals = [np.std(evaluation_results[s]) for s in SEED_LIST]\n",
    "labels = [str(s) for s in SEED_LIST]\n",
    "\n",
    "plt.figure(figsize=(max(8, 3 * len(SEED_LIST)), 6))\n",
    "plt.bar(labels, means, yerr=stds_vals, capsize=5, color=seed_colors[:len(SEED_LIST)], alpha=0.8)\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.axhline(y=float(np.mean(all_r)), color='blue', linestyle='-', linewidth=2,\n",
    "            label=f'Overall Mean ({np.mean(all_r):.1f})')\n",
    "\n",
    "plt.title(f\"GA Mean Reward per Seed ({EVALUATION_EPISODES} episodes each)\", fontsize=14)\n",
    "plt.xlabel(\"Seed\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Distribution Histograms (overlaid per seed)\n",
    "\n",
    "all_r = np.concatenate([evaluation_results[s] for s in SEED_LIST])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i, seed in enumerate(SEED_LIST):\n",
    "    plt.hist(evaluation_results[seed], bins=10, alpha=0.5,\n",
    "             color=seed_colors[i], edgecolor='black', label=f\"Seed {seed}\")\n",
    "\n",
    "plt.axvline(x=float(np.mean(all_r)), color='blue', linestyle='-', linewidth=2,\n",
    "            label=f'Overall Mean ({np.mean(all_r):.1f})')\n",
    "plt.axvline(x=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.title('GA Reward Distribution across Seeds')\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot per Seed\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "data = [evaluation_results[seed] for seed in SEED_LIST]\n",
    "bp = ax.boxplot(data, tick_labels=[str(s) for s in SEED_LIST], patch_artist=True)\n",
    "for patch, color in zip(bp['boxes'], seed_colors[:len(SEED_LIST)]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "ax.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "ax.set_title(f\"GA: Reward Distribution per Seed ({EVALUATION_EPISODES} episodes each)\", fontsize=14)\n",
    "ax.set_xlabel(\"Seed\")\n",
    "ax.set_ylabel(\"Reward\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Baseline Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Agent Baseline Evaluation\n",
    "\n",
    "random_results = {}\n",
    "\n",
    "for seed in SEED_LIST:\n",
    "    print(f\"Running random agent with seed {seed}...\")\n",
    "    env = gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED)\n",
    "    env.action_space.seed(seed)\n",
    "    episode_rewards = []\n",
    "\n",
    "    for ep in range(EVALUATION_EPISODES):\n",
    "        obs, info = env.reset(seed=seed + ep)\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            total_reward += float(reward)\n",
    "            done = terminated or truncated\n",
    "\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "    random_results[seed] = np.array(episode_rewards)\n",
    "    env.close()\n",
    "\n",
    "print(\"Random baseline evaluation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Comparison: Table + Chart\n",
    "\n",
    "all_ga = np.concatenate([evaluation_results[s] for s in SEED_LIST])\n",
    "all_random = np.concatenate([random_results[s] for s in SEED_LIST])\n",
    "\n",
    "rows = [\n",
    "    {\n",
    "        \"Agent\": \"Random\",\n",
    "        \"Mean Reward\": f\"{np.mean(all_random):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(all_random):.2f}\",\n",
    "        \"Min\": f\"{np.min(all_random):.2f}\",\n",
    "        \"Max\": f\"{np.max(all_random):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_random >= 200).sum() / len(all_random) * 100:.1f}%\"\n",
    "    },\n",
    "    {\n",
    "        \"Agent\": \"GA\",\n",
    "        \"Mean Reward\": f\"{np.mean(all_ga):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(all_ga):.2f}\",\n",
    "        \"Min\": f\"{np.min(all_ga):.2f}\",\n",
    "        \"Max\": f\"{np.max(all_ga):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_ga >= 200).sum() / len(all_ga) * 100:.1f}%\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"*** BASELINE COMPARISON ***\")\n",
    "print(pd.DataFrame(rows).to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Bar chart\n",
    "agent_labels = [\"Random\", \"GA\"]\n",
    "agent_means = [np.mean(all_random), np.mean(all_ga)]\n",
    "agent_stds = [np.std(all_random), np.std(all_ga)]\n",
    "bar_colors = [\"gray\", \"tab:green\"]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.bar(agent_labels, agent_means, yerr=agent_stds, capsize=6,\n",
    "               color=bar_colors, alpha=0.8)\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "\n",
    "for bar, mean in zip(bars, agent_means):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5,\n",
    "             f'{mean:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.title(f\"Baseline Comparison: Random vs GA ({EVALUATION_EPISODES * len(SEED_LIST)} episodes each)\", fontsize=14)\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Statistical Significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Significance: GA vs Random (Mann-Whitney U + Chi-squared)\n",
    "\n",
    "# --- Reward comparison (Mann-Whitney U) ---\n",
    "mwu_result = stats.mannwhitneyu(all_ga, all_random, alternative='two-sided')\n",
    "stat_reward = float(mwu_result.statistic)\n",
    "p_reward = float(mwu_result.pvalue)\n",
    "\n",
    "# --- Success rate comparison (Chi-squared) ---\n",
    "ga_successes = int((all_ga >= 200).sum())\n",
    "random_successes = int((all_random >= 200).sum())\n",
    "ga_total = len(all_ga)\n",
    "random_total = len(all_random)\n",
    "\n",
    "contingency = np.array([\n",
    "    [ga_successes, ga_total - ga_successes],\n",
    "    [random_successes, random_total - random_successes]\n",
    "])\n",
    "\n",
    "if np.all(contingency.sum(axis=0) > 0) and np.all(contingency.sum(axis=1) > 0):\n",
    "    result = stats.chi2_contingency(contingency)\n",
    "    chi2 = float(result.statistic)  # type: ignore[attr-defined]\n",
    "    p_success = float(result.pvalue)  # type: ignore[attr-defined]\n",
    "    chi2_valid = True\n",
    "else:\n",
    "    chi2, p_success = 0.0, 1.0\n",
    "    chi2_valid = False\n",
    "\n",
    "# --- Results table ---\n",
    "chi2_note = \"\" if chi2_valid else \" (skipped: zero row/col)\"\n",
    "rows = [\n",
    "    {\n",
    "        \"Metric\": \"Mean Reward\",\n",
    "        \"GA Value\": f\"{np.mean(all_ga):.2f}\",\n",
    "        \"Random Value\": f\"{np.mean(all_random):.2f}\",\n",
    "        \"Test\": \"Mann-Whitney U\",\n",
    "        \"Statistic\": f\"{stat_reward:.1f}\",\n",
    "        \"p-value\": f\"{p_reward:.4f}\",\n",
    "        \"Significant (p<0.05)\": \"Yes\" if p_reward < 0.05 else \"No\"\n",
    "    },\n",
    "    {\n",
    "        \"Metric\": \"Success Rate (>=200)\",\n",
    "        \"GA Value\": f\"{ga_successes/ga_total*100:.1f}%\",\n",
    "        \"Random Value\": f\"{random_successes/random_total*100:.1f}%\",\n",
    "        \"Test\": f\"Chi-squared{chi2_note}\",\n",
    "        \"Statistic\": f\"{chi2:.2f}\",\n",
    "        \"p-value\": f\"{p_success:.4f}\",\n",
    "        \"Significant (p<0.05)\": \"Yes\" if (chi2_valid and p_success < 0.05) else \"No\"\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"*** STATISTICAL SIGNIFICANCE TESTS: GA vs Random ***\")\n",
    "print(f\"Sample size per agent: {ga_total} episodes ({EVALUATION_EPISODES} episodes x {len(SEED_LIST)} seeds)\")\n",
    "print()\n",
    "print(pd.DataFrame(rows).to_string(index=False))\n",
    "print()\n",
    "if p_reward < 0.05:\n",
    "    print(f\"The reward difference between GA and Random is statistically significant (p={p_reward:.4f}).\")\n",
    "else:\n",
    "    print(f\"No statistically significant reward difference between GA and Random (p={p_reward:.4f}).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Agent Behavior Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect per-step data: actions and trajectories\n",
    "\n",
    "action_counts = np.zeros(len(ACTION_LABELS), dtype=int)\n",
    "trajectory_data = []  # list of (x_positions, y_positions)\n",
    "\n",
    "for seed in SEED_LIST:\n",
    "    genome = best_genomes[seed]\n",
    "    nn = NeuralNetwork(INPUT_SIZE, HIDDEN1_SIZE, HIDDEN2_SIZE, OUTPUT_SIZE, genome)\n",
    "    env = gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED)\n",
    "\n",
    "    for ep in range(EVALUATION_EPISODES):\n",
    "        obs, info = env.reset(seed=seed + ep)\n",
    "        done = False\n",
    "        x_pos, y_pos = [obs[0]], [obs[1]]\n",
    "\n",
    "        while not done:\n",
    "            output = nn.forward(obs)\n",
    "            action = int(np.argmax(output))\n",
    "            action_counts[action] += 1\n",
    "\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            x_pos.append(obs[0])\n",
    "            y_pos.append(obs[1])\n",
    "\n",
    "        # Keep trajectory for the first TRAJECTORY_EPISODES episodes of the first seed\n",
    "        if seed == SEED_LIST[0] and ep < TRAJECTORY_EPISODES:\n",
    "            trajectory_data.append((np.array(x_pos), np.array(y_pos)))\n",
    "\n",
    "    env.close()\n",
    "\n",
    "total_actions = action_counts.sum()\n",
    "print(f\"GA: {total_actions:,} total actions collected across {EVALUATION_EPISODES * len(SEED_LIST)} episodes\")\n",
    "print(\"\\nBehavior data collection complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Distribution\n",
    "\n",
    "n_actions = len(ACTION_LABELS)\n",
    "x = np.arange(n_actions)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Absolute counts\n",
    "ax1.bar(x, action_counts, color='tab:green', alpha=0.8)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(ACTION_LABELS, rotation=15)\n",
    "ax1.set_title(\"Action Counts (Absolute)\", fontsize=13)\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Percentage distribution\n",
    "pcts = action_counts / action_counts.sum() * 100\n",
    "bars = ax2.bar(x, pcts, color='tab:green', alpha=0.8)\n",
    "for bar, pct in zip(bars, pcts):\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,\n",
    "             f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(ACTION_LABELS, rotation=15)\n",
    "ax2.set_title(\"Action Distribution (Percentage)\", fontsize=13)\n",
    "ax2.set_ylabel(\"Percentage (%)\")\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "fig.suptitle(f\"GA: Action Distribution ({EVALUATION_EPISODES * len(SEED_LIST)} episodes)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory Plots: x-y paths of the lander\n",
    "\n",
    "traj_colors = list(plt.colormaps[\"Set2\"](range(8)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "for i, (x_pos, y_pos) in enumerate(trajectory_data):\n",
    "    ax.plot(x_pos, y_pos, color=traj_colors[i], linewidth=1.5, alpha=0.8,\n",
    "            label=f\"Episode {i+1}\")\n",
    "    ax.scatter(x_pos[0], y_pos[0], color=traj_colors[i], marker='o', s=60, zorder=5)\n",
    "    ax.scatter(x_pos[-1], y_pos[-1], color=traj_colors[i], marker='x', s=80, zorder=5)\n",
    "\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "ax.scatter(0, 0, color='red', marker='^', s=120, zorder=10, label='Landing Pad')\n",
    "\n",
    "ax.set_title(f\"GA Lander Trajectories (seed {SEED_LIST[0]}, {TRAJECTORY_EPISODES} episodes)\", fontsize=14)\n",
    "ax.set_xlabel(\"X Position\")\n",
    "ax.set_ylabel(\"Y Position\")\n",
    "ax.legend(fontsize=9)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GIF Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIF Visualization (one per seed, best genome)\n",
    "\n",
    "output_dir = os.path.join(NOTEBOOK_DIR, \"outputs_ga\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for seed in SEED_LIST:\n",
    "    print(f\"Generating GIF for GA seed {seed} (best genome)...\")\n",
    "\n",
    "    gif_path = os.path.join(output_dir, f\"ga_seed{seed}.gif\")\n",
    "    record_genome_gif(best_genomes[seed], seed, gif_path)\n",
    "\n",
    "    print(f\"  Saved: {gif_path}\")\n",
    "    display(Image(filename=gif_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Table\n",
    "\n",
    "params = {\n",
    "    \"input_size\": INPUT_SIZE,\n",
    "    \"hidden1_size\": HIDDEN1_SIZE,\n",
    "    \"hidden2_size\": HIDDEN2_SIZE,\n",
    "    \"output_size\": OUTPUT_SIZE,\n",
    "    \"population_size\": POPULATION_SIZE,\n",
    "    \"mutation_rate\": MUTATION_RATE,\n",
    "    \"generations\": GENERATIONS,\n",
    "    \"eval_seeds_per_generation\": EVAL_SEEDS_PER_GEN,\n",
    "    \"elitism\": 3,\n",
    "    \"parent_selection\": \"top 20%\",\n",
    "    \"crossover\": \"uniform gene-wise\",\n",
    "    \"mutation\": \"Gaussian (clipped +/-0.1)\",\n",
    "    \"activation\": \"tanh (hidden), linear (output)\",\n",
    "    \"max_workers\": MAX_WORKERS,\n",
    "}\n",
    "\n",
    "rows = [{\"Parameter\": k, \"Value\": str(v)} for k, v in params.items()]\n",
    "\n",
    "print(\"*** GA Hyperparameters ***\")\n",
    "print(pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Experimental Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Details\n",
    "\n",
    "| Property | Value |\n",
    "|---|---|\n",
    "| Environment | `LunarLander-v3` (Gymnasium) |\n",
    "| Observation Space | `Box(8,)` — continuous 8-dimensional vector |\n",
    "| Action Space | `Discrete(4)` — do nothing, fire left, fire main, fire right |\n",
    "| Solved Threshold | Mean reward >= 200 over 100 consecutive episodes |\n",
    "| Wind | Disabled (`enable_wind=False`) |\n",
    "\n",
    "**Observation vector:** `[x, y, vx, vy, angle, angular_velocity, left_leg_contact, right_leg_contact]`\n",
    "\n",
    "**Reward structure:**\n",
    "- Moving toward the landing pad: positive\n",
    "- Moving away: negative\n",
    "- Crash: -100\n",
    "- Successful landing: +100\n",
    "- Each leg ground contact: +10\n",
    "- Firing main engine: -0.3 per frame\n",
    "- Firing side engine: -0.03 per frame\n",
    "\n",
    "**Termination rules:**\n",
    "- **Terminated (success):** The lander comes to rest on the ground with both legs in contact, near-zero velocity\n",
    "- **Terminated (crash):** The lander body contacts the ground, or the lander moves outside the viewport boundaries\n",
    "- **Truncated (timeout):** The episode exceeds 1000 timesteps without termination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System and library versions\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"Gymnasium: {gym.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Network: {INPUT_SIZE} -> {HIDDEN1_SIZE} -> {HIDDEN2_SIZE} -> {OUTPUT_SIZE}\")\n",
    "print(f\"Activation: tanh (hidden), linear (output)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_taap_p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
