{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md0",
   "metadata": {},
   "source": [
    "# Annex 2: Three-Way Comparison - DQN vs PPO vs GA\n",
    "\n",
    "This notebook loads the best trained models from DQN, PPO, and GA and runs a\n",
    "unified comparison across all three approaches.\n",
    "\n",
    "- **DQN / PPO**: gradient-based deep RL (Stable-Baselines3), 2x256 MLP, 1.5M env steps\n",
    "- **GA**: gradient-free neuroevolution, 2x10 MLP, ~225M env steps (5000 generations x 50 population x 3 eval seeds)\n",
    "\n",
    "No training is required. Run `lab011.ipynb` (DQN/PPO) and `05_annex1_genetic.ipynb` (GA) first.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md1",
   "metadata": {},
   "source": [
    "The training budgets are not directly comparable. DQN and PPO each used 1.5M environment\n",
    "steps with gradient-based optimization, while the GA used approximately 225M environment\n",
    "interactions (150x more) with no gradient computation. This difference is inherent to the\n",
    "algorithms: evolutionary methods require many more samples because they receive no directional\n",
    "feedback per weight, only a scalar fitness signal per genome.\n",
    "\n",
    "The comparison here focuses on **final policy quality** rather than sample efficiency, answering\n",
    "the question: regardless of how they were trained, which approach produced the best landing policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "import imageio\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# GA inference\n",
    "from genetic.evolution.neural_network import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "SEED_LIST = [42, 123, 3407]\n",
    "\n",
    "ALGORITHM_MAP = {\n",
    "    \"dqn\": DQN,\n",
    "    \"ppo\": PPO,\n",
    "}\n",
    "\n",
    "NOTEBOOK_DIR = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "GYMNASIUM_MODEL = \"LunarLander-v3\"\n",
    "WIND_ENABLED = False\n",
    "EVALUATION_EPISODES = 20\n",
    "TRAJECTORY_EPISODES = 3\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# Session prefixes\n",
    "DQN_PPO_SESSION = \"lab011\"\n",
    "GA_SESSION = \"annex1\"\n",
    "\n",
    "# GA network architecture\n",
    "GA_INPUT = 8\n",
    "GA_HIDDEN1 = 10\n",
    "GA_HIDDEN2 = 10\n",
    "GA_OUTPUT = 4\n",
    "\n",
    "# All three algorithms for comparison\n",
    "ALL_ALGO_NAMES = [\"dqn\", \"ppo\", \"ga\"]\n",
    "ALL_ALGO_COLORS = {\"dqn\": \"tab:blue\", \"ppo\": \"tab:orange\", \"ga\": \"tab:green\"}\n",
    "ACTION_LABELS = [\"Do Nothing\", \"Fire Left\", \"Fire Main\", \"Fire Right\"]\n",
    "\n",
    "seed_colors = list(plt.colormaps[\"tab10\"](range(10)))\n",
    "\n",
    "\n",
    "# --- Discovery functions ---\n",
    "\n",
    "def discover_sb3_best_models(session_prefix):\n",
    "    models_root = os.path.join(NOTEBOOK_DIR, \"../models\")\n",
    "    best_models = {}\n",
    "    for algo_name in ALGORITHM_MAP:\n",
    "        best_models[algo_name] = {}\n",
    "        algo_dir = os.path.join(models_root, algo_name)\n",
    "        if not os.path.isdir(algo_dir):\n",
    "            continue\n",
    "        for run_folder in sorted(glob.glob(os.path.join(algo_dir, \"????-??-??_??_??_??\"))):\n",
    "            best_model_path = os.path.join(run_folder, \"best_model.zip\")\n",
    "            if not os.path.isfile(best_model_path):\n",
    "                continue\n",
    "            for f in os.listdir(run_folder):\n",
    "                if f.startswith(session_prefix) and f.endswith(\".zip\") and f != \"best_model.zip\":\n",
    "                    seed_str = f.replace(\".zip\", \"\").split(\"_\")[-1]\n",
    "                    if seed_str.isdigit() and int(seed_str) in SEED_LIST:\n",
    "                        best_models[algo_name][int(seed_str)] = best_model_path\n",
    "                    break\n",
    "    return best_models\n",
    "\n",
    "\n",
    "def discover_ga_best_genomes(session_prefix):\n",
    "    models_root = os.path.join(NOTEBOOK_DIR, \"../models\")\n",
    "    ga_dir = os.path.join(models_root, \"ga\")\n",
    "    genomes = {}\n",
    "    if not os.path.isdir(ga_dir):\n",
    "        return genomes\n",
    "    for run_folder in sorted(glob.glob(os.path.join(ga_dir, \"????-??-??_??_??_??\"))):\n",
    "        for f in os.listdir(run_folder):\n",
    "            if f.startswith(session_prefix) and f.endswith(\".npy\"):\n",
    "                seed_str = f.replace(\".npy\", \"\").split(\"_\")[-1]\n",
    "                if seed_str.isdigit() and int(seed_str) in SEED_LIST:\n",
    "                    genomes[int(seed_str)] = os.path.join(run_folder, f)\n",
    "    return genomes\n",
    "\n",
    "\n",
    "# Discover all models\n",
    "sb3_models = discover_sb3_best_models(DQN_PPO_SESSION)\n",
    "ga_genomes = discover_ga_best_genomes(GA_SESSION)\n",
    "\n",
    "print(f\"DQN/PPO session: {DQN_PPO_SESSION}\")\n",
    "print(f\"GA session: {GA_SESSION}\")\n",
    "print(f\"Seeds: {SEED_LIST}\")\n",
    "print(f\"Evaluation episodes per seed: {EVALUATION_EPISODES}\")\n",
    "print()\n",
    "print(\"Discovered models:\")\n",
    "for algo_name in [\"dqn\", \"ppo\"]:\n",
    "    for seed in SEED_LIST:\n",
    "        path = sb3_models.get(algo_name, {}).get(seed)\n",
    "        print(f\"  {algo_name.upper()} seed {seed}: {path if path else 'NOT FOUND'}\")\n",
    "for seed in SEED_LIST:\n",
    "    path = ga_genomes.get(seed)\n",
    "    print(f\"  GA seed {seed}: {path if path else 'NOT FOUND'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md5",
   "metadata": {},
   "source": [
    "All three algorithms are evaluated under identical conditions: 20 deterministic episodes per\n",
    "seed, same environment configuration (no wind), same seeds. DQN and PPO use SB3's\n",
    "`evaluate_policy` with `deterministic=True`. The GA uses `argmax` over the network output,\n",
    "which is inherently deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all algorithms\n",
    "\n",
    "evaluation_results = {}  # {algo: {seed: np.array}}\n",
    "\n",
    "# --- DQN and PPO ---\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "    evaluation_results[algo_name] = {}\n",
    "    for seed in SEED_LIST:\n",
    "        load_path = sb3_models.get(algo_name, {}).get(seed)\n",
    "        if load_path is None:\n",
    "            print(f\"SKIPPING {algo_name.upper()} seed {seed} - not found\")\n",
    "            continue\n",
    "\n",
    "        print(f\"Evaluating {algo_name.upper()} seed {seed}...\")\n",
    "\n",
    "        def make_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        model = algo_class.load(load_path, env=DummyVecEnv([make_env]), device=DEVICE)\n",
    "        eval_env = Monitor(gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED))\n",
    "        eval_env.reset(seed=seed)\n",
    "\n",
    "        rewards, _ = evaluate_policy(\n",
    "            model, eval_env,\n",
    "            n_eval_episodes=EVALUATION_EPISODES,\n",
    "            deterministic=True,\n",
    "            return_episode_rewards=True\n",
    "        )\n",
    "        evaluation_results[algo_name][seed] = np.array(rewards)\n",
    "        eval_env.close()\n",
    "\n",
    "    print(f\"{algo_name.upper()}: done.\\n\")\n",
    "\n",
    "# --- GA ---\n",
    "evaluation_results[\"ga\"] = {}\n",
    "for seed in SEED_LIST:\n",
    "    genome_path = ga_genomes.get(seed)\n",
    "    if genome_path is None:\n",
    "        print(f\"SKIPPING GA seed {seed} - not found\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Evaluating GA seed {seed}...\")\n",
    "    genome = np.load(genome_path)\n",
    "    nn = NeuralNetwork(GA_INPUT, GA_HIDDEN1, GA_HIDDEN2, GA_OUTPUT, genome)\n",
    "    env = gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED)\n",
    "    rewards = []\n",
    "\n",
    "    for ep in range(EVALUATION_EPISODES):\n",
    "        obs, _ = env.reset(seed=seed + ep)\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        while not done:\n",
    "            output = nn.forward(obs)\n",
    "            action = np.argmax(output)\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += float(reward)\n",
    "        rewards.append(total_reward)\n",
    "\n",
    "    evaluation_results[\"ga\"][seed] = np.array(rewards)\n",
    "    env.close()\n",
    "\n",
    "print(\"GA: done.\\n\")\n",
    "print(\"All evaluations complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md8",
   "metadata": {},
   "source": [
    "Per-algorithm evaluation tables showing mean, standard deviation, min, max, and success\n",
    "rate across all seeds. The overall row aggregates all 60 episodes (20 per seed x 3 seeds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-Algorithm Evaluation Summary Tables\n",
    "\n",
    "for algo_name in ALL_ALGO_NAMES:\n",
    "    rows = []\n",
    "    for seed in SEED_LIST:\n",
    "        r = evaluation_results.get(algo_name, {}).get(seed)\n",
    "        if r is None:\n",
    "            continue\n",
    "        rows.append({\n",
    "            \"Seed\": seed,\n",
    "            \"Mean Reward\": f\"{np.mean(r):.2f}\",\n",
    "            \"Std Dev\": f\"{np.std(r):.2f}\",\n",
    "            \"Min Reward\": f\"{np.min(r):.2f}\",\n",
    "            \"Max Reward\": f\"{np.max(r):.2f}\",\n",
    "            \"Success Rate\": f\"{(r >= 200).sum() / len(r) * 100:.1f}%\"\n",
    "        })\n",
    "\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST\n",
    "                            if s in evaluation_results.get(algo_name, {})])\n",
    "    rows.append({\n",
    "        \"Seed\": \"Overall\",\n",
    "        \"Mean Reward\": f\"{np.mean(all_r):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(all_r):.2f}\",\n",
    "        \"Min Reward\": f\"{np.min(all_r):.2f}\",\n",
    "        \"Max Reward\": f\"{np.max(all_r):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_r >= 200).sum() / len(all_r) * 100:.1f}%\"\n",
    "    })\n",
    "\n",
    "    print(f\"*** {algo_name.upper()} EVALUATION SUMMARY ***\")\n",
    "    print(f\"Episodes per seed: {EVALUATION_EPISODES} | Total: {len(all_r)}\")\n",
    "    print(pd.DataFrame(rows).to_string(index=False))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md10",
   "metadata": {},
   "source": [
    "Combined comparison table with one row per algorithm, showing the headline performance numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-Algorithm Combined Summary Table\n",
    "\n",
    "rows = []\n",
    "for algo_name in ALL_ALGO_NAMES:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST\n",
    "                            if s in evaluation_results.get(algo_name, {})])\n",
    "    rows.append({\n",
    "        \"Algorithm\": algo_name.upper(),\n",
    "        \"Mean Reward\": f\"{np.mean(all_r):.2f}\",\n",
    "        \"Std Dev\": f\"{np.std(all_r):.2f}\",\n",
    "        \"Min Reward\": f\"{np.min(all_r):.2f}\",\n",
    "        \"Max Reward\": f\"{np.max(all_r):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_r >= 200).sum() / len(all_r) * 100:.1f}%\",\n",
    "        \"Network\": \"2x256 MLP\" if algo_name != \"ga\" else \"2x10 MLP\",\n",
    "        \"Env Steps\": \"1.5M\" if algo_name != \"ga\" else \"~225M\",\n",
    "    })\n",
    "\n",
    "print(\"*** THREE-WAY COMPARISON ***\")\n",
    "print(f\"Seeds: {SEED_LIST} | Episodes per seed: {EVALUATION_EPISODES}\")\n",
    "print()\n",
    "print(pd.DataFrame(rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Comparison Charts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md13",
   "metadata": {},
   "source": [
    "Grouped bar chart showing mean reward per seed for all three algorithms side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-Way Grouped Bar Chart (per seed)\n",
    "\n",
    "n_algos = len(ALL_ALGO_NAMES)\n",
    "n_seeds = len(SEED_LIST)\n",
    "bar_width = 0.8 / n_algos\n",
    "x = np.arange(n_seeds)\n",
    "\n",
    "plt.figure(figsize=(max(10, 3 * n_seeds), 6))\n",
    "for i, algo_name in enumerate(ALL_ALGO_NAMES):\n",
    "    means = [np.mean(evaluation_results[algo_name][s]) for s in SEED_LIST\n",
    "             if s in evaluation_results.get(algo_name, {})]\n",
    "    stds = [np.std(evaluation_results[algo_name][s]) for s in SEED_LIST\n",
    "            if s in evaluation_results.get(algo_name, {})]\n",
    "    offset = (i - (n_algos - 1) / 2) * bar_width\n",
    "    plt.bar(x + offset, means, bar_width, yerr=stds, capsize=4,\n",
    "            label=algo_name.upper(), color=ALL_ALGO_COLORS[algo_name], alpha=0.8)\n",
    "\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.xticks(x, [str(s) for s in SEED_LIST])\n",
    "plt.title(f\"DQN vs PPO vs GA: Mean Reward per Seed ({EVALUATION_EPISODES} episodes each)\", fontsize=14)\n",
    "plt.xlabel(\"Seed\")\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md15",
   "metadata": {},
   "source": [
    "**Figure 1** - Mean evaluation reward per seed for all three algorithms with standard deviation error bars."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md16",
   "metadata": {},
   "source": [
    "Overall mean reward collapsed across all seeds and episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-Way Overall Mean Reward Bar Chart\n",
    "\n",
    "overall_means = []\n",
    "overall_stds = []\n",
    "for algo_name in ALL_ALGO_NAMES:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST\n",
    "                            if s in evaluation_results.get(algo_name, {})])\n",
    "    overall_means.append(np.mean(all_r))\n",
    "    overall_stds.append(np.std(all_r))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar([a.upper() for a in ALL_ALGO_NAMES], overall_means, yerr=overall_stds,\n",
    "               capsize=6, color=[ALL_ALGO_COLORS[a] for a in ALL_ALGO_NAMES], alpha=0.8)\n",
    "plt.axhline(y=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "\n",
    "for bar, mean in zip(bars, overall_means):\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 5,\n",
    "             f'{mean:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.title(f\"Overall Mean Reward: DQN vs PPO vs GA ({EVALUATION_EPISODES * len(SEED_LIST)} episodes each)\", fontsize=14)\n",
    "plt.ylabel(\"Mean Reward\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md18",
   "metadata": {},
   "source": [
    "**Figure 2** - Overall mean evaluation reward for each algorithm, aggregated across all seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md19",
   "metadata": {},
   "source": [
    "Overlaid reward distributions showing the spread and overlap of evaluation rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-Way Reward Distribution Comparison\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for algo_name in ALL_ALGO_NAMES:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST\n",
    "                            if s in evaluation_results.get(algo_name, {})])\n",
    "    plt.hist(all_r, bins=15, alpha=0.5, color=ALL_ALGO_COLORS[algo_name],\n",
    "             edgecolor='black', label=f\"{algo_name.upper()} (mean={np.mean(all_r):.1f})\")\n",
    "\n",
    "plt.axvline(x=200, color='red', linestyle='--', label='Solved Threshold (200)')\n",
    "plt.title('Reward Distribution: DQN vs PPO vs GA (all seeds combined)', fontsize=14)\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md21",
   "metadata": {},
   "source": [
    "**Figure 3** - Overlaid reward distributions for all three algorithms across all seeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md22",
   "metadata": {},
   "source": [
    "Box plots showing the full reward distribution per seed for each algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-Way Box Plot Comparison per Seed\n",
    "\n",
    "fig, axes = plt.subplots(1, len(SEED_LIST), figsize=(6 * len(SEED_LIST), 5), sharey=True)\n",
    "if len(SEED_LIST) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, seed in zip(axes, SEED_LIST):\n",
    "    data = [evaluation_results[algo_name][seed] for algo_name in ALL_ALGO_NAMES\n",
    "            if seed in evaluation_results.get(algo_name, {})]\n",
    "    bp = ax.boxplot(data, labels=[a.upper() for a in ALL_ALGO_NAMES], patch_artist=True)\n",
    "    for patch, algo_name in zip(bp['boxes'], ALL_ALGO_NAMES):\n",
    "        patch.set_facecolor(ALL_ALGO_COLORS[algo_name])\n",
    "        patch.set_alpha(0.6)\n",
    "    ax.axhline(y=200, color='red', linestyle='--')\n",
    "    ax.set_title(f\"Seed {seed}\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Reward\")\n",
    "fig.suptitle(f\"DQN vs PPO vs GA: Reward Distribution per Seed ({EVALUATION_EPISODES} episodes each)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md24",
   "metadata": {},
   "source": [
    "**Figure 4** - Box plot comparison of reward distributions per seed for all three algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Statistical Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md26",
   "metadata": {},
   "source": [
    "Pairwise Mann-Whitney U tests comparing reward distributions between all algorithm pairs,\n",
    "and Chi-squared tests comparing success rates. With three algorithms, there are three pairwise\n",
    "comparisons: DQN vs PPO, DQN vs GA, and PPO vs GA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise Statistical Significance Tests\n",
    "\n",
    "algo_all_rewards = {}\n",
    "for algo_name in ALL_ALGO_NAMES:\n",
    "    algo_all_rewards[algo_name] = np.concatenate([\n",
    "        evaluation_results[algo_name][s] for s in SEED_LIST\n",
    "        if s in evaluation_results.get(algo_name, {})\n",
    "    ])\n",
    "\n",
    "pairs = [(\"dqn\", \"ppo\"), (\"dqn\", \"ga\"), (\"ppo\", \"ga\")]\n",
    "\n",
    "reward_rows = []\n",
    "success_rows = []\n",
    "\n",
    "for a1, a2 in pairs:\n",
    "    r1 = algo_all_rewards[a1]\n",
    "    r2 = algo_all_rewards[a2]\n",
    "\n",
    "    # Mann-Whitney U\n",
    "    mwu = stats.mannwhitneyu(r1, r2, alternative='two-sided')\n",
    "\n",
    "    reward_rows.append({\n",
    "        \"Comparison\": f\"{a1.upper()} vs {a2.upper()}\",\n",
    "        f\"{a1.upper()} Mean\": f\"{np.mean(r1):.2f}\",\n",
    "        f\"{a2.upper()} Mean\": f\"{np.mean(r2):.2f}\",\n",
    "        \"U Statistic\": f\"{float(mwu.statistic):.1f}\",\n",
    "        \"p-value\": f\"{float(mwu.pvalue):.4f}\",\n",
    "        \"Significant\": \"Yes\" if mwu.pvalue < 0.05 else \"No\"\n",
    "    })\n",
    "\n",
    "    # Chi-squared for success rates\n",
    "    s1 = int((r1 >= 200).sum())\n",
    "    s2 = int((r2 >= 200).sum())\n",
    "    f1 = len(r1) - s1\n",
    "    f2 = len(r2) - s2\n",
    "    contingency = np.array([[s1, f1], [s2, f2]])\n",
    "\n",
    "    if np.all(contingency.sum(axis=0) > 0) and np.all(contingency.sum(axis=1) > 0):\n",
    "        chi2_result = stats.chi2_contingency(contingency)\n",
    "        chi2_stat = float(chi2_result[0])\n",
    "        chi2_p = float(chi2_result[1])\n",
    "        chi2_valid = True\n",
    "    else:\n",
    "        chi2_stat, chi2_p = 0.0, 1.0\n",
    "        chi2_valid = False\n",
    "\n",
    "    note = \"\" if chi2_valid else \" (skipped)\"\n",
    "    success_rows.append({\n",
    "        \"Comparison\": f\"{a1.upper()} vs {a2.upper()}\",\n",
    "        f\"{a1.upper()} Success\": f\"{s1/len(r1)*100:.1f}%\",\n",
    "        f\"{a2.upper()} Success\": f\"{s2/len(r2)*100:.1f}%\",\n",
    "        \"Test\": f\"Chi-squared{note}\",\n",
    "        \"Statistic\": f\"{chi2_stat:.2f}\",\n",
    "        \"p-value\": f\"{chi2_p:.4f}\",\n",
    "        \"Significant\": \"Yes\" if (chi2_valid and chi2_p < 0.05) else \"No\"\n",
    "    })\n",
    "\n",
    "print(\"*** PAIRWISE REWARD COMPARISON (Mann-Whitney U) ***\")\n",
    "print(f\"Sample size per algorithm: {len(algo_all_rewards['dqn'])} episodes\")\n",
    "print()\n",
    "print(pd.DataFrame(reward_rows).to_string(index=False))\n",
    "print()\n",
    "print(\"*** PAIRWISE SUCCESS RATE COMPARISON (Chi-squared) ***\")\n",
    "print()\n",
    "print(pd.DataFrame(success_rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md28",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Agent Behavior Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md29",
   "metadata": {},
   "source": [
    "Action distribution comparison across all three algorithms, revealing differences in control\n",
    "strategy. The GA's smaller network (2x10 vs 2x256) may produce a simpler, more decisive policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect per-step data: actions and trajectories for all three algorithms\n",
    "\n",
    "action_counts = {}\n",
    "trajectory_data = {}\n",
    "\n",
    "# --- DQN and PPO ---\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "    action_counts[algo_name] = np.zeros(len(ACTION_LABELS), dtype=int)\n",
    "    trajectory_data[algo_name] = []\n",
    "\n",
    "    for seed in SEED_LIST:\n",
    "        load_path = sb3_models.get(algo_name, {}).get(seed)\n",
    "        if load_path is None:\n",
    "            continue\n",
    "\n",
    "        def make_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        model = algo_class.load(load_path, env=DummyVecEnv([make_env]), device=DEVICE)\n",
    "        env = gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED)\n",
    "\n",
    "        for ep in range(EVALUATION_EPISODES):\n",
    "            obs, info = env.reset(seed=seed + ep)\n",
    "            done = False\n",
    "            x_pos, y_pos = [obs[0]], [obs[1]]\n",
    "\n",
    "            while not done:\n",
    "                action, _ = model.predict(obs, deterministic=True)\n",
    "                action_int = int(action)\n",
    "                action_counts[algo_name][action_int] += 1\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                x_pos.append(obs[0])\n",
    "                y_pos.append(obs[1])\n",
    "\n",
    "            if seed == SEED_LIST[0] and ep < TRAJECTORY_EPISODES:\n",
    "                trajectory_data[algo_name].append((np.array(x_pos), np.array(y_pos)))\n",
    "\n",
    "        env.close()\n",
    "\n",
    "# --- GA ---\n",
    "action_counts[\"ga\"] = np.zeros(len(ACTION_LABELS), dtype=int)\n",
    "trajectory_data[\"ga\"] = []\n",
    "\n",
    "for seed in SEED_LIST:\n",
    "    genome_path = ga_genomes.get(seed)\n",
    "    if genome_path is None:\n",
    "        continue\n",
    "\n",
    "    genome = np.load(genome_path)\n",
    "    nn = NeuralNetwork(GA_INPUT, GA_HIDDEN1, GA_HIDDEN2, GA_OUTPUT, genome)\n",
    "    env = gym.make(GYMNASIUM_MODEL, enable_wind=WIND_ENABLED)\n",
    "\n",
    "    for ep in range(EVALUATION_EPISODES):\n",
    "        obs, _ = env.reset(seed=seed + ep)\n",
    "        done = False\n",
    "        x_pos, y_pos = [obs[0]], [obs[1]]\n",
    "\n",
    "        while not done:\n",
    "            output = nn.forward(obs)\n",
    "            action = int(np.argmax(output))\n",
    "            action_counts[\"ga\"][action] += 1\n",
    "            obs, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            x_pos.append(obs[0])\n",
    "            y_pos.append(obs[1])\n",
    "\n",
    "        if seed == SEED_LIST[0] and ep < TRAJECTORY_EPISODES:\n",
    "            trajectory_data[\"ga\"].append((np.array(x_pos), np.array(y_pos)))\n",
    "\n",
    "    env.close()\n",
    "\n",
    "for algo_name in ALL_ALGO_NAMES:\n",
    "    total = action_counts[algo_name].sum()\n",
    "    print(f\"{algo_name.upper()}: {total:,} total actions across {EVALUATION_EPISODES * len(SEED_LIST)} episodes\")\n",
    "\n",
    "print(\"\\nBehavior data collection complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md31",
   "metadata": {},
   "source": [
    "Action distribution bar charts showing absolute counts and percentages for all three algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-Way Action Distribution\n",
    "\n",
    "n_actions = len(ACTION_LABELS)\n",
    "x = np.arange(n_actions)\n",
    "bar_width = 0.25\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Absolute counts\n",
    "for i, algo_name in enumerate(ALL_ALGO_NAMES):\n",
    "    offset = (i - 1) * bar_width\n",
    "    ax1.bar(x + offset, action_counts[algo_name], bar_width,\n",
    "            label=algo_name.upper(), color=ALL_ALGO_COLORS[algo_name], alpha=0.8)\n",
    "\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(ACTION_LABELS, rotation=15)\n",
    "ax1.set_title(\"Action Counts (Absolute)\", fontsize=13)\n",
    "ax1.set_ylabel(\"Count\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Percentage distribution\n",
    "for i, algo_name in enumerate(ALL_ALGO_NAMES):\n",
    "    pcts = action_counts[algo_name] / action_counts[algo_name].sum() * 100\n",
    "    offset = (i - 1) * bar_width\n",
    "    bars = ax2.bar(x + offset, pcts, bar_width,\n",
    "                   label=algo_name.upper(), color=ALL_ALGO_COLORS[algo_name], alpha=0.8)\n",
    "    for bar, pct in zip(bars, pcts):\n",
    "        ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,\n",
    "                 f'{pct:.1f}%', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(ACTION_LABELS, rotation=15)\n",
    "ax2.set_title(\"Action Distribution (Percentage)\", fontsize=13)\n",
    "ax2.set_ylabel(\"Percentage (%)\")\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "fig.suptitle(f\"DQN vs PPO vs GA: Action Distribution ({EVALUATION_EPISODES * len(SEED_LIST)} episodes each)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md33",
   "metadata": {},
   "source": [
    "**Figure 5** - Action usage frequency for all three algorithms across all evaluation episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md34",
   "metadata": {},
   "source": [
    "Sample landing trajectories for each algorithm and a three-way overlay comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trajectory Plots: per algorithm\n",
    "\n",
    "traj_colors = list(plt.colormaps[\"Set2\"](range(8)))\n",
    "\n",
    "fig, axes = plt.subplots(1, len(ALL_ALGO_NAMES), figsize=(8 * len(ALL_ALGO_NAMES), 6), sharey=True)\n",
    "\n",
    "for ax, algo_name in zip(axes, ALL_ALGO_NAMES):\n",
    "    for i, (x_pos, y_pos) in enumerate(trajectory_data[algo_name]):\n",
    "        ax.plot(x_pos, y_pos, color=traj_colors[i], linewidth=1.5, alpha=0.8,\n",
    "                label=f\"Episode {i+1}\")\n",
    "        ax.scatter(x_pos[0], y_pos[0], color=traj_colors[i], marker='o', s=60, zorder=5)\n",
    "        ax.scatter(x_pos[-1], y_pos[-1], color=traj_colors[i], marker='x', s=80, zorder=5)\n",
    "\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "    ax.scatter(0, 0, color='red', marker='^', s=120, zorder=10, label='Landing Pad')\n",
    "    ax.set_title(f\"{algo_name.upper()} Trajectories (seed {SEED_LIST[0]})\", fontsize=13)\n",
    "    ax.set_xlabel(\"X Position\")\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "axes[0].set_ylabel(\"Y Position\")\n",
    "fig.suptitle(f\"Lander Trajectories: DQN vs PPO vs GA ({TRAJECTORY_EPISODES} episodes each)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md36",
   "metadata": {},
   "source": [
    "**Figure 6** - Sample landing trajectories for each algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md37",
   "metadata": {},
   "source": [
    "Direct overlay of one trajectory from each algorithm on the same chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-Way Trajectory Overlay\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "for algo_name in ALL_ALGO_NAMES:\n",
    "    if trajectory_data[algo_name]:\n",
    "        x_pos, y_pos = trajectory_data[algo_name][0]\n",
    "        plt.plot(x_pos, y_pos, color=ALL_ALGO_COLORS[algo_name], linewidth=2, alpha=0.8,\n",
    "                 label=f\"{algo_name.upper()}\")\n",
    "        plt.scatter(x_pos[0], y_pos[0], color=ALL_ALGO_COLORS[algo_name], marker='o', s=80, zorder=5)\n",
    "        plt.scatter(x_pos[-1], y_pos[-1], color=ALL_ALGO_COLORS[algo_name], marker='x', s=100, zorder=5)\n",
    "\n",
    "plt.scatter(0, 0, color='red', marker='^', s=150, zorder=10, label='Landing Pad')\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5, alpha=0.3)\n",
    "\n",
    "plt.title(f\"DQN vs PPO vs GA: Trajectory Comparison (seed {SEED_LIST[0]}, episode 1)\", fontsize=14)\n",
    "plt.xlabel(\"X Position\")\n",
    "plt.ylabel(\"Y Position\")\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md39",
   "metadata": {},
   "source": [
    "**Figure 7** - Direct overlay of landing trajectories from all three algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## GIF Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md41",
   "metadata": {},
   "source": [
    "Animated visualizations of a single deterministic episode for each algorithm and seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GIF Visualizations for all three algorithms\n",
    "\n",
    "output_dir = os.path.join(NOTEBOOK_DIR, \"outputs_comparison\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# --- DQN and PPO ---\n",
    "for algo_name, algo_class in ALGORITHM_MAP.items():\n",
    "    for seed in SEED_LIST:\n",
    "        load_path = sb3_models.get(algo_name, {}).get(seed)\n",
    "        if load_path is None:\n",
    "            continue\n",
    "\n",
    "        print(f\"Generating GIF for {algo_name.upper()} seed {seed}...\")\n",
    "\n",
    "        def make_vis_env(s=seed):\n",
    "            env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "            env.reset(seed=s)\n",
    "            return env\n",
    "\n",
    "        model = algo_class.load(load_path, env=DummyVecEnv([make_vis_env]), device=DEVICE)\n",
    "        vis_env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "        frames = []\n",
    "        obs, _ = vis_env.reset(seed=seed)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            obs, reward, terminated, truncated, _ = vis_env.step(action)\n",
    "            done = terminated or truncated\n",
    "            frames.append(vis_env.render())\n",
    "\n",
    "        vis_env.close()\n",
    "        gif_path = os.path.join(output_dir, f\"{algo_name}_seed{seed}.gif\")\n",
    "        imageio.mimsave(gif_path, frames, fps=30)\n",
    "        print(f\"  Saved: {gif_path}\")\n",
    "        display(Image(filename=gif_path))\n",
    "\n",
    "# --- GA ---\n",
    "for seed in SEED_LIST:\n",
    "    genome_path = ga_genomes.get(seed)\n",
    "    if genome_path is None:\n",
    "        continue\n",
    "\n",
    "    print(f\"Generating GIF for GA seed {seed}...\")\n",
    "    genome = np.load(genome_path)\n",
    "    nn = NeuralNetwork(GA_INPUT, GA_HIDDEN1, GA_HIDDEN2, GA_OUTPUT, genome)\n",
    "    vis_env = gym.make(GYMNASIUM_MODEL, render_mode=\"rgb_array\", enable_wind=WIND_ENABLED)\n",
    "    frames = []\n",
    "    obs, _ = vis_env.reset(seed=seed)\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        frames.append(vis_env.render())\n",
    "        output = nn.forward(obs)\n",
    "        action = np.argmax(output)\n",
    "        obs, reward, terminated, truncated, _ = vis_env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "    vis_env.close()\n",
    "    gif_path = os.path.join(output_dir, f\"ga_seed{seed}.gif\")\n",
    "    imageio.mimsave(gif_path, frames, fps=30)\n",
    "    print(f\"  Saved: {gif_path}\")\n",
    "    display(Image(filename=gif_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md43",
   "metadata": {},
   "source": [
    "**Figure 8** - Animated visualization of a single deterministic episode for each algorithm and seed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Algorithm Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md45",
   "metadata": {},
   "source": [
    "Summary table comparing the three approaches across key dimensions: optimization method,\n",
    "network size, training budget, and final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithm Comparison Summary Table\n",
    "\n",
    "summary_rows = []\n",
    "for algo_name in ALL_ALGO_NAMES:\n",
    "    all_r = np.concatenate([evaluation_results[algo_name][s] for s in SEED_LIST\n",
    "                            if s in evaluation_results.get(algo_name, {})])\n",
    "\n",
    "    if algo_name == \"dqn\":\n",
    "        opt_method = \"Gradient-based (off-policy)\"\n",
    "        network = \"8-256-256-4 (MLP)\"\n",
    "        params = \"~70k\"\n",
    "        budget = \"1.5M env steps\"\n",
    "    elif algo_name == \"ppo\":\n",
    "        opt_method = \"Gradient-based (on-policy)\"\n",
    "        network = \"8-256-256-4 (MLP)\"\n",
    "        params = \"~70k\"\n",
    "        budget = \"1.5M env steps\"\n",
    "    else:\n",
    "        opt_method = \"Evolutionary (gradient-free)\"\n",
    "        network = \"8-10-10-4 (MLP)\"\n",
    "        params = \"~274\"\n",
    "        budget = \"~225M env steps\"\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"Algorithm\": algo_name.upper(),\n",
    "        \"Optimization\": opt_method,\n",
    "        \"Network\": network,\n",
    "        \"Parameters\": params,\n",
    "        \"Training Budget\": budget,\n",
    "        \"Mean Reward\": f\"{np.mean(all_r):.2f}\",\n",
    "        \"Success Rate\": f\"{(all_r >= 200).sum() / len(all_r) * 100:.1f}%\",\n",
    "    })\n",
    "\n",
    "print(\"*** ALGORITHM COMPARISON SUMMARY ***\")\n",
    "print(pd.DataFrame(summary_rows).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md47",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_taap_p2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
